[
  {
    "title": "Unifying Information Across Domains",
    "create_time": 1738717218.959835,
    "update_time": 1738725931.794856,
    "mapping": {
      "aaa10197-b1f8-45e4-aaf0-793f6dad647c": {
        "id": "aaa10197-b1f8-45e4-aaf0-793f6dad647c",
        "message": null,
        "parent": null,
        "children": [
          "c632d5e0-68a4-4a3f-925e-3b2eea5f6471"
        ]
      },
      "c632d5e0-68a4-4a3f-925e-3b2eea5f6471": {
        "id": "c632d5e0-68a4-4a3f-925e-3b2eea5f6471",
        "message": {
          "id": "c632d5e0-68a4-4a3f-925e-3b2eea5f6471",
          "author": {
            "role": "user",
            "name": null,
            "metadata": {}
          },
          "create_time": 1738717218.975416,
          "update_time": null,
          "content": {
            "content_type": "text",
            "parts": [
              "In my opinion, information is a concept that is not sufficiently unified. Every domain has a similar looking, but fundamentally different definition for entropy, which is the expectation of -log p over a different kind of distribution p for every field. Most actual use cases of information so far rely on things like mutual information, encoding/compression, or discretization of some form.\n\nThere is something big missing here that is more fundamental than all of the derived quantities that biologists and thermodynamics researchers use. Look far and wide for creative ideas at unifying information, explaining how biology, thermodynamics, computer science and many more are special applications of the general theory of information. I am not talking about information theory. Information theory is very specific to length of encoding and has limited generalization ability to, for example, continuous systems. At least, I have not seen solid attempts. Ennlighten me about all the grand unification schemes of information in the past and any ongoing work on this topic"
            ]
          },
          "status": "finished_successfully",
          "end_turn": null,
          "weight": 1.0,
          "metadata": {
            "system_hints": [
              "research"
            ],
            "serialization_metadata": {
              "custom_symbol_offsets": []
            },
            "dictation": false,
            "request_id": "90cef6782fe836d0-IAD",
            "message_source": null,
            "timestamp_": "absolute",
            "message_type": null
          },
          "recipient": "all",
          "channel": null
        },
        "parent": "aaa10197-b1f8-45e4-aaf0-793f6dad647c",
        "children": [
          "9887d42b-4227-441b-8da7-e2a49f81246f"
        ]
      },
      "9887d42b-4227-441b-8da7-e2a49f81246f": {
        "id": "9887d42b-4227-441b-8da7-e2a49f81246f",
        "message": {
          "id": "9887d42b-4227-441b-8da7-e2a49f81246f",
          "author": {
            "role": "assistant",
            "name": null,
            "metadata": {}
          },
          "create_time": 1738717224.125269,
          "update_time": null,
          "content": {
            "content_type": "text",
            "parts": [
              "This is a fascinating and deep question! To ensure I find the most relevant work, could you clarify a few things:\n\n- Are you interested in theoretical frameworks only, or do you also want empirical applications?\n- Should I focus on mathematical formalisms (e.g., category theory, measure-theoretic entropy, etc.), or do you prefer conceptual/philosophical perspectives?\n- Would you like me to include speculative or emerging ideas, even if they are not yet widely accepted?\n- Are there specific researchers or schools of thought you are already familiar with, so I can avoid redundant information?\n\nLet me know so I can refine the search accordingly!"
            ]
          },
          "status": "finished_successfully",
          "end_turn": true,
          "weight": 1.0,
          "metadata": {
            "message_type": null,
            "model_slug": "research",
            "default_model_slug": "o1-pro",
            "parent_id": "c632d5e0-68a4-4a3f-925e-3b2eea5f6471",
            "request_id": "90cef6782fe836d0-IAD",
            "timestamp_": "absolute"
          },
          "recipient": "all",
          "channel": null
        },
        "parent": "c632d5e0-68a4-4a3f-925e-3b2eea5f6471",
        "children": [
          "c44219ca-55b0-4a17-80cb-923ab372b110"
        ]
      },
      "c44219ca-55b0-4a17-80cb-923ab372b110": {
        "id": "c44219ca-55b0-4a17-80cb-923ab372b110",
        "message": {
          "id": "c44219ca-55b0-4a17-80cb-923ab372b110",
          "author": {
            "role": "user",
            "name": null,
            "metadata": {}
          },
          "create_time": 1738717646.101456,
          "update_time": null,
          "content": {
            "content_type": "text",
            "parts": [
              "Go as broad as possible, finding at least 30 sources. Focus on the application of mathematical principles to make predictions, especially those which are broadly useful and not just domain specific. Avoid pure math like overly jargon-y theorems/lemmas, but otherwise I am an expert in advanced mathematics and physics, so you should be extremely technical, citing specific equations. I want you to explore all schools of thought, and give an exhaustive list of attempts"
            ]
          },
          "status": "finished_successfully",
          "end_turn": null,
          "weight": 1.0,
          "metadata": {
            "system_hints": [
              "research"
            ],
            "serialization_metadata": {
              "custom_symbol_offsets": []
            },
            "dictation": false,
            "request_id": "90cf00e65e8cac8a-YYZ",
            "message_source": null,
            "timestamp_": "absolute",
            "message_type": null
          },
          "recipient": "all",
          "channel": null
        },
        "parent": "9887d42b-4227-441b-8da7-e2a49f81246f",
        "children": [
          "945cef7b-8a77-49cd-bf99-4c16e003333c"
        ]
      },
      "945cef7b-8a77-49cd-bf99-4c16e003333c": {
        "id": "945cef7b-8a77-49cd-bf99-4c16e003333c",
        "message": {
          "id": "945cef7b-8a77-49cd-bf99-4c16e003333c",
          "author": {
            "role": "assistant",
            "name": "research_kickoff_tool",
            "metadata": {}
          },
          "create_time": 1738717653.374289,
          "update_time": null,
          "content": {
            "content_type": "text",
            "parts": [
              ""
            ]
          },
          "status": "finished_successfully",
          "end_turn": false,
          "weight": 1.0,
          "metadata": {
            "async_task_id": "task_5FLsdpdbjGT9fntC7vOEJ7Lv",
            "async_task_conversation_id": "67a2b9d4-9d68-8005-be5f-d0b32cf1bdec",
            "async_task_created_at": "2025-02-05 01:07:33.320231+00:00",
            "async_task_title": "Grand Unification of Information Across Disciplines",
            "async_task_prompt": "Conduct an exhaustive review of past and ongoing attempts to unify the concept of information across different disciplines, focusing on mathematical principles that enable prediction. The goal is to identify theoretical frameworks that generalize across physics, thermodynamics, biology, computer science, and other fields while avoiding purely domain-specific definitions.\n\nThe research should:\n- Identify at least 30 sources from diverse schools of thought, including modern and historical perspectives.\n- Focus on approaches that provide mathematical rigor and predictive capability.\n- Provide detailed explanations, including specific equations and models.\n- Avoid purely abstract mathematical treatments that lack broad applicability.\n- Include ongoing research and emerging theories that may contribute to a general theory of information.\n\nPrioritize sources that attempt to construct a fundamental and broadly applicable theory of information, rather than those focused on specific applications or domain-limited definitions. Present the findings in a structured manner, categorizing different schools of thought and highlighting their mathematical foundations.",
            "async_task_type": "research",
            "async_task_status_messages": {
              "initial": "Grand Unification of Information Across Disciplines",
              "completed_with_time": "Research completed in 9m",
              "completed_no_time": "Completed research",
              "completed": "Researched for ",
              "error": "Research failed",
              "cancelled": "Research cancelled"
            },
            "b1de6e2_s": true,
            "citations": [],
            "content_references": [],
            "message_type": null,
            "model_slug": "research",
            "default_model_slug": "o1-pro",
            "parent_id": "c44219ca-55b0-4a17-80cb-923ab372b110",
            "request_id": "90cf00e65e8cac8a-YYZ",
            "timestamp_": "absolute"
          },
          "recipient": "all",
          "channel": null
        },
        "parent": "c44219ca-55b0-4a17-80cb-923ab372b110",
        "children": [
          "79c763e0-d714-4804-b391-d7987e58123b"
        ]
      },
      "79c763e0-d714-4804-b391-d7987e58123b": {
        "id": "79c763e0-d714-4804-b391-d7987e58123b",
        "message": {
          "id": "79c763e0-d714-4804-b391-d7987e58123b",
          "author": {
            "role": "assistant",
            "name": null,
            "metadata": {}
          },
          "create_time": 1738718171.960537,
          "update_time": null,
          "content": {
            "content_type": "text",
            "parts": [
              "# Unified Theories of Information: A Cross-Disciplinary Review\n\n## Introduction  \n**Information** is a central concept across science and engineering, yet its definition and role can vary widely by discipline. Efforts to **unify the concept of information** seek a general theoretical framework applicable from physics and thermodynamics to biology and computer science. Such a framework should be **mathematically rigorous** and enable **prediction**, rather than being merely metaphorical or domain-specific. This review surveys historical and ongoing attempts to develop a **general theory of information**, highlighting key schools of thought. We focus on approaches with broad applicability and mathematical foundations \u2013 from Shannon\u2019s communication theory and algorithmic complexity, to statistical measures, physical entropy, biological information, and recent interdisciplinary theories. Each section presents the core ideas (with representative equations or models) and demonstrates how they contribute to a unified understanding of information. Citations are provided for at least 30 sources spanning these diverse perspectives.\n\n## Shannon\u2019s Classical Information Theory  \nClaude E. **Shannon\u2019s** formulation of information theory (1948) is a cornerstone for any unified concept of information. In Shannon\u2019s theory, information is defined in terms of **uncertainty reduction**. Given a random variable or a message with outcomes having probabilities \\(p_i\\), Shannon defined the **entropy** (information content) as:  \n\n\\[ H = -\\sum_{i} p_i \\log p_i, \\]  \n\nwhich measures the average \u201csurprise\u201d or uncertainty of the outcome\u301010\u2020L33-L37\u3011. This quantity (often in bits when log base 2 is used) is the fundamental unit of information. Intuitively, *information is the resolution of uncertainty*\u30109\u2020L299-L306\u3011 \u2013 if an event is very uncertain (many possible outcomes), learning its outcome yields a lot of information (high entropy); if it\u2019s almost predictable, the information gain is low. Shannon\u2019s entropy has **mathematical rigor** \u2013 it is derived from axioms requiring any reasonable information measure to behave consistently (e.g. additivity for independent events).  \n\nShannon\u2019s theory was first developed for **communication systems**: how to encode, transmit, and decode messages efficiently and reliably over noisy channels\u30109\u2020L259-L268\u3011\u30109\u2020L271-L275\u3011. However, its impact has been **far broader**. The same entropy formula appears in **thermal physics**, **statistics**, **linguistics**, **neurobiology**, and even in analyses of **black holes**\u30109\u2020L283-L291\u3011. This broad applicability is a hint that Shannon\u2019s definition captures a fundamental aspect of information across domains. Shannon also defined measures like **mutual information** (the information shared between variables) and derived results such as the **channel capacity** theorem, which quantitatively predicts the maximum error-free data rate possible over a noisy channel. These results show the *predictive power* of the theory \u2013 e.g. the Shannon\u2013Hartley law \\(C = B \\log_2(1 + S/N)\\) predicts communication rates from signal-to-noise ratios.  \n\nImportantly, Shannon\u2019s entropy is mathematically analogous to **Boltzmann-Gibbs entropy** in thermodynamics (differing by a constant \\(k_B\\)). Shannon himself, on von Neumann\u2019s advice, even called his measure \u201centropy.\u201d This led to decades of cross-disciplinary discussion about whether Shannon\u2019s \u201cinformation entropy\u201d is the same as thermodynamic entropy. While they are conceptually distinct in context, Shannon\u2019s work provided a template for interpreting entropy as *missing information* about a system\u2019s state\u30107\u2020L493-L500\u3011. **E. T. Jaynes** later leveraged this connection (see below), but already Shannon\u2019s theory offered a general quantitative tool. It is **domain-agnostic** \u2013 applicable to any system that can be described by probabilities \u2013 which is why it has been a foundation for attempts to unify information across fields. Shannon\u2019s framework is *predictive* in that it enables precise calculation of uncertainties, communication rates, and compression limits, making it a rigorous baseline for information measures.  \n\n## Algorithmic Information and Complexity  \nAnother major theoretical framework is **algorithmic information theory**, developed by **Andrey Kolmogorov**, **Ray Solomonoff**, **Gregory Chaitin**, and others in the 1960s. Where Shannon\u2019s entropy deals with probabilistic ensembles of messages, **Kolmogorov complexity** deals with individual objects (strings of data) and measures information as the length of the **shortest program** that can produce that object on a universal computer\u301012\u2020L177-L185\u3011\u301012\u2020L193-L197\u3011. Formally, the Kolmogorov complexity \\(K(x)\\) of a string \\(x\\) is defined as:  \n\n\\[ K(x) \\;=\\; \\min_{p:\\; U(p)=x} \\, \\ell(p), \\]  \n\nthe length \\(\\ell(p)\\) of the shortest binary program \\(p\\) that outputs \\(x\\) on a universal reference computer \\(U\\). In simple terms, \\(K(x)\\) measures how *compressible* or *simple* \\(x\\) is\u301012\u2020L176-L184\u3011. A random-looking string has high complexity (no short description), while a highly ordered or patterned string has low complexity (it can be generated by a short algorithm). Kolmogorov complexity provides a **mathematically rigorous** notion of *information content* that generalizes Shannon\u2019s: it doesn\u2019t require probabilistic assumptions and instead captures the inherent complexity of individual objects or datasets. \n\nAlthough \\(K(x)\\) is uncomputable in general (no algorithm can compute the exact complexity for all \\(x\\) due to G\u00f6del\u2019s and Turing\u2019s results\u301012\u2020L185-L194\u3011), it serves as an *idealized* yardstick. It has broad conceptual applicability \u2013 for example, it can be used to define **randomness** (a string is algorithmically random if no program much shorter than the string can produce it). It also underpins **Occam\u2019s razor** in model selection: simpler explanations have lower Kolmogorov complexity. In fact, **Solomonoff\u2019s theory of inductive inference** uses algorithmic information as a formal basis for **prediction**. Solomonoff showed that the best possible predictive model for observed data is essentially the **shortest program that reproduces the data**\u301014\u2020L165-L173\u3011. His method assigns higher a priori weight to simple (short description) hypotheses \u2013 encapsulating Occam\u2019s razor \u2013 and updates probabilities with Bayes\u2019 rule. This **Solomonoff induction** gives a theoretical **universal predictor** by considering all possible algorithms that could generate the data\u301014\u2020L175-L183\u3011. It proves that under certain assumptions, the optimal way to predict future data is to use a *universal prior* that favors shorter descriptions\u301014\u2020L165-L173\u3011\u301014\u2020L175-L183\u3011. In essence, Solomonoff\u2019s result says *the most predictive theory is the one with the shortest algorithmic information* \u2013 a profound link between simplicity and predictive power. (While incomputable in exact form, this idea has influenced practical criteria like the **Minimum Description Length (MDL)** principle, which uses data compression as a proxy for balancing model complexity and fit.)\n\nAlgorithmic information theory thus attempts to **unify information and prediction** in a very general setting: *any* process that produces data can be thought of as computation, and the information content of the data is tied to the length of the shortest computational description. This has implications in computer science (e.g. defining randomness), in physics (e.g. identifying simplicity in physical laws), and even in biology (genomes can be seen as strings \u2013 one can ask how complex DNA sequences are in the Kolmogorov sense). It provides a rigorous scaffold for a general theory of information, complementing Shannon\u2019s probabilistic approach with an *algorithmic* one. Together, these two views (probabilistic entropy and algorithmic complexity) are often considered the twin pillars of information theory.\n\n## Statistical and Geometrical Information Measures  \nBeyond Shannon\u2019s entropy and algorithmic complexity, information can also be framed in terms of **statistical inference**. One key concept here is **Fisher information**, introduced by statistician Ronald Fisher in the 1920s. Fisher information measures how much a random variable \\(X\\) (or an experiment) tells us about an unknown parameter \\(\\theta\\) upon which the probability of \\(X\\) depends\u301016\u2020L164-L172\u3011. Formally, if \\(f(x;\\theta)\\) is the probability density of data \\(X\\) given parameter \\(\\theta\\), the Fisher information \\(I(\\theta)\\) is defined as:  \n\n\\[ I(\\theta) = \\mathbb{E}\\!\\Bigg[ \\Big(\\frac{\\partial}{\\partial \\theta} \\ln f(X;\\,\\theta)\\Big)^2 \\Bigg], \\]  \n\nthe expected squared sensitivity of the log-likelihood to changes in \\(\\theta\\). Intuitively, if the distribution of \\(X\\) shifts a lot when \\(\\theta\\) changes (sharply peaked likelihood), then the data carry a lot of information about \\(\\theta\\)\u301016\u2020L197-L205\u3011. Fisher information has a clear predictive role: it sets a lower bound on parameter estimation error via the **Cram\u00e9r\u2013Rao bound** (the variance of any unbiased estimator of \\(\\theta\\) is at least \\(1/[N\\,I(\\theta)]\\) for \\(N\\) samples). This makes it a cornerstone of **experimental design** \u2013 one can compute Fisher information to decide how much data or which experimental setup would yield the most information about parameters. \n\nIn the quest for a general information theory, Fisher information provides a link between information and the **dynamics of estimation**. It has been applied not only in classical statistics but also in **physics**. Notably, attempts have been made to derive fundamental physical laws from an information principle by extremizing Fisher information. For example, Frieden and others have suggested that physical fields and laws (from quantum mechanics to relativity) can be obtained by assuming nature **maximizes Fisher information** under given constraints\u301016\u2020L187-L190\u3011. In such a view, the stable states or equations of motion of a system are those that carry the most information about the parameters defining those states, an intriguing cross-disciplinary hypothesis.\n\nAnother statistical measure is the **Kullback\u2013Leibler divergence** (KL divergence), also called **relative entropy**. This measures the information gained when one updates from a prior distribution to a posterior distribution. While we won\u2019t delve deep, KL divergence is fundamental in Bayesian inference and machine learning, quantifying how much a posterior distribution \u201csurprises\u201d us relative to a prior. Methods like **maximum likelihood** or **Bayesian updating** can be seen as information-processing: data makes us prefer distributions that encode lower KL divergence (less \u201csurprise\u201d). The widely used **Akaike Information Criterion (AIC)** for model selection is directly derived from an information measure (essentially, it estimates the KL divergence of a model from truth plus a complexity penalty). These illustrate that in statistics and machine learning, information concepts guide how we choose predictive models and update knowledge.\n\nIn addition, a modern branch known as **information geometry** merges information theory with differential geometry. Pioneered by C.R. **Rao** and **Shun\u2019ichi Amari**, information geometry treats families of probability distributions as geometric manifolds\u301048\u2020L130-L138\u3011. In this framework, the **Fisher information metric** plays the role of a Riemannian metric on the space of probability distributions\u301048\u2020L146-L154\u3011. Two distributions that can be easily distinguished by data (high information difference) are \u201cfar apart\u201d in this geometry. Information geometry has found uses in statistical physics, machine learning and even quantum theory, providing a unifying geometric language. It reveals deep structural principles \u2013 for instance, the fact that exponential families of distributions form flat manifolds under the appropriate connection relates to the entropy maximization property of those distributions. While somewhat abstract, information geometry reinforces the unity of information concepts by showing that whether one speaks of statistical estimation or thermodynamic ensembles, there is an underlying geometry governed by information. It is another tool indicating that *information is a fundamental quantity that can be treated in a unified way*, cutting across probability, statistics, and physics.\n\n## Physical and Thermodynamic Information  \nPhysics has long grappled with the relationship between **information and entropy**. In thermodynamics, **Boltzmann** and **Gibbs** defined entropy as \\( S = k_B \\ln \\Omega \\), where \\(\\Omega\\) is the number of microstates consistent with the macrostate of a system. This is essentially a count of missing information about which exact microstate the system is in. Decades later, Shannon\u2019s entropy formula was recognized to be mathematically identical in form (with \\(k_B\\) and a log base conversion) to the Boltzmann entropy. This realization prompted E. T. **Jaynes** in 1957 to propose the **Principle of Maximum Entropy (MaxEnt)**: when we have limited information about a system, the probability distribution that best represents our knowledge is the one with maximum Shannon entropy consistent with the known constraints\u301045\u2020L99-L107\u3011\u301045\u2020L107-L115\u3011. Jaynes showed that **statistical mechanics can be derived as a problem of statistical inference**\u301045\u2020L99-L107\u3011. For example, if all we know is the average energy of a system, the MaxEnt distribution is the Boltzmann (exponential) distribution \u2013 exactly the result from traditional physics. In other words, the thermodynamic entropy and Shannon\u2019s information entropy follow the *same underlying logic*\u301045\u2020L99-L107\u3011. This was a landmark in unifying information with physics: it reframed thermodynamics as *information theory about molecular states*. Jaynes\u2019 MaxEnt method has since been used in countless fields (from physics to image processing) to infer probability distributions in an unbiased way, illustrating broad applicability. His work cemented the view that **entropy is information**, and thermodynamic entropy increase (the Second Law) corresponds to the loss of information about a system\u2019s detailed state. Not everyone agreed on equating these entropies directly, but the formal correspondence is undeniable. For example, one can derive thermodynamic entropy as a special case of Shannon\u2019s measure for particular probability distributions\u30107\u2020L455-L463\u3011\u30107\u2020L493-L500\u3011. This insight helps unify physical and informational descriptions of systems \u2013 a hot gas\u2019s entropy can be seen as our uncertainty about particle positions and velocities\u30107\u2020L456-L464\u3011.\n\nThe famous **Maxwell\u2019s demon paradox** (1867) sharpened the link between information and thermodynamics. A \u201cdemon\u201d that acquires information about individual molecules (e.g. their speeds) could seemingly violate the second law by sorting fast and slow molecules to create a temperature difference (thus doing work for free). In 1929, **Le\u00f3 Szil\u00e1rd** analyzed a simple one-molecule engine and showed that the demon\u2019s act of **measurement** gains one bit of information, and that gaining or erasing that information has an unavoidable thermodynamic cost (resolving the paradox). This was later generalized by **Rolf Landauer** in 1961. **Landauer\u2019s principle** states that any logically *irreversible* operation (like erasing a bit or merging computation paths) must be accompanied by an increase in thermodynamic entropy of at least \\(k_B \\ln 2\\) per bit\u301021\u2020L42-L46\u3011\u301021\u2020L62-L68\u3011. Equivalently, erasing one bit of information dissipates a minimum energy of \\(k_B T \\ln 2\\) as heat (at temperature T)\u301021\u2020L62-L68\u3011. Landauer\u2019s principle \u2013 \u201c**information is physical**\u201d \u2013 provides a quantitative bridge between information theory and thermodynamics. It has been experimentally verified in recent years. It implies no computation that destroys information is free: the second law is saved because the demon, in erasing or recording information, pays a price in entropy. Landauer\u2019s and **Charles Bennett\u2019s** work in the 1980s on reversible computing further showed that if computation is done *reversibly* (no information loss), it in principle can be accomplished with no entropy increase\u301021\u2020L62-L68\u3011. This spurred research into physical limits of computing and tied the concept of information tightly into the laws of physics. \n\nAnother striking unification comes from **black hole physics**. In the 1970s, **Jacob Bekenstein** and **Stephen Hawking** discovered that black holes have entropy proportional to the area of their event horizon (Hawking later provided the coefficient \\(S = \\frac{k_B c^3}{4G\\hbar} A\\)). Bekenstein proposed a general **bound** on the information content of any region: for a system of radius \\(R\\) and energy \\(E\\), the entropy (in bits, or Shannon information) is bounded by \\(S \\le \\frac{2\\pi k_B R E}{\\hbar c \\ln 2}\\)\u301057\u2020L152-L160\u3011. In other words, there is a finite maximum information that can be packed into a given volume of space with given energy\u301057\u2020L139-L147\u3011. If you try to cram more information, you\u2019d collapse into a black hole. This **Bekenstein bound** (and related ideas like the **holographic principle**) suggests that physics fundamentally limits information density, tying geometric concepts (area, volume) to bits of information\u301057\u2020L139-L147\u3011. It reinforces that information is not an abstract human construct, but something embedded in physical law. In fact, the holographic principle posits that the *information content of a region of space is encoded on its boundary*, much like a hologram \u2013 an idea stemming from trying to unify quantum mechanics, gravity, and information theory.\n\nFurthermore, **quantum mechanics** itself has been rephrased in informational terms. **Quantum information theory** extends classical information concepts to quantum states. The **von Neumann entropy** \\(S(\\rho) = -\\mathrm{Tr}(\\rho \\log \\rho)\\) of a quantum state (density matrix \\(\\rho\\)) is the direct analog of Shannon entropy\u301027\u2020L145-L153\u3011. It measures the uncertainty or information content of a quantum state, generalizing Gibbs\u2019 entropy to quantum ensembles\u301027\u2020L145-L153\u3011. Quantum entropy plays a role in quantum entanglement \u2013 e.g. the **mutual information** between two entangled particles quantifies their correlations and is nonzero even when separated (unlike classical independent bits)\u301026\u2020L13-L16\u3011. Concepts like **quantum channel capacity** and **qubits** generalize Shannon\u2019s classical theory. All these demonstrate that as physics has progressed, it increasingly speaks the language of information. Wheeler\u2019s famous motto \u201c**It from bit**\u201d captures this: the idea that physical things (\u201cit\u201d) ultimately derive from information (\u201cbit\u201d). We will return to Wheeler shortly.\n\nIn summary, the laws of thermodynamics and quantum mechanics themselves seem to involve information at a fundamental level. Entropy in thermodynamics is essentially information about microstates; the second law implies a growth of missing information (in a closed system). The limits of computation and storage are dictated by physics (Landauer\u2019s bound, Bekenstein\u2019s bound). These links provide strong support for a unified theory: *the same mathematical measures (entropy, information) appear in both information science and physical science*, enabling predictions like heat dissipation per logic operation or information capacity of physical systems. They suggest that **information is a conserved or at least constrained quantity in the physical world**, much like energy, tying together previously separate domains.\n\n## Cybernetics and Control Theory Approaches  \nIn the 1940s and 50s, parallel to Shannon\u2019s work, scientists like **Norbert Wiener** and **W. Ross Ashby** were developing **cybernetics** \u2013 the study of control and communication in animals and machines. Cybernetics explicitly tried to create a *universal language* to describe information flow in systems, whether mechanical, electronic, or biological\u301035\u2020L136-L144\u3011\u301035\u2020L148-L157\u3011. Wiener defined cybernetics as the science of \u201ccontrol and communication\u201d and emphasized that *information is a distinct fundamental quantity*. He famously stated, *\u201cInformation is information, not matter or energy. No materialism which does not admit this can survive at the present day.\u201d*\u301039\u2020L1-L4\u3011. This quote\u301039\u2020L1-L4\u3011 highlights that by 1948, information was being considered on par with matter and energy in importance \u2013 a clear push toward a unified concept. \n\nWiener\u2019s work tied closely with Shannon\u2019s: he looked at **signal processing, feedback loops, and messages** in systems. In control theory, *information* is what allows a regulator to reduce uncertainty about a system\u2019s state and correct its behavior. Wiener and colleagues applied information measures to problems like predicting time series (the Wiener filter) \u2013 an early example of information-based *prediction theory*. He also introduced the idea of **negative feedback** in control systems, showing mathematically how information about the difference between desired and actual state can be used to drive the system toward goals. In doing so, he laid groundwork for analyzing machines and organisms with the same principles. For example, a thermostat and homeostasis in the body can both be seen as systems that use information (temperature or internal signals) to maintain stability.\n\n**Ashby\u2019s** contribution, the **Law of Requisite Variety**, is another early attempt to quantify a general principle of information for control. Stated simply, *\u201conly variety can destroy variety\u201d*\u301036\u2020L1-L4\u3011. \u201cVariety\u201d here means the number of possible states of a system (i.e. the information capacity, often measured in bits of entropy). Ashby\u2019s law says that for a controller to effectively regulate a system disturbed by some environment, the controller must be able to produce at least as many states as the environment produces disturbances\u301036\u2020L1-L4\u3011. In other words, a regulator needs an internal model or repertoire of actions with enough informational complexity to counteract the uncertainty (variety) in its inputs. Mathematically, one can relate variety to Shannon entropy; the law becomes an information inequality: the controller\u2019s channel capacity must exceed the entropy of disturbances. This concept has been applied in fields from neuroscience (brain must have models matching the complexity of stimuli) to management (organizations need as much diversity of responses as the challenges they face). It\u2019s a beautiful example of a domain-specific rule (in control theory) that actually carries an informational generality. By quantifying the balance of information between a system and its controller, Ashby was effectively unifying biology (e.g. how organisms adapt) and engineering under one principle. Modern interpretations see it as an early statement about necessary **information transfer** for causation: to control X, you need information about X. \n\nCybernetics also introduced terms like **feedback**, **signal-to-noise ratio**, and **redundancy** into biology and social sciences, all rooted in information theory. For instance, **Shannon\u2019s entropy** was used by **Ecologist H. T. Odum** and others in the 1950s to measure diversity in ecosystems (viewing an ecosystem as an information channel of energy). **Gregory Bateson**, a cybernetic thinker, defined information memorably as \u201ca difference that makes a difference,\u201d capturing the idea that only when a distinction leads to some change (control action, cognitive update) does it count as information. While Bateson\u2019s phrase is qualitative, it resonates with the more formal notion that *information is what can induce an observable change or reduction of uncertainty*.\n\nIn short, cybernetics contributed to a unified theory by **extending information concepts to goal-directed systems**. It made clear that *information flow is what links perception to action* in any system. The mathematics of feedback control (differential equations with terms for error signals) and the logic of decision-making (yes/no decisions as bits) both rely on managing information. Cybernetics thereby provided early **predictive models** for how systems behave (e.g. stability criteria) using information-theoretic language. These ideas were foundational for later fields like **informatics, computer control, robotics, and systems biology**, all of which assume that information (and its processing) is the unifying currency of their analysis.\n\n## Information in Biology and Life  \nBiology is perhaps the most challenging domain for a unified information theory, because it involves complex, evolving systems. Yet, biological systems scream \u201cinformation\u201d at multiple levels \u2013 from the digital code of DNA to the neural signals in brains. Attempts to formalize biological information date back at least to **Erwin Schr\u00f6dinger** in 1944. In *What is Life?*, Schr\u00f6dinger famously argued that organisms maintain order by **feeding on \u201cnegative entropy\u201d** from their environment\u301043\u2020L270-L278\u3011. By importing energy (like food or sunlight) and exporting entropy (waste heat), living systems sustain low internal entropy (high order) against the natural tendency to disorder. Schr\u00f6dinger didn\u2019t give a precise equation for \u201cnegative entropy,\u201d but his idea framed life in thermodynamic information terms: life is extraordinary because it locally *decreases entropy*, something only possible by **drawing information/ordering influences from outside**\u301043\u2020L270-L278\u3011. This concept was inspirational \u2013 it suggested that the secrets of biology might be understood by how organisms handle information and entropy. In modern terms, one could say organisms perform information processing to keep themselves far from thermodynamic equilibrium (a view developed further by researchers like Jeremy England with the idea of dissipation-driven self-organization). Schr\u00f6dinger\u2019s negentropy concept set the stage for thinking of DNA, metabolism, etc., in informational terms.\n\nThe discovery of the **genetic code** in the 1950s confirmed that life has a digital foundation. DNA is often called the \u201cbook of life\u201d \u2013 it encodes hereditary information in a sequence of nucleotide \u201cletters\u201d (A, C, G, T). This is remarkably analogous to a linear message or program. In fact, molecular biologists quickly borrowed Shannon\u2019s theories: one can talk about the **information content of a gene** (in bits), the **mutual information** between DNA and RNA, etc. The genome can be viewed as a message transmitted through generations, subject to noise (mutations). Pioneers like **Hubert Yockey** applied information theory directly to biology. Yockey argued that the genome and proteome form a coding system isomorphic to a communication system, and that biology could be quantitatively understood with concepts like Shannon entropy and coding theory\u301051\u2020L5-L13\u3011. As one source summarizes, *the genetic information system, being linear and digital, resembles the algorithmic language of computers \u2013 allowing the use of information theory to measure the information in genetic sequences*\u301051\u2020L5-L13\u3011. For example, one can calculate the Shannon entropy of binding sites on DNA to see how specific a molecular signal is, or use algorithmic complexity to analyze genome structure. These approaches have concrete predictive power: information theory can predict, say, how many mutations can occur before a gene\u2019s message is corrupted, or how much \u201cmutual information\u201d is shared between DNA and the protein it encodes (which relates to error-correcting properties of the genetic code). Bioinformatics today routinely uses these measures to find functional sites in genomes or to compare sequences.\n\nOn a larger scale, **evolution itself can be seen as an information process**: populations \u201clearn\u201d about their environment over generations, with DNA as the memory. Concepts like **information gain** have been used to quantify the increase of adaptive information in a gene pool over time. There are measures of **biotic information** attempting to capture the complexity or \u201cknowledge\u201d embodied in organisms (for instance, the idea of genetic information increasing through natural selection). While more philosophical, these ideas treat Darwinian evolution akin to a search algorithm accumulating information about survival strategies in the environment.\n\nNeuroscience is another area: the brain is often likened to an information-processing system. Neurons communicate via spikes, and theories like **neural coding** explicitly ask how much information (in bits) neural signals carry about stimuli. The field of **complex systems** in biology (ecologies, physiological systems) also frequently uses information metrics to describe organization, complexity, and network dynamics. For example, **information entropy** can measure the diversity of an ecosystem\u2019s state, or **mutual information** can indicate connectivity between parts of a biological network (who influences whom).\n\nA recent development bridging biology and physics is **Assembly Theory (2021-2023)** by Sara Walker, Lee Cronin, and collaborators. Assembly theory attempts to define a quantitative measure of complexity that works for molecules, living systems, and even technological objects. It introduces the idea of an **\u201cassembly index\u201d** for a molecule, defined by the minimal number of steps (bond formation operations) required to build that molecule from basic building blocks\u301028\u2020L28-L36\u3011. High assembly index (many steps) indicates the object is complex and likely produced by an evolutionary process rather than random chance. In their 2023 paper, the team formalized a quantity called **\u201cassembly\u201d** for a collection of objects, which measures \u201chow much selection is required\u201d to produce those objects given their abundances and assembly indices\u301028\u2020L36-L44\u3011. This framework unifies information from chemistry (how to build molecules) with evolutionary theory (selection processes) to quantify complexity in a single scale. As Walker explains, *\u201cAssembly theory provides a completely new lens for looking at physics, chemistry and biology as different perspectives of the same underlying reality.\u201d*\u301028\u2020L40-L48\u3011. By treating **memory** (records of past assembly steps) as a physical resource, assembly theory links information to the emergence of complexity. It\u2019s backed by experimental aspects (they can measure assembly index in the lab) and has predictive aims like distinguishing life-derived molecules from random ones\u301028\u2020L21-L29\u3011\u301028\u2020L58-L64\u3011. Assembly theory is very much an **ongoing research** front, but it exemplifies the drive towards a fundamental theory of information that can identify \u201clife\u201d or \u201ctechnology\u201d by their information-theoretic signatures. It attempts to generalize the concept of information to *include historical context and causation* (selection requires information about past successful configurations), pushing beyond static entropy measures.\n\nAnother influential cross-disciplinary theory is the **Free Energy Principle (FEP)** in neuroscience and biology, formulated by **Karl Friston** (2000s). The FEP posits that living systems (particularly brains) maintain their order by minimizing their **variational free energy**, which is an information-theoretic quantity equivalent to the difference between a system\u2019s model of the world and the actual sensory input (a form of Kullback\u2013Leibler divergence). In simple terms, organisms strive to minimize \u201csurprise\u201d or prediction error. Initially proposed as a unifying theory of the brain (viewing the brain as an \u201cinference machine\u201d that predicts sensations)\u301029\u2020L5-L8\u3011, the free energy principle has been expanded as a candidate **unifying theory of life** and cognition\u301030\u2020L162-L170\u3011. It claims that any self-organizing system that resists decay (maintains homeostasis) can be described as minimizing free energy, which in practice means it is actively predicting and adjusting to its environment. This connects **thermodynamics, Bayesian inference, and biology** in one framework. The mathematics involve recursive Bayesian updates and ensemble densities, but conceptually it says life is always performing a kind of **predictive modeling** to keep itself going. A recent review describes FEP as \u201ca formal model of neuronal processes\u2026widely recognised in neuroscience as a unifying theory of the brain and biobehavior,\u201d now extended \u201cbeyond the brain to explain the dynamics of living systems, and their unique capacity to avoid decay\u201d\u301030\u2020L162-L170\u3011. Thus, it attempts to explain at once metabolic regulation, movement, perception, and even high-level behavior as outcomes of one principle: minimize informational free energy (i.e. align internal beliefs with sensory data to reduce surprise). The FEP is still being tested and debated, but if validated, it could serve as a *grand unifying theory of information in biology*, showing how even entropy-defying behavior (life) is in accordance with information-theoretic optimization. Notably, it provides equations linking **entropy production, information gain, and adaptive behavior**, and offers a **predictive framework** for everything from single-cell chemotaxis to human psychological dynamics under one umbrella.\n\nIn summary, biology has increasingly been interpreted through information. From genetic code as information storage to ecosystems as information networks, from organismal complexity as accumulated information (negentropy) to brains as predictive information processors, these perspectives illustrate theoretical attempts to generalize information\u2019s role in living systems. They use mathematics from information theory to make predictions (e.g., mutation rates, neural bit rates, population learning curves). This not only helps unify biology with physics/CS concepts, but also yields practical predictive models in epidemiology, neurobiology, and evolutionary theory. The challenge is that biological systems are very complex and open-ended, but frameworks like assembly theory and the free energy principle show that the pursuit of a fundamental information theory of life is active and yielding testable hypotheses.\n\n## Toward Unified and Fundamental Theories  \nBuilding on all the above, many researchers have sought an even deeper unification \u2013 treating information as perhaps the *most fundamental* aspect of reality. One of the earliest and most famous champions of this idea was physicist **John Archibald Wheeler**. In the 1980s, Wheeler coined the phrase \u201c**It from bit**,\u201d suggesting that physical phenomena (\u201cit\u201d) ultimately derive from binary choices (\u201cbits\u201d). In his words, *\u201cEvery it \u2013 every particle, every field of force, even the space-time continuum itself \u2013 derives its function, its meaning, its very existence entirely\u2026 from the apparatus-elicited answers to yes-or-no questions, binary choices, bits.\u201d*\u301025\u2020L53-L60\u3011. This dramatic statement\u301025\u2020L53-L60\u3011 captures the essence of digital physics: the conjecture that at the most fundamental level, the universe is made of information. Wheeler\u2019s own work in quantum physics (e.g., the delayed-choice experiment) had shown how the act of observation (acquiring information) seemed to affect reality. \u201cIt from bit\u201d was his philosophical synthesis: the universe might fundamentally be a grand information processing system, with physical laws emerging from informational rules. While this remains an interpretation, not established fact, it has inspired research into quantum information theory, computational physics, and theories of quantum gravity that put information at the forefront.\n\nFollowing that line, **Seth Lloyd** in his 2006 book *Programming the Universe* explicitly argued that *the Universe is a quantum computer*. Every interaction between particles is like a logic gate computing an outcome. In this view, the evolution of the universe is literally a computation that processes information (the initial conditions) into new information states\u301053\u2020L6-L13\u3011\u301053\u2020L15-L18\u3011. Lloyd pointed out that understanding physics might advance by viewing entropy not just as \u201cdisorder\u201d but as information \u2013 for instance, reframing the Second Law as \u201ccomputing new states\u201d\u301055\u2020L1-L4\u3011. He also estimated the computational capacity of the universe (~10^120 ops on 10^90 bits, famously). The key takeaway is that if the universe is a computer, then *information is the most basic currency of reality*, and physical laws are algorithms. This is a bold unification attempt bridging physics and computer science. It hasn\u2019t (yet) led to a new physical theory that outperforms current ones, but it aligns with developments in quantum computing and black hole thermodynamics (e.g., the idea that black hole evaporation is like information processing). Lloyd\u2019s and others\u2019 work also connects with the **holographic principle** \u2013 the notion that the maximum information in a volume is proportional to its surface area suggests spacetime itself has an informational explanation. Indeed, some approaches to quantum gravity (like Erik Verlinde\u2019s entropic gravity) derive gravity as an emergent phenomenon from information/entropy considerations. These remain speculative but show the trend of information-centric thinking in fundamental physics.\n\nIn the realm of philosophy and information science, there have been attempts at a **Unified Theory of Information (UTI)** by scholars like Wolfgang Hofkirchner and Luciano Floridi. Hofkirchner\u2019s approach, for example, links information to self-organization, trying to integrate the various \u201clevels\u201d (from physical signals to biological and social meaning) into one framework of emergent information\u30104\u2020L5-L13\u3011. Floridi\u2019s *Philosophy of Information* similarly seeks a coherent account of information, defining it as \u201cdata + meaning\u201d in some contexts and exploring its role in reality and knowledge\u301037\u2020L9-L17\u3011. While these are more conceptual than mathematical, they provide clarity on different **types of information (syntactic, semantic, pragmatic)** and stress that any unified theory must account not just for raw bits, but for *meaningful* information when observers and cognition are involved. For instance, Floridi distinguishes between merely quantified information (Shannon-style) and information that has semantic value (relevant in biology or AI). Some emerging efforts, like **Kolchinsky and Wolpert (2018)**, attempt to marry semantic information with thermodynamics, defining \u201csemantic information\u201d as information that is causally useful to a system (linking it to work extraction). This brings meaning into the fold of physical information theory.\n\nFinally, it\u2019s worth noting that **interdisciplinary research programs** continue to sprout. For example, the Santa Fe Institute has long studied complexity and information in systems, devising measures like **effective complexity** (Gell-Mann and Lloyd) which splits information into regularities versus randomness, and **logical depth** (Charles Bennett) which measures the computational work to generate an object (connecting information and thermodynamic \u201ceffort\u201d). These are attempts to refine our understanding of what *meaningful* or *useful* information is, beyond raw entropy. While not universally accepted, they enrich the toolkit for a future unified theory.\n\nTo illustrate an ongoing attempt: A 2023 article in *Royal Society A* proposes a \u201cgrand unified theory\u201d connecting physics, life, information, and cognition by introducing a **fourth law of thermodynamics** (maximum entropy production) and concepts from systems theory\u30103\u2020L234-L243\u3011\u30103\u2020L244-L252\u3011. It argues that by extending thermodynamics to include information-producing processes (like self-replication that maximizes entropy production in the environment), one can dissolve the perceived gap between inanimate matter and life/mind\u30103\u2020L242-L250\u3011. While highly theoretical, it shows the ambition to incorporate **information principles (like entropy production or self-referential information in autopoietic systems)** directly into the laws of physics.\n\nIn summary, the pursuit of a **broadly applicable, fundamental theory of information** is very much alive. From \u201cit from bit\u201d in physics, to assembly theory in biology, to the free energy principle in neuroscience \u2013 these frameworks prioritize information as the key to understanding and predicting system behavior across scales. They are built on mathematical formalisms (whether it\u2019s quantum circuits, assembly pathways, or variational Bayesian math) and aim to be testable. They also avoid being merely abstract by tying back to real-world observations (e.g., testing assembly theory on chemical datasets, or using FEP to model neuronal response). While no single theory yet unites all aspects of information, the convergence of ideas is notable. **Shannon\u2019s entropy** remains a common thread, appearing in equations from black hole entropy to model selection criteria. **Prediction** is another common theme: virtually all these frameworks involve reducing uncertainty about the future \u2013 whether it\u2019s a brain minimizing surprise, a model maximizing likelihood, or a physical system reaching equilibrium. This underscores the idea that *information is fundamentally about predicting the world around us or the system\u2019s own state*.\n\n## Conclusion  \nAcross physics, computing, biology, and beyond, the concept of information has been sharpened into a variety of **mathematical frameworks** that often echo and reinforce one another. Shannon\u2019s probabilistic entropy, Kolmogorov\u2019s algorithmic complexity, Fisher\u2019s statistical information, and Boltzmann\u2019s thermodynamic entropy all provide lenses through which \u201cinformation\u201d can be quantified and used to make predictions. These lenses are remarkably interrelated \u2013 frequently essentially the same equations appear in different guises. Newer theories like assembly theory, the free energy principle, and digital physics speculations strive to bridge the remaining gaps, indicating that a **unified theory of information** is on the horizon. Such a theory would treat information as a continuum from fundamental physics (bits in particles and fields) up through life (genes and neural bits) to intelligence (knowledge and semantic information), governed by consistent principles. \n\nWhile challenges remain \u2013 for instance, incorporating **semantics and meaning** (important in biology and cognition) into mathematical formulations, or reconciling quantum information with classical information at cosmic scales \u2013 the progress so far is encouraging. The approaches reviewed here demonstrate broad applicability: they are not confined to a single domain but have been successfully applied to multiple fields (e.g., Shannon entropy in physics and linguistics\u30109\u2020L283-L291\u3011, or algorithmic complexity in genetics and art analysis). They also possess **predictive power**: from predicting communication reliability, to inferring molecular structures, to explaining why organisms behave as they do. By prioritizing formalism and concrete measures (bits, entropy, algorithms, likelihoods), these theories avoid hand-wavy notions and instead enable calculations and tests. In doing so, they pave the way for a future where \u201cinformation\u201d is as universal and rigorously understood as \u201cenergy\u201d is today \u2013 a true scientific unifier across natural and artificial systems.\n\nUltimately, the unification of the concept of information teaches us that **knowledge about any system** \u2013 be it an atom, a cell, or a society \u2013 can be quantified and analyzed with common methods. It highlights the deep truth that *to understand something is to grasp the information it embodies and conveys*. The continued cross-pollination of ideas from physics, computer science, biology, and philosophy will likely yield an even more cohesive theory, one that not only generalizes across domains but also enriches each domain\u2019s ability to predict and control the phenomena of interest. The work reviewed here forms the multifaceted foundation of that emerging unified theory of information\u301057\u2020L139-L147\u3011\u301025\u2020L53-L60\u3011, reminding us that at a fundamental level, we live in an **informational universe**. \n\n"
            ]
          },
          "status": "finished_successfully",
          "end_turn": true,
          "weight": 1.0,
          "metadata": {
            "citations": [
              {
                "start_ix": 1636,
                "end_ix": 1648,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Shannon\u2019s entropy",
                  "url": "https://planetmath.org/shannonsentropy#:~:text=The%20quantity%20is%20interpreted%20as,actual%20outcome%20of%20is%20revealed",
                  "text": "The quantity is interpreted as the information content of the outcome , and is also called the Hartley information  of . Hence the Shannon\u2019s entropy is the average[Image 9: Mathworld][Image 10: Planetmath] amount of  7 contained in random variable , it is also the uncertainty removed after the actual outcome of is revealed.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 10,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 33,
                    "end_line_num": 37
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 1803,
                "end_ix": 1816,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Information theory - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Information_theory#:~:text=Information%20theory%20studies%20the%20transmission%2C,in%20spite%20of%20the%20channel",
                  "text": "Information theory studies the transmission, processing, extraction, and utilization of information . Abstractly, information can be thought of as the resolution of uncertainty. In the case of communication of information over a noisy channel, this abstract concept was formalized in 1948 by  168 in a paper entitled A Mathematical Theory of Communication, in which information is thought of as a set of possible messages, and the goal is to send these messages over a noisy channel, and to have the receiver reconstruct the message with low probability of error, in spite of the channel",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 9,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 299,
                    "end_line_num": 306
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 2340,
                "end_ix": 2353,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Information theory - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Information_theory#:~:text=established%20and%20formalized%20by%20Claude,3",
                  "text": "established and formalized by Claude Shannon  in the 1940s,[ 1 ] though early contributions were made in the 1920s through the works of  169 and Ralph Hartley . It is at the intersection of  171, mathematics ,  173, computer science ,  175, physics , and  177.[ 2 ][ 3 ]",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 9,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 259,
                    "end_line_num": 268
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 2353,
                "end_ix": 2366,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Information theory - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Information_theory#:~:text=important%20measures%20in%20information%20theory,theory%20%20and%20%20188",
                  "text": "important measures in information theory are mutual information ,  158, error exponents , and  152. Important sub-fields of information theory include source coding ,  186, algorithmic information theory  and  188.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 9,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 271,
                    "end_line_num": 275
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 2561,
                "end_ix": 2574,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Information theory - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Information_theory#:~:text=%5D,study%20of%20%20216%20outer",
                  "text": "][ 3 ] The theory has also found applications in other areas, including statistical inference ,[ 7 ]  198, neurobiology ,[ 8 ]  200,[ 9 ] signal processing ,[ 2 ]  202, the evolution[ 10 ] and function[ 11 ] of molecular codes (bioinformatics ),  204,[ 12 ] molecular dynamics ,[ 13 ]  206, quantum computing ,  208, intelligence gathering ,  210,[ 14 ] pattern recognition ,  212,[ 15 ] the analysis of music ,[ 16 ][ 17 ]  214,[ 18 ] imaging system design,[ 19 ] study of  216 outer",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 9,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 283,
                    "end_line_num": 291
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 3642,
                "end_ix": 3655,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Entropy, Shannon\u2019s Measure of Information and Boltzmann\u2019s H-Theorem",
                  "url": "https://www.mdpi.com/1099-4300/19/2/48#:~:text=In%201948%2C%20Shannon%20sought%20and,both%20information%20theory%20and%20thermodynamics",
                  "text": "In 1948, Shannon sought and found a remarkable measure of information, of uncertainty [1 , 82] and unlikelihood. It was not a measure of any information, not any uncertainty about any proposition, and not the unlikelihood about the occurrence of any event. However, because the quantity he found has the same mathematical form as the entropy in statistical mechanics, he called his measure, as allegedly suggested by von Neumann: \u201centropy\u201d. This proved to be a grievous mistake which had caused a great confusion in both information theory and thermodynamics.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 7,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 493,
                    "end_line_num": 500
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 4649,
                "end_ix": 4663,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Kolmogorov complexity - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Kolmogorov_complexity#:~:text=mathematics%20,generalization%20of%20classical%20information%20theory",
                  "text": "mathematics ), the Kolmogorov complexity of an object, such as a piece of text, is the length of a shortest  93 (in a predetermined programming language ) that produces the object as output. It is a measure of the  95 resources needed to specify the object, and is also known as algorithmic complexity, Solomonoff\u2013Kolmogorov\u2013Chaitin complexity, program-size complexity, descriptive complexity, or algorithmic entropy. It is named after Andrey Kolmogorov, who first published on the subject in 1963[ 1 ][ 2 ] and is a generalization of classical information theory.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 12,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 177,
                    "end_line_num": 185
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 4663,
                "end_ix": 4677,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Kolmogorov complexity - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Kolmogorov_complexity#:~:text=length%20of%20the%20ultimately%20compressed,3",
                  "text": "length of the ultimately compressed version of a file (i.e., anything which can be put in a computer). Formally, it is the length of a shortest program from which the file can be reconstructed. While Kolmogorov complexity is uncomputable, various approaches have been proposed and reviewed.[ 3 ]",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 12,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 193,
                    "end_line_num": 197
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 5005,
                "end_ix": 5019,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Kolmogorov complexity - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Kolmogorov_complexity#:~:text=In%20algorithmic%20information%20theory%20,generalization%20of%20classical%20information%20theory",
                  "text": "In algorithmic information theory  (a subfield of  91 and mathematics ), the Kolmogorov complexity of an object, such as a piece of text, is the length of a shortest  93 (in a predetermined programming language ) that produces the object as output. It is a measure of the  95 resources needed to specify the object, and is also known as algorithmic complexity, Solomonoff\u2013Kolmogorov\u2013Chaitin complexity, program-size complexity, descriptive complexity, or algorithmic entropy. It is named after Andrey Kolmogorov, who first published on the subject in 1963[ 1 ][ 2 ] and is a generalization of classical information theory.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 12,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 176,
                    "end_line_num": 184
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 5584,
                "end_ix": 5598,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Kolmogorov complexity - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Kolmogorov_complexity#:~:text=The%20notion%20of%20Kolmogorov%20complexity,of%20a%20shortest%20program%20from",
                  "text": "The notion of Kolmogorov complexity can be used to state and prove impossibility  results akin to  98, G\u00f6del's incompleteness theorem , and  100. In particular, no program P computing a lower bound for each text's Kolmogorov complexity can return a value essentially larger than P's own length (see section \u00a7 Chaitin's incompleteness theorem); hence no single program can compute the exact Kolmogorov complexity for infinitely many texts. Kolmogorov complexity is the length of the ultimately compressed version of a file (i.e., anything which can be put in a computer). Formally, it is the length of a shortest program from",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 12,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 185,
                    "end_line_num": 194
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 6203,
                "end_ix": 6217,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Solomonoff's theory of inductive inference - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference#:~:text=Solomonoff%27s%20theory%20of%20inductive%20inference,it%20encompasses%20%2067",
                  "text": "Solomonoff's theory of inductive inference proves that, under its common sense assumptions (axioms), the best possible scientific model is the shortest algorithm that generates the empirical data under consideration. In addition to the choice of data, other assumptions are that, to avoid the post-hoc fallacy, the programming language must be chosen prior to the data[ 1 ] and that the environment being observed is generated by an unknown algorithm. This is also called a theory of induction . Due to its basis in the dynamical ( 65) character of Algorithmic Information Theory , it encompasses  67.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 14,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 165,
                    "end_line_num": 173
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 6519,
                "end_ix": 6533,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Solomonoff's theory of inductive inference - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference#:~:text=theoretical%20computer%20science%20.,probability%20to%20any%20computable%20theory",
                  "text": "theoretical computer science .[ 2 ][ 3 ][ 4 ] In essence, Solomonoff's induction derives the  71 of any computable  theory, given a sequence of observed data. This posterior probability is derived from  73 and some universal prior, that is, a prior that assigns a positive probability to any computable theory.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 14,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 175,
                    "end_line_num": 183
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 6678,
                "end_ix": 6692,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Solomonoff's theory of inductive inference - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference#:~:text=Solomonoff%27s%20theory%20of%20inductive%20inference,it%20encompasses%20%2067",
                  "text": "Solomonoff's theory of inductive inference proves that, under its common sense assumptions (axioms), the best possible scientific model is the shortest algorithm that generates the empirical data under consideration. In addition to the choice of data, other assumptions are that, to avoid the post-hoc fallacy, the programming language must be chosen prior to the data[ 1 ] and that the environment being observed is generated by an unknown algorithm. This is also called a theory of induction . Due to its basis in the dynamical ( 65) character of Algorithmic Information Theory , it encompasses  67.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 14,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 165,
                    "end_line_num": 173
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 6692,
                "end_ix": 6706,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Solomonoff's theory of inductive inference - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference#:~:text=theoretical%20computer%20science%20.,probability%20to%20any%20computable%20theory",
                  "text": "theoretical computer science .[ 2 ][ 3 ][ 4 ] In essence, Solomonoff's induction derives the  71 of any computable  theory, given a sequence of observed data. This posterior probability is derived from  73 and some universal prior, that is, a prior that assigns a positive probability to any computable theory.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 14,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 175,
                    "end_line_num": 183
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 8370,
                "end_ix": 8384,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Fisher information - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Fisher_information#:~:text=In%20mathematical%20statistics%20%2C%20the,86%20of%20the%20observed%20information",
                  "text": "In mathematical statistics , the Fisher information is a way of measuring the amount of  82 that an observable random variable  X carries about an unknown parameter \u03b8 of a distribution that models X. Formally, it is the  84 of the score , or the  86 of the observed information.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 16,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 164,
                    "end_line_num": 172
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 8896,
                "end_ix": 8910,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Fisher information - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Fisher_information#:~:text=observable%20random%20variable%20%20,displaystyle%20%5Ctheta%20%7D%5D%2C%20it",
                  "text": "observable random variable  [Image 0: {\\displaystyle X}] carries about an unknown  105 [Image 1: {\\displaystyle \\theta }] upon which the probability of [Image 2: {\\displaystyle X}] depends. Let [Image 3: {\\displaystyle f(X;\\theta )}] be the probability density function  (or  107) for [Image 4: {\\displaystyle X}] conditioned on the value of [Image 5: {\\displaystyle \\theta }]. It describes the probability that we observe a given outcome of [Image 6: {\\displaystyle X}], given a known value of [Image 7: {\\displaystyle \\theta }]. If [Image 8: {\\displaystyle f}] is sharply peaked with respect to changes in [Image 9: {\\displaystyle \\theta }], it",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 16,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 197,
                    "end_line_num": 205
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 9902,
                "end_ix": 9916,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Fisher information - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Fisher_information#:~:text=Statistical%20systems%20of%20a%20scientific,nature%20of%20the%20system%20constraints",
                  "text": "Statistical systems of a scientific nature (physical, biological, etc.) whose likelihood functions obey shift invariance have been shown to obey maximum Fisher information.[ 4 ] The level of the maximum depends upon the nature of the system constraints.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 16,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 187,
                    "end_line_num": 190
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 11328,
                "end_ix": 11342,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Information geometry - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Information_geometry#:~:text=Information%20geometry%20is%20an%20interdisciplinary,points%20correspond%20to%20%2053",
                  "text": "Information geometry is an interdisciplinary field that applies the techniques of differential geometry  to study  49 and statistics . [ 1 ] It studies  51, which are Riemannian manifolds  whose points correspond to  53.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 48,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 130,
                    "end_line_num": 138
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 11476,
                "end_ix": 11490,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Information geometry - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Information_geometry#:~:text=Historically%2C%20information%20geometry%20can%20be,4",
                  "text": "Historically, information geometry can be traced back to the work of C. R. Rao , who was the first to treat the  57 as a Riemannian metric .[ 2 ][ 3 ] The modern theory is largely due to  59, whose work has been greatly influential on the development of the field.[ 4 ]",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 48,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 146,
                    "end_line_num": 154
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 13260,
                "end_ix": 13273,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Jaynes and the Principle of Maximum Entropy \u2014 SFI Press",
                  "url": "https://www.sfipress.org/14-jaynes-1957#:~:text=and%20information%20entropy%2C%20which%20he,introduces%20the%20%E2%80%9Cprinciple%20of%20maximum",
                  "text": "and information entropy, which he immediately identified as a significant shift in perspective. In this paper, he showed that entropy as expressed in the work of Boltzmann and Gibbs in the field of thermodynamics and Shannon entropy follow from the same underlying logic, thus establishing entropy as a general concept. Maximum entropy inference is introduced and in sections 3, 4, and 5 examples are given showing how the application of the principle of maximum entropy results in a conceptual shift in the approach to statistical mechanics. Jaynes proved that statistical mechanics can be interpreted in non-physical terms as statistical inference. In his 1957 paper he introduces the \u201cprinciple of maximum",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 45,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 99,
                    "end_line_num": 107
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 13273,
                "end_ix": 13287,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Jaynes and the Principle of Maximum Entropy \u2014 SFI Press",
                  "url": "https://www.sfipress.org/14-jaynes-1957#:~:text=statistical%20inference,by%20choosing%20the%20probability%20distribution",
                  "text": "statistical inference. In his 1957 paper he introduces the \u201cprinciple of maximum entropy\u201d and proposed the maximization of the entropy function H, subject to certain constraints, as a general principle of statistical inference applicable to a diverse range of areas. The principle has since proved particularly useful in providing the least biased probability distribution for a set of unknown prior probabilities. Jaynes\u2019s view is that assigning equal probabilities in situations of total ignorance is the least prejudiced choice. However, when ignorance is not total, inferences should be based on all and only the information available. This is assured by choosing the probability distribution",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 45,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 107,
                    "end_line_num": 115
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 13386,
                "end_ix": 13399,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Jaynes and the Principle of Maximum Entropy \u2014 SFI Press",
                  "url": "https://www.sfipress.org/14-jaynes-1957#:~:text=and%20information%20entropy%2C%20which%20he,introduces%20the%20%E2%80%9Cprinciple%20of%20maximum",
                  "text": "and information entropy, which he immediately identified as a significant shift in perspective. In this paper, he showed that entropy as expressed in the work of Boltzmann and Gibbs in the field of thermodynamics and Shannon entropy follow from the same underlying logic, thus establishing entropy as a general concept. Maximum entropy inference is introduced and in sections 3, 4, and 5 examples are given showing how the application of the principle of maximum entropy results in a conceptual shift in the approach to statistical mechanics. Jaynes proved that statistical mechanics can be interpreted in non-physical terms as statistical inference. In his 1957 paper he introduces the \u201cprinciple of maximum",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 45,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 99,
                    "end_line_num": 107
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 13689,
                "end_ix": 13702,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Jaynes and the Principle of Maximum Entropy \u2014 SFI Press",
                  "url": "https://www.sfipress.org/14-jaynes-1957#:~:text=and%20information%20entropy%2C%20which%20he,introduces%20the%20%E2%80%9Cprinciple%20of%20maximum",
                  "text": "and information entropy, which he immediately identified as a significant shift in perspective. In this paper, he showed that entropy as expressed in the work of Boltzmann and Gibbs in the field of thermodynamics and Shannon entropy follow from the same underlying logic, thus establishing entropy as a general concept. Maximum entropy inference is introduced and in sections 3, 4, and 5 examples are given showing how the application of the principle of maximum entropy results in a conceptual shift in the approach to statistical mechanics. Jaynes proved that statistical mechanics can be interpreted in non-physical terms as statistical inference. In his 1957 paper he introduces the \u201cprinciple of maximum",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 45,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 99,
                    "end_line_num": 107
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 14439,
                "end_ix": 14452,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Entropy, Shannon\u2019s Measure of Information and Boltzmann\u2019s H-Theorem",
                  "url": "https://www.mdpi.com/1099-4300/19/2/48#:~:text=Shannon%20Measure%20of%20Information%20,the%20Second%20Law%20of%20Thermodynamics",
                  "text": "Shannon Measure of Information (SMI) provides a solid and quantitative basis for the interpretation of the thermodynamic entropy. The entropy measures the uncertainty in the distribution of the locations and momenta of all the particles; as well as two corrections due to the uncertainty principle and the indistinguishability of the particles. Finally we show that the H-function as defined by Boltzmann is an SMI but not entropy. Therefore; much of what has been written on the H-theorem is irrelevant to entropy and the Second Law of Thermodynamics.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 7,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 455,
                    "end_line_num": 463
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 14452,
                "end_ix": 14465,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Entropy, Shannon\u2019s Measure of Information and Boltzmann\u2019s H-Theorem",
                  "url": "https://www.mdpi.com/1099-4300/19/2/48#:~:text=In%201948%2C%20Shannon%20sought%20and,both%20information%20theory%20and%20thermodynamics",
                  "text": "In 1948, Shannon sought and found a remarkable measure of information, of uncertainty [1 , 82] and unlikelihood. It was not a measure of any information, not any uncertainty about any proposition, and not the unlikelihood about the occurrence of any event. However, because the quantity he found has the same mathematical form as the entropy in statistical mechanics, he called his measure, as allegedly suggested by von Neumann: \u201centropy\u201d. This proved to be a grievous mistake which had caused a great confusion in both information theory and thermodynamics.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 7,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 493,
                    "end_line_num": 500
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 14635,
                "end_ix": 14648,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Entropy, Shannon\u2019s Measure of Information and Boltzmann\u2019s H-Theorem",
                  "url": "https://www.mdpi.com/1099-4300/19/2/48#:~:text=for%20the%20interpretation%20of%20the,the%20Second%20Law%20of%20Thermodynamics",
                  "text": "for the interpretation of the thermodynamic entropy. The entropy measures the uncertainty in the distribution of the locations and momenta of all the particles; as well as two corrections due to the uncertainty principle and the indistinguishability of the particles. Finally we show that the H-function as defined by Boltzmann is an SMI but not entropy. Therefore; much of what has been written on the H-theorem is irrelevant to entropy and the Second Law of Thermodynamics.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 7,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 456,
                    "end_line_num": 464
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 15504,
                "end_ix": 15516,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Landauer's_principle",
                  "url": "https://www.chemeurope.com/en/encyclopedia/Landauer%27s_principle.html#:~:text=Landauer%27s%20Principle%2C%20first%20argued%20in,%28Bennett%202003%29%5B2",
                  "text": "Landauer's Principle, first argued in 1961[1] by Rolf Landauer of IBM, holds that \"any logically irreversible manipulation of information, such as the erasure of a bit or the merging of two computation paths, must be accompanied by a corresponding entropy increase in non-information bearing degrees of freedom of the information processing apparatus or its environment\". (Bennett 2003)[2].",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 21,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 42,
                    "end_line_num": 46
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 15516,
                "end_ix": 15528,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Landauer's_principle",
                  "url": "https://www.chemeurope.com/en/encyclopedia/Landauer%27s_principle.html#:~:text=Specifically%2C%20each%20bit%20of%20lost,the%20study%20of%20reversible%20computing",
                  "text": "Specifically, each bit of lost information will lead to the release of an amount kT ln 2 of heat, where k is the Boltzmann constant and T is the absolute temperature of the circuit. On the other hand, if no information is erased, computation may in principle be achieved which is thermodynamically reversible, and require no release of heat. This has led to considerable interest in the study of reversible computing.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 21,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 62,
                    "end_line_num": 68
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 15648,
                "end_ix": 15660,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Landauer's_principle",
                  "url": "https://www.chemeurope.com/en/encyclopedia/Landauer%27s_principle.html#:~:text=Specifically%2C%20each%20bit%20of%20lost,the%20study%20of%20reversible%20computing",
                  "text": "Specifically, each bit of lost information will lead to the release of an amount kT ln 2 of heat, where k is the Boltzmann constant and T is the absolute temperature of the circuit. On the other hand, if no information is erased, computation may in principle be achieved which is thermodynamically reversible, and require no release of heat. This has led to considerable interest in the study of reversible computing.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 21,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 62,
                    "end_line_num": 68
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 16232,
                "end_ix": 16244,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Landauer's_principle",
                  "url": "https://www.chemeurope.com/en/encyclopedia/Landauer%27s_principle.html#:~:text=Specifically%2C%20each%20bit%20of%20lost,the%20study%20of%20reversible%20computing",
                  "text": "Specifically, each bit of lost information will lead to the release of an amount kT ln 2 of heat, where k is the Boltzmann constant and T is the absolute temperature of the circuit. On the other hand, if no information is erased, computation may in principle be achieved which is thermodynamically reversible, and require no release of heat. This has led to considerable interest in the study of reversible computing.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 21,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 62,
                    "end_line_num": 68
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 16895,
                "end_ix": 16909,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Bekenstein bound - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Bekenstein_bound#:~:text=The%20universal%20form%20of%20the,field%20theory%20in%20curved%20spacetime",
                  "text": "The universal form of the bound was originally found by Jacob Bekenstein in 1981 as the inequality [ 1 ][ 2 ][ 3 ] [Image 1: {\\displaystyle S\\leq {\\frac {2\\pi kRE}{\\hbar c}},}] where S is the  55, k is the Boltzmann constant , R is the  66 of a sphere  that can enclose the given system, E is the total  68 including any rest masses , \u0127 is the  70, and c is the speed of light . Note that while gravity plays a significant role in its enforcement, the expression for the bound does not contain the  72\u00a0G, and so, it ought to apply to quantum field theory in curved spacetime.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 57,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 152,
                    "end_line_num": 160
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 17030,
                "end_ix": 17044,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Bekenstein bound - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Bekenstein_bound#:~:text=In%20physics%20%2C%20the%20Bekenstein,and%20the%20energy%20are%20finite",
                  "text": "In physics , the Bekenstein bound (named after  60) is an upper limit on the thermodynamic entropy  S, or  62 H, that can be contained within a given finite region of space which has a finite amount of energy\u2014or conversely, the maximum amount of information required to perfectly describe a given physical system down to the quantum level.[ 1 ] It implies that the information of a physical system, or the information necessary to perfectly describe that system, must be finite if the region of space and the energy are finite.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 57,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 139,
                    "end_line_num": 147
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 17324,
                "end_ix": 17338,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Bekenstein bound - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Bekenstein_bound#:~:text=In%20physics%20%2C%20the%20Bekenstein,and%20the%20energy%20are%20finite",
                  "text": "In physics , the Bekenstein bound (named after  60) is an upper limit on the thermodynamic entropy  S, or  62 H, that can be contained within a given finite region of space which has a finite amount of energy\u2014or conversely, the maximum amount of information required to perfectly describe a given physical system down to the quantum level.[ 1 ] It implies that the information of a physical system, or the information necessary to perfectly describe that system, must be finite if the region of space and the energy are finite.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 57,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 139,
                    "end_line_num": 147
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 18012,
                "end_ix": 18026,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Von Neumann entropy - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Von_Neumann_entropy#:~:text=In%20physics%20%2C%20the%20von,ln%7D%20%7D%5D%20denotes%20the",
                  "text": "In physics , the von Neumann entropy, named after  62, is a measure of the statistical uncertainty within a description of a quantum system. It extends the concept of Gibbs entropy  from classical  64 to quantum statistical mechanics , and it is the quantum counterpart of the  66 from classical information theory . For a quantum-mechanical system described by a  68 \u03c1, the von Neumann entropy is[ 1 ] [Image 0: {\\displaystyle S=-\\operatorname {tr} (\\rho \\ln \\rho ),}] where [Image 1: {\\displaystyle \\operatorname {tr} }] denotes the trace and [Image 2: {\\displaystyle \\operatorname {ln} }] denotes the",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 27,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 145,
                    "end_line_num": 153
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 18147,
                "end_ix": 18161,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Von Neumann entropy - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Von_Neumann_entropy#:~:text=In%20physics%20%2C%20the%20von,ln%7D%20%7D%5D%20denotes%20the",
                  "text": "In physics , the von Neumann entropy, named after  62, is a measure of the statistical uncertainty within a description of a quantum system. It extends the concept of Gibbs entropy  from classical  64 to quantum statistical mechanics , and it is the quantum counterpart of the  66 from classical information theory . For a quantum-mechanical system described by a  68 \u03c1, the von Neumann entropy is[ 1 ] [Image 0: {\\displaystyle S=-\\operatorname {tr} (\\rho \\ln \\rho ),}] where [Image 1: {\\displaystyle \\operatorname {tr} }] denotes the trace and [Image 2: {\\displaystyle \\operatorname {ln} }] denotes the",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 27,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 145,
                    "end_line_num": 153
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 18382,
                "end_ix": 18394,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Quantum mutual information - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Quantum_mutual_information#:~:text=Quantum%20mutual%20information%20,1",
                  "text": "Quantum mutual information - Wikipedia ... von Neumann, is a measure of correlation between subsystems of quantum state. It is the quantum mechanical analog of Shannon mutual information. Contents. 1 ...",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 26,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 13,
                    "end_line_num": 16
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 19993,
                "end_ix": 20007,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "W. Ross Ashby, Cybernetics and Requisite Variety (1956)",
                  "url": "https://www.panarchy.org/ashby/variety.1956.html#:~:text=One%20is%20that%20it%20offers,concepts%20that%2C%20by%20having%20exact",
                  "text": "One is that it offers a single vocabulary and a single set of concepts suitable for representing the most diverse types of systems. Until recently, any attempt to relate the many facts known about, say, servo-mechanisms to what was known about the cerebellum was made unnecessarily difficult by the fact that the properties of servo-mechanisms were described in words redolent of the automatic pilot, or the radio set, or the hydraulic brake, while those of the cerebellum were described in words redolent of the dissecting room and the bedside - aspects that are irrelevant to the similarities between a servo-mechanism and a cerebellar reflex. Cybernetics offers one set of concepts that, by having exact",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 35,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 136,
                    "end_line_num": 144
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 20007,
                "end_ix": 20021,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "W. Ross Ashby, Cybernetics and Requisite Variety (1956)",
                  "url": "https://www.panarchy.org/ashby/variety.1956.html#:~:text=It%20has%20been%20found%20repeatedly,be%20made%20use%20of%20in",
                  "text": "It has been found repeatedly in science that the discovery that two branches are related leads to each branch helping in the development of the other. The result is often a markedly accelerated growth of both. The infinitesimal calculus and astronomy, the virus and the protein molecule, the chromosomes and heredity are examples that come to mind. Neither, of course, can give proofs about the laws of the other, but each can give suggestions that may be of the greatest assistance and fruitfulness. Here I need only mention the fact that cybernetics is likely to reveal a great number of interesting and suggestive parallelisms between machine and brain and society. And it can provide the common language by which discoveries in one branch can readily be made use of in",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 35,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 148,
                    "end_line_num": 157
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 20312,
                "end_ix": 20322,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Norbert Wiener",
                  "url": "https://www.informationphilosopher.com/solutions/scientists/wiener/#:~:text=activity,132",
                  "text": "activity. Information is information, not matter or energy. No materialism which does not admit this can survive at the present day. > > (Cybernetics, 2nd edition, p.132)",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 39,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 1,
                    "end_line_num": 4
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 20334,
                "end_ix": 20344,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Norbert Wiener",
                  "url": "https://www.informationphilosopher.com/solutions/scientists/wiener/#:~:text=activity,132",
                  "text": "activity. Information is information, not matter or energy. No materialism which does not admit this can survive at the present day. > > (Cybernetics, 2nd edition, p.132)",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 39,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 1,
                    "end_line_num": 4
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 21577,
                "end_ix": 21587,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "W. Ross Ashby, Cybernetics and Requisite Variety (1956)",
                  "url": "https://www.panarchy.org/ashby/variety.1956.html#:~:text=11%2F11,apply%20a%20measure%20to%20regulation",
                  "text": "11/11. The Law of Requisite Variety [\"Only variety can destroy variety.\"] enables us to apply a measure to regulation.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 36,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 1,
                    "end_line_num": 4
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 21926,
                "end_ix": 21936,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "W. Ross Ashby, Cybernetics and Requisite Variety (1956)",
                  "url": "https://www.panarchy.org/ashby/variety.1956.html#:~:text=11%2F11,apply%20a%20measure%20to%20regulation",
                  "text": "11/11. The Law of Requisite Variety [\"Only variety can destroy variety.\"] enables us to apply a measure to regulation.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 36,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 1,
                    "end_line_num": 4
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 25017,
                "end_ix": 25031,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "What Is Life? - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/What_Is_Life%3F#:~:text=second%20law%20of%20thermodynamics%20%2C,in%20an%20%20104",
                  "text": "second law of thermodynamics , according to which  100 only increases in a closed system  (such as the universe). Schr\u00f6dinger explains that living matter evades the decay to  102 by homeostatically maintaining negative entropy  in an  104.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 43,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 270,
                    "end_line_num": 278
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 25485,
                "end_ix": 25499,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "What Is Life? - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/What_Is_Life%3F#:~:text=second%20law%20of%20thermodynamics%20%2C,in%20an%20%20104",
                  "text": "second law of thermodynamics , according to which  100 only increases in a closed system  (such as the universe). Schr\u00f6dinger explains that living matter evades the decay to  102 by homeostatically maintaining negative entropy  in an  104.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 43,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 270,
                    "end_line_num": 278
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 26858,
                "end_ix": 26869,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "book_X dist.pdf",
                  "url": "https://assets.cambridge.org/97805218/02932/frontmatter/9780521802932_frontmatter.pdf#:~:text=biology,computer%20competent%20reader%20who%20is",
                  "text": "biology. The genetical information system, because it is linear and digital, resembles the algorithmic language of computers. George Gamow pointed out that the application of Shannon\u2019s information theory breaks genetics and molecular biology out of the descriptive mode into the quantitative mode, and Dr. Yockey develops this theme, discussing how information theory and coding theory can be applied to molecular biology. He discusses how these tools for measuring the information in the sequences of the genome and the proteome are essential for our complete understanding of the nature and origin of life. The author writes for the computer competent reader who is",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 51,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 5,
                    "end_line_num": 13
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 27093,
                "end_ix": 27104,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "book_X dist.pdf",
                  "url": "https://assets.cambridge.org/97805218/02932/frontmatter/9780521802932_frontmatter.pdf#:~:text=biology,computer%20competent%20reader%20who%20is",
                  "text": "biology. The genetical information system, because it is linear and digital, resembles the algorithmic language of computers. George Gamow pointed out that the application of Shannon\u2019s information theory breaks genetics and molecular biology out of the descriptive mode into the quantitative mode, and Dr. Yockey develops this theme, discussing how information theory and coding theory can be applied to molecular biology. He discusses how these tools for measuring the information in the sequences of the genome and the proteome are essential for our complete understanding of the nature and origin of life. The author writes for the computer competent reader who is",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 51,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 5,
                    "end_line_num": 13
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 29452,
                "end_ix": 29464,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "New 'assembly theory' unifies physics and biology to explain evolution, complexity | ASU News",
                  "url": "https://news.asu.edu/20231005-new-assembly-theory-unifies-physics-and-biology-explain-evolution-complexity#:~:text=This%20research%20builds%20on%20the,derived%20molecules",
                  "text": "This research builds on the team's previous work developing assembly theory as an empirically validated approach to life detection, with implications for the search for alien life and efforts to evolve new life forms in the laboratory. In prior work, the team assigned a complexity score to molecules called the molecular assembly index, based on the minimal number of bond-forming steps required to build a molecule. They showed how this index is experimentally measurable and how high values correlate with life-derived molecules.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 28,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 28,
                    "end_line_num": 36
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 29824,
                "end_ix": 29836,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "New 'assembly theory' unifies physics and biology to explain evolution, complexity | ASU News",
                  "url": "https://news.asu.edu/20231005-new-assembly-theory-unifies-physics-and-biology-explain-evolution-complexity#:~:text=The%20new%20study%20introduces%20mathematical,their%20abundance%20and%20assembly%20indices",
                  "text": "The new study introduces mathematical formalism around a physical quantity called \"assembly\" that captures how much selection is required to produce a given set of complex objects, based on their abundance and assembly indices.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 28,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 36,
                    "end_line_num": 44
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 30176,
                "end_ix": 30188,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "New 'assembly theory' unifies physics and biology to explain evolution, complexity | ASU News",
                  "url": "https://news.asu.edu/20231005-new-assembly-theory-unifies-physics-and-biology-explain-evolution-complexity#:~:text=,Evolution%20and%20the%20Beyond%20Center",
                  "text": "\"Assembly theory provides a completely new lens for looking at physics, chemistry and biology as different perspectives of the same underlying reality,\" explained lead author Sara Walker, a theoretical physicist, origin of life researcher and professor with appointments in Arizona State University's School of Earth and Space Exploration,\u00a0Biosocial Complexity Initiative,\u00a0Center for Social Dynamics and Complexity (CSDC), Biodesign Center for Mechanisms of Evolution and the Beyond Center.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 28,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 40,
                    "end_line_num": 48
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 30500,
                "end_ix": 30512,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "New 'assembly theory' unifies physics and biology to explain evolution, complexity | ASU News",
                  "url": "https://news.asu.edu/20231005-new-assembly-theory-unifies-physics-and-biology-explain-evolution-complexity#:~:text=An%20international%20team%20of%20researchers,physical%20laws%20of%20the%20universe",
                  "text": "An international team of researchers has developed a new theoretical framework that bridges physics and biology to provide a unified approach for understanding how complexity and evolution emerge in nature. This new work on \"assembly theory,\" published in Nature, represents a major advance in our fundamental comprehension of biological evolution and how it is governed by the physical laws of the universe.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 28,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 21,
                    "end_line_num": 29
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 30512,
                "end_ix": 30524,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "New 'assembly theory' unifies physics and biology to explain evolution, complexity | ASU News",
                  "url": "https://news.asu.edu/20231005-new-assembly-theory-unifies-physics-and-biology-explain-evolution-complexity#:~:text=,chemistry%2C%20biology%20and%20information%20theory",
                  "text": "\"Assembly theory provides an entirely new way to look at the matter that makes up our world, as defined not just by immutable particles but by the memory needed to build objects through selection over time,\" said Professor Lee Cronin, a chemist from the University of Glasgow and co-lead author. \"With further work, this approach has the potential to transform fields from cosmology to computer science. It represents a new frontier at the intersection of physics, chemistry, biology and information theory.\"",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 28,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 58,
                    "end_line_num": 64
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 31616,
                "end_ix": 31626,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "An Overview of the Free Energy Principle and Related Research",
                  "url": "https://direct.mit.edu/neco/article/36/5/963/119791/An-Overview-of-the-Free-Energy-Principle-and#:~:text=An%20Overview%20of%20the%20Free,implementing%20perception%20through%20Bayesian",
                  "text": "An Overview of the Free Energy Principle and Related Research The free energy principle states that the human brain operates as a Bayesian inference machine, implementing perception through Bayesian ...",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 29,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 5,
                    "end_line_num": 8
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 31728,
                "end_ix": 31742,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "\n            Answering Schr\u00f6dinger's question: A free-energy formulation - PMC\n        ",
                  "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC5857288/#:~:text=The%20free,Homo%20sapiens%2C%20before%20translating%20variational",
                  "text": "The free-energy principle (FEP) is a formal model of neuronal processes that is widely recognised in neuroscience as a unifying theory of the brain and biobehaviour. More recently, however, it has been extended beyond the brain to explain the dynamics of living systems, and their unique capacity to avoid decay. The aim of this review is to synthesise these advances with a meta- theoretical ontology of biological systems called variational neuroethology, which integrates the FEP with Tinbergen's four research questions to explain biological systems across spatial and temporal scales. We exemplify this framework by applying it to Homo sapiens, before translating variational",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 30,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 162,
                    "end_line_num": 170
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 32495,
                "end_ix": 32509,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "\n            Answering Schr\u00f6dinger's question: A free-energy formulation - PMC\n        ",
                  "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC5857288/#:~:text=The%20free,Homo%20sapiens%2C%20before%20translating%20variational",
                  "text": "The free-energy principle (FEP) is a formal model of neuronal processes that is widely recognised in neuroscience as a unifying theory of the brain and biobehaviour. More recently, however, it has been extended beyond the brain to explain the dynamics of living systems, and their unique capacity to avoid decay. The aim of this review is to synthesise these advances with a meta- theoretical ontology of biological systems called variational neuroethology, which integrates the FEP with Tinbergen's four research questions to explain biological systems across spatial and temporal scales. We exemplify this framework by applying it to Homo sapiens, before translating variational",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 30,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 162,
                    "end_line_num": 170
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 34855,
                "end_ix": 34867,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "It from bit? | plus.maths.org",
                  "url": "https://plus.maths.org/content/it-bit-0#:~:text=,no%20questions%2C%20binary%20choices%2C%20bits",
                  "text": "\"Every it \u2013 every particle, every field of force, even the space-time continuum itself _ derives its function, its meaning, its very existence entirely... from the apparatus-elected answers to yes or no questions, binary choices, bits.\"",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 25,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 53,
                    "end_line_num": 60
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 34892,
                "end_ix": 34904,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "It from bit? | plus.maths.org",
                  "url": "https://plus.maths.org/content/it-bit-0#:~:text=,no%20questions%2C%20binary%20choices%2C%20bits",
                  "text": "\"Every it \u2013 every particle, every field of force, even the space-time continuum itself _ derives its function, its meaning, its very existence entirely... from the apparatus-elected answers to yes or no questions, binary choices, bits.\"",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 25,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 53,
                    "end_line_num": 60
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 35956,
                "end_ix": 35967,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Programming the Universe: A Quantum Computer Scientist Takes ...",
                  "url": "https://www.amazon.com/Programming-Universe-Quantum-Computer-Scientist/dp/1400033861#:~:text=The%20universe%20is%20a%20quantum,As",
                  "text": "The universe is a quantum computer. This begs the question: What does the universe compute? It computes itself. The universe computes its own behavior. As ...",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 53,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 6,
                    "end_line_num": 13
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 35967,
                "end_ix": 35979,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "The universe as quantum computer - ResearchGate",
                  "url": "https://www.researchgate.net/publication/303418850_The_universe_as_quantum_computer#:~:text=The%20universe%20as%20quantum%20computer,Theoretic",
                  "text": "The universe as quantum computer - ResearchGate Lloyd's thesis: 4 The universe is a quantum computer [98, 196, 197]. ... Fundamental Physics and Computation: The Computer-Theoretic ...",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 53,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 15,
                    "end_line_num": 18
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 36167,
                "end_ix": 36177,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Programming the Universe - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Programming_the_Universe#:~:text=proposes%20that%20the%20Universe%20is,fully%20simulated%20using%20a%20quantum",
                  "text": "proposes that the Universe is a quantum computer (supercomputer), and advances in the understanding of physics may come from viewing entropy as a phenomenon of information, rather than simply thermodynamics. Lloyd also postulates that the Universe can be fully simulated using a quantum",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 55,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 1,
                    "end_line_num": 4
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 37625,
                "end_ix": 37635,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Amazon.com: EMERGENT INFORMATION: A UNIFIED THEORY OF ...",
                  "url": "https://www.amazon.com/Emergent-Information-Unified-Framework-Scientific/dp/9814313483#:~:text=Amazon,13",
                  "text": "Amazon.com: EMERGENT INFORMATION: A UNIFIED THEORY OF ... Wolfgang Hofkirchner. EMERGENT INFORMATION: A UNIFIED THEORY OF INFORMATION FRAMEWORK (World Scientific Series in Information Studies, 3). 1st Edition. ISBN-13 ...",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 4,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 5,
                    "end_line_num": 13
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 37822,
                "end_ix": 37833,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "A Concise Account of Information as Meaning Ascribed to Symbols ...",
                  "url": "https://www.mdpi.com/1099-4300/25/1/177#:~:text=A%20Concise%20Account%20of%20Information,here%20are%20in%20line",
                  "text": "A Concise Account of Information as Meaning Ascribed to Symbols ... As mathematician and philosopher N. Wiener [15] put it, \u201cInformation is information, not matter or energy.\u201d Therefore, the arguments presented here are in line ...",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 37,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 9,
                    "end_line_num": 17
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 39528,
                "end_ix": 39541,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "A grand unified theory for the unification of physics, life, information and cognition (mind) | Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",
                  "url": "https://royalsocietypublishing.org/doi/10.1098/rsta.2022.0277#:~:text=The%20modern%20scientific%20world%20view,An%20expanded",
                  "text": "The modern scientific world view was built on the incommensurability between cognition (mind) and physics (matter) and later life and physics (the autonomy of biology). Fuelled by Boltzmann's view of the second law of thermodynamics as a \u2018law of disorder\u2019, the idea of \u2018two opposing rivers\u2019, the river of physics \u2018flowing down\u2019 to disorder and the river of life and mind \u2018flowing up\u2019 to higher states of order became a cornerstone of contemporary thinking. The deleterious result of this paradigmatic separation of physics, life and mind has been to considerably incapacitate each by bracketing many of the deepest problems of science, including the very nature of life itself and its cognitive capabilities, outside the theoretical reach of contemporary science. An expanded",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 3,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 234,
                    "end_line_num": 243
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 39541,
                "end_ix": 39554,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "A grand unified theory for the unification of physics, life, information and cognition (mind) | Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",
                  "url": "https://royalsocietypublishing.org/doi/10.1098/rsta.2022.0277#:~:text=view%20of%20physics%2C%20notably%20the,modern%20science%20associated%20with%20it",
                  "text": "view of physics, notably the addition of the fourth law of thermodynamics (LMEP), or the law of maximum entropy production, coupled with first law time- translation symmetry, and the self-referencing circularity of the relational ontology of autocatakinetic systems, provides the basis for a grand unified theory unifying physics, life, information and cognition (mind). This dissolves the dysfunctional myth of the two rivers, and solves the previously insoluble problems at the foundations of modern science associated with it.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 3,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 244,
                    "end_line_num": 252
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 39793,
                "end_ix": 39806,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "A grand unified theory for the unification of physics, life, information and cognition (mind) | Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",
                  "url": "https://royalsocietypublishing.org/doi/10.1098/rsta.2022.0277#:~:text=science%2C%20including%20the%20very%20nature,modern%20science%20associated%20with%20it",
                  "text": "science, including the very nature of life itself and its cognitive capabilities, outside the theoretical reach of contemporary science. An expanded view of physics, notably the addition of the fourth law of thermodynamics (LMEP), or the law of maximum entropy production, coupled with first law time- translation symmetry, and the self-referencing circularity of the relational ontology of autocatakinetic systems, provides the basis for a grand unified theory unifying physics, life, information and cognition (mind). This dissolves the dysfunctional myth of the two rivers, and solves the previously insoluble problems at the foundations of modern science associated with it.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 3,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 242,
                    "end_line_num": 250
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 42716,
                "end_ix": 42729,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Information theory - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Information_theory#:~:text=%5D,study%20of%20%20216%20outer",
                  "text": "][ 3 ] The theory has also found applications in other areas, including statistical inference ,[ 7 ]  198, neurobiology ,[ 8 ]  200,[ 9 ] signal processing ,[ 2 ]  202, the evolution[ 10 ] and function[ 11 ] of molecular codes (bioinformatics ),  204,[ 12 ] molecular dynamics ,[ 13 ]  206, quantum computing ,  208, intelligence gathering ,  210,[ 14 ] pattern recognition ,  212,[ 15 ] the analysis of music ,[ 16 ][ 17 ]  214,[ 18 ] imaging system design,[ 19 ] study of  216 outer",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 9,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 283,
                    "end_line_num": 291
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 44013,
                "end_ix": 44027,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "Bekenstein bound - Wikipedia",
                  "url": "https://en.wikipedia.org/wiki/Bekenstein_bound#:~:text=In%20physics%20%2C%20the%20Bekenstein,and%20the%20energy%20are%20finite",
                  "text": "In physics , the Bekenstein bound (named after  60) is an upper limit on the thermodynamic entropy  S, or  62 H, that can be contained within a given finite region of space which has a finite amount of energy\u2014or conversely, the maximum amount of information required to perfectly describe a given physical system down to the quantum level.[ 1 ] It implies that the information of a physical system, or the information necessary to perfectly describe that system, must be finite if the region of space and the energy are finite.",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 57,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 139,
                    "end_line_num": 147
                  },
                  "og_tags": null
                }
              },
              {
                "start_ix": 44027,
                "end_ix": 44039,
                "citation_format_type": "tether_v4",
                "metadata": {
                  "type": "webpage",
                  "title": "It from bit? | plus.maths.org",
                  "url": "https://plus.maths.org/content/it-bit-0#:~:text=,no%20questions%2C%20binary%20choices%2C%20bits",
                  "text": "\"Every it \u2013 every particle, every field of force, even the space-time continuum itself _ derives its function, its meaning, its very existence entirely... from the apparatus-elected answers to yes or no questions, binary choices, bits.\"",
                  "pub_date": null,
                  "extra": {
                    "cited_message_idx": 25,
                    "search_result_idx": null,
                    "evidence_text": "",
                    "start_line_num": 53,
                    "end_line_num": 60
                  },
                  "og_tags": null
                }
              }
            ],
            "content_references": [
              {
                "matched_text": "\u301010\u2020L33-L37\u3011",
                "start_idx": 1636,
                "end_idx": 1648,
                "alt": null,
                "type": "webpage_extended",
                "title": "Shannon\u2019s entropy",
                "url": "https://planetmath.org/shannonsentropy#:~:text=The%20quantity%20is%20interpreted%20as,actual%20outcome%20of%20is%20revealed",
                "pub_date": null,
                "snippet": "The quantity is interpreted as the information content of the outcome , and is also called the Hartley information  of . Hence the Shannon\u2019s entropy is the average[Image 9: Mathworld][Image 10: Planetmath] amount of  7 contained in random variable , it is also the uncertainty removed after the actual outcome of is revealed.",
                "attribution": "planetmath.org"
              },
              {
                "matched_text": "\u30109\u2020L299-L306\u3011",
                "start_idx": 1803,
                "end_idx": 1816,
                "alt": null,
                "type": "webpage_extended",
                "title": "Information theory - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Information_theory#:~:text=Information%20theory%20studies%20the%20transmission%2C,in%20spite%20of%20the%20channel",
                "pub_date": null,
                "snippet": "Information theory studies the transmission, processing, extraction, and utilization of information . Abstractly, information can be thought of as the resolution of uncertainty. In the case of communication of information over a noisy channel, this abstract concept was formalized in 1948 by  168 in a paper entitled A Mathematical Theory of Communication, in which information is thought of as a set of possible messages, and the goal is to send these messages over a noisy channel, and to have the receiver reconstruct the message with low probability of error, in spite of the channel",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u30109\u2020L259-L268\u3011",
                "start_idx": 2340,
                "end_idx": 2353,
                "alt": null,
                "type": "webpage_extended",
                "title": "Information theory - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Information_theory#:~:text=established%20and%20formalized%20by%20Claude,3",
                "pub_date": null,
                "snippet": "established and formalized by Claude Shannon  in the 1940s,[ 1 ] though early contributions were made in the 1920s through the works of  169 and Ralph Hartley . It is at the intersection of  171, mathematics ,  173, computer science ,  175, physics , and  177.[ 2 ][ 3 ]",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u30109\u2020L271-L275\u3011",
                "start_idx": 2353,
                "end_idx": 2366,
                "alt": null,
                "type": "webpage_extended",
                "title": "Information theory - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Information_theory#:~:text=important%20measures%20in%20information%20theory,theory%20%20and%20%20188",
                "pub_date": null,
                "snippet": "important measures in information theory are mutual information ,  158, error exponents , and  152. Important sub-fields of information theory include source coding ,  186, algorithmic information theory  and  188.",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u30109\u2020L283-L291\u3011",
                "start_idx": 2561,
                "end_idx": 2574,
                "alt": null,
                "type": "webpage_extended",
                "title": "Information theory - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Information_theory#:~:text=%5D,study%20of%20%20216%20outer",
                "pub_date": null,
                "snippet": "][ 3 ] The theory has also found applications in other areas, including statistical inference ,[ 7 ]  198, neurobiology ,[ 8 ]  200,[ 9 ] signal processing ,[ 2 ]  202, the evolution[ 10 ] and function[ 11 ] of molecular codes (bioinformatics ),  204,[ 12 ] molecular dynamics ,[ 13 ]  206, quantum computing ,  208, intelligence gathering ,  210,[ 14 ] pattern recognition ,  212,[ 15 ] the analysis of music ,[ 16 ][ 17 ]  214,[ 18 ] imaging system design,[ 19 ] study of  216 outer",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u30107\u2020L493-L500\u3011",
                "start_idx": 3642,
                "end_idx": 3655,
                "alt": null,
                "type": "webpage_extended",
                "title": "Entropy, Shannon\u2019s Measure of Information and Boltzmann\u2019s H-Theorem",
                "url": "https://www.mdpi.com/1099-4300/19/2/48#:~:text=In%201948%2C%20Shannon%20sought%20and,both%20information%20theory%20and%20thermodynamics",
                "pub_date": null,
                "snippet": "In 1948, Shannon sought and found a remarkable measure of information, of uncertainty [1 , 82] and unlikelihood. It was not a measure of any information, not any uncertainty about any proposition, and not the unlikelihood about the occurrence of any event. However, because the quantity he found has the same mathematical form as the entropy in statistical mechanics, he called his measure, as allegedly suggested by von Neumann: \u201centropy\u201d. This proved to be a grievous mistake which had caused a great confusion in both information theory and thermodynamics.",
                "attribution": "mdpi.com"
              },
              {
                "matched_text": "\u301012\u2020L177-L185\u3011",
                "start_idx": 4649,
                "end_idx": 4663,
                "alt": null,
                "type": "webpage_extended",
                "title": "Kolmogorov complexity - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Kolmogorov_complexity#:~:text=mathematics%20,generalization%20of%20classical%20information%20theory",
                "pub_date": null,
                "snippet": "mathematics ), the Kolmogorov complexity of an object, such as a piece of text, is the length of a shortest  93 (in a predetermined programming language ) that produces the object as output. It is a measure of the  95 resources needed to specify the object, and is also known as algorithmic complexity, Solomonoff\u2013Kolmogorov\u2013Chaitin complexity, program-size complexity, descriptive complexity, or algorithmic entropy. It is named after Andrey Kolmogorov, who first published on the subject in 1963[ 1 ][ 2 ] and is a generalization of classical information theory.",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301012\u2020L193-L197\u3011",
                "start_idx": 4663,
                "end_idx": 4677,
                "alt": null,
                "type": "webpage_extended",
                "title": "Kolmogorov complexity - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Kolmogorov_complexity#:~:text=length%20of%20the%20ultimately%20compressed,3",
                "pub_date": null,
                "snippet": "length of the ultimately compressed version of a file (i.e., anything which can be put in a computer). Formally, it is the length of a shortest program from which the file can be reconstructed. While Kolmogorov complexity is uncomputable, various approaches have been proposed and reviewed.[ 3 ]",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301012\u2020L176-L184\u3011",
                "start_idx": 5005,
                "end_idx": 5019,
                "alt": null,
                "type": "webpage_extended",
                "title": "Kolmogorov complexity - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Kolmogorov_complexity#:~:text=In%20algorithmic%20information%20theory%20,generalization%20of%20classical%20information%20theory",
                "pub_date": null,
                "snippet": "In algorithmic information theory  (a subfield of  91 and mathematics ), the Kolmogorov complexity of an object, such as a piece of text, is the length of a shortest  93 (in a predetermined programming language ) that produces the object as output. It is a measure of the  95 resources needed to specify the object, and is also known as algorithmic complexity, Solomonoff\u2013Kolmogorov\u2013Chaitin complexity, program-size complexity, descriptive complexity, or algorithmic entropy. It is named after Andrey Kolmogorov, who first published on the subject in 1963[ 1 ][ 2 ] and is a generalization of classical information theory.",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301012\u2020L185-L194\u3011",
                "start_idx": 5584,
                "end_idx": 5598,
                "alt": null,
                "type": "webpage_extended",
                "title": "Kolmogorov complexity - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Kolmogorov_complexity#:~:text=The%20notion%20of%20Kolmogorov%20complexity,of%20a%20shortest%20program%20from",
                "pub_date": null,
                "snippet": "The notion of Kolmogorov complexity can be used to state and prove impossibility  results akin to  98, G\u00f6del's incompleteness theorem , and  100. In particular, no program P computing a lower bound for each text's Kolmogorov complexity can return a value essentially larger than P's own length (see section \u00a7 Chaitin's incompleteness theorem); hence no single program can compute the exact Kolmogorov complexity for infinitely many texts. Kolmogorov complexity is the length of the ultimately compressed version of a file (i.e., anything which can be put in a computer). Formally, it is the length of a shortest program from",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301014\u2020L165-L173\u3011",
                "start_idx": 6203,
                "end_idx": 6217,
                "alt": null,
                "type": "webpage_extended",
                "title": "Solomonoff's theory of inductive inference - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference#:~:text=Solomonoff%27s%20theory%20of%20inductive%20inference,it%20encompasses%20%2067",
                "pub_date": null,
                "snippet": "Solomonoff's theory of inductive inference proves that, under its common sense assumptions (axioms), the best possible scientific model is the shortest algorithm that generates the empirical data under consideration. In addition to the choice of data, other assumptions are that, to avoid the post-hoc fallacy, the programming language must be chosen prior to the data[ 1 ] and that the environment being observed is generated by an unknown algorithm. This is also called a theory of induction . Due to its basis in the dynamical ( 65) character of Algorithmic Information Theory , it encompasses  67.",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301014\u2020L175-L183\u3011",
                "start_idx": 6519,
                "end_idx": 6533,
                "alt": null,
                "type": "webpage_extended",
                "title": "Solomonoff's theory of inductive inference - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference#:~:text=theoretical%20computer%20science%20.,probability%20to%20any%20computable%20theory",
                "pub_date": null,
                "snippet": "theoretical computer science .[ 2 ][ 3 ][ 4 ] In essence, Solomonoff's induction derives the  71 of any computable  theory, given a sequence of observed data. This posterior probability is derived from  73 and some universal prior, that is, a prior that assigns a positive probability to any computable theory.",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301014\u2020L165-L173\u3011",
                "start_idx": 6678,
                "end_idx": 6692,
                "alt": null,
                "type": "webpage_extended",
                "title": "Solomonoff's theory of inductive inference - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference#:~:text=Solomonoff%27s%20theory%20of%20inductive%20inference,it%20encompasses%20%2067",
                "pub_date": null,
                "snippet": "Solomonoff's theory of inductive inference proves that, under its common sense assumptions (axioms), the best possible scientific model is the shortest algorithm that generates the empirical data under consideration. In addition to the choice of data, other assumptions are that, to avoid the post-hoc fallacy, the programming language must be chosen prior to the data[ 1 ] and that the environment being observed is generated by an unknown algorithm. This is also called a theory of induction . Due to its basis in the dynamical ( 65) character of Algorithmic Information Theory , it encompasses  67.",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301014\u2020L175-L183\u3011",
                "start_idx": 6692,
                "end_idx": 6706,
                "alt": null,
                "type": "webpage_extended",
                "title": "Solomonoff's theory of inductive inference - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference#:~:text=theoretical%20computer%20science%20.,probability%20to%20any%20computable%20theory",
                "pub_date": null,
                "snippet": "theoretical computer science .[ 2 ][ 3 ][ 4 ] In essence, Solomonoff's induction derives the  71 of any computable  theory, given a sequence of observed data. This posterior probability is derived from  73 and some universal prior, that is, a prior that assigns a positive probability to any computable theory.",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301016\u2020L164-L172\u3011",
                "start_idx": 8370,
                "end_idx": 8384,
                "alt": null,
                "type": "webpage_extended",
                "title": "Fisher information - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Fisher_information#:~:text=In%20mathematical%20statistics%20%2C%20the,86%20of%20the%20observed%20information",
                "pub_date": null,
                "snippet": "In mathematical statistics , the Fisher information is a way of measuring the amount of  82 that an observable random variable  X carries about an unknown parameter \u03b8 of a distribution that models X. Formally, it is the  84 of the score , or the  86 of the observed information.",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301016\u2020L197-L205\u3011",
                "start_idx": 8896,
                "end_idx": 8910,
                "alt": null,
                "type": "webpage_extended",
                "title": "Fisher information - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Fisher_information#:~:text=observable%20random%20variable%20%20,displaystyle%20%5Ctheta%20%7D%5D%2C%20it",
                "pub_date": null,
                "snippet": "observable random variable  [Image 0: {\\displaystyle X}] carries about an unknown  105 [Image 1: {\\displaystyle \\theta }] upon which the probability of [Image 2: {\\displaystyle X}] depends. Let [Image 3: {\\displaystyle f(X;\\theta )}] be the probability density function  (or  107) for [Image 4: {\\displaystyle X}] conditioned on the value of [Image 5: {\\displaystyle \\theta }]. It describes the probability that we observe a given outcome of [Image 6: {\\displaystyle X}], given a known value of [Image 7: {\\displaystyle \\theta }]. If [Image 8: {\\displaystyle f}] is sharply peaked with respect to changes in [Image 9: {\\displaystyle \\theta }], it",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301016\u2020L187-L190\u3011",
                "start_idx": 9902,
                "end_idx": 9916,
                "alt": null,
                "type": "webpage_extended",
                "title": "Fisher information - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Fisher_information#:~:text=Statistical%20systems%20of%20a%20scientific,nature%20of%20the%20system%20constraints",
                "pub_date": null,
                "snippet": "Statistical systems of a scientific nature (physical, biological, etc.) whose likelihood functions obey shift invariance have been shown to obey maximum Fisher information.[ 4 ] The level of the maximum depends upon the nature of the system constraints.",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301048\u2020L130-L138\u3011",
                "start_idx": 11328,
                "end_idx": 11342,
                "alt": null,
                "type": "webpage_extended",
                "title": "Information geometry - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Information_geometry#:~:text=Information%20geometry%20is%20an%20interdisciplinary,points%20correspond%20to%20%2053",
                "pub_date": null,
                "snippet": "Information geometry is an interdisciplinary field that applies the techniques of differential geometry  to study  49 and statistics . [ 1 ] It studies  51, which are Riemannian manifolds  whose points correspond to  53.",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301048\u2020L146-L154\u3011",
                "start_idx": 11476,
                "end_idx": 11490,
                "alt": null,
                "type": "webpage_extended",
                "title": "Information geometry - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Information_geometry#:~:text=Historically%2C%20information%20geometry%20can%20be,4",
                "pub_date": null,
                "snippet": "Historically, information geometry can be traced back to the work of C. R. Rao , who was the first to treat the  57 as a Riemannian metric .[ 2 ][ 3 ] The modern theory is largely due to  59, whose work has been greatly influential on the development of the field.[ 4 ]",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301045\u2020L99-L107\u3011",
                "start_idx": 13260,
                "end_idx": 13273,
                "alt": null,
                "type": "webpage_extended",
                "title": "Jaynes and the Principle of Maximum Entropy \u2014 SFI Press",
                "url": "https://www.sfipress.org/14-jaynes-1957#:~:text=and%20information%20entropy%2C%20which%20he,introduces%20the%20%E2%80%9Cprinciple%20of%20maximum",
                "pub_date": null,
                "snippet": "and information entropy, which he immediately identified as a significant shift in perspective. In this paper, he showed that entropy as expressed in the work of Boltzmann and Gibbs in the field of thermodynamics and Shannon entropy follow from the same underlying logic, thus establishing entropy as a general concept. Maximum entropy inference is introduced and in sections 3, 4, and 5 examples are given showing how the application of the principle of maximum entropy results in a conceptual shift in the approach to statistical mechanics. Jaynes proved that statistical mechanics can be interpreted in non-physical terms as statistical inference. In his 1957 paper he introduces the \u201cprinciple of maximum",
                "attribution": "sfipress.org"
              },
              {
                "matched_text": "\u301045\u2020L107-L115\u3011",
                "start_idx": 13273,
                "end_idx": 13287,
                "alt": null,
                "type": "webpage_extended",
                "title": "Jaynes and the Principle of Maximum Entropy \u2014 SFI Press",
                "url": "https://www.sfipress.org/14-jaynes-1957#:~:text=statistical%20inference,by%20choosing%20the%20probability%20distribution",
                "pub_date": null,
                "snippet": "statistical inference. In his 1957 paper he introduces the \u201cprinciple of maximum entropy\u201d and proposed the maximization of the entropy function H, subject to certain constraints, as a general principle of statistical inference applicable to a diverse range of areas. The principle has since proved particularly useful in providing the least biased probability distribution for a set of unknown prior probabilities. Jaynes\u2019s view is that assigning equal probabilities in situations of total ignorance is the least prejudiced choice. However, when ignorance is not total, inferences should be based on all and only the information available. This is assured by choosing the probability distribution",
                "attribution": "sfipress.org"
              },
              {
                "matched_text": "\u301045\u2020L99-L107\u3011",
                "start_idx": 13386,
                "end_idx": 13399,
                "alt": null,
                "type": "webpage_extended",
                "title": "Jaynes and the Principle of Maximum Entropy \u2014 SFI Press",
                "url": "https://www.sfipress.org/14-jaynes-1957#:~:text=and%20information%20entropy%2C%20which%20he,introduces%20the%20%E2%80%9Cprinciple%20of%20maximum",
                "pub_date": null,
                "snippet": "and information entropy, which he immediately identified as a significant shift in perspective. In this paper, he showed that entropy as expressed in the work of Boltzmann and Gibbs in the field of thermodynamics and Shannon entropy follow from the same underlying logic, thus establishing entropy as a general concept. Maximum entropy inference is introduced and in sections 3, 4, and 5 examples are given showing how the application of the principle of maximum entropy results in a conceptual shift in the approach to statistical mechanics. Jaynes proved that statistical mechanics can be interpreted in non-physical terms as statistical inference. In his 1957 paper he introduces the \u201cprinciple of maximum",
                "attribution": "sfipress.org"
              },
              {
                "matched_text": "\u301045\u2020L99-L107\u3011",
                "start_idx": 13689,
                "end_idx": 13702,
                "alt": null,
                "type": "webpage_extended",
                "title": "Jaynes and the Principle of Maximum Entropy \u2014 SFI Press",
                "url": "https://www.sfipress.org/14-jaynes-1957#:~:text=and%20information%20entropy%2C%20which%20he,introduces%20the%20%E2%80%9Cprinciple%20of%20maximum",
                "pub_date": null,
                "snippet": "and information entropy, which he immediately identified as a significant shift in perspective. In this paper, he showed that entropy as expressed in the work of Boltzmann and Gibbs in the field of thermodynamics and Shannon entropy follow from the same underlying logic, thus establishing entropy as a general concept. Maximum entropy inference is introduced and in sections 3, 4, and 5 examples are given showing how the application of the principle of maximum entropy results in a conceptual shift in the approach to statistical mechanics. Jaynes proved that statistical mechanics can be interpreted in non-physical terms as statistical inference. In his 1957 paper he introduces the \u201cprinciple of maximum",
                "attribution": "sfipress.org"
              },
              {
                "matched_text": "\u30107\u2020L455-L463\u3011",
                "start_idx": 14439,
                "end_idx": 14452,
                "alt": null,
                "type": "webpage_extended",
                "title": "Entropy, Shannon\u2019s Measure of Information and Boltzmann\u2019s H-Theorem",
                "url": "https://www.mdpi.com/1099-4300/19/2/48#:~:text=Shannon%20Measure%20of%20Information%20,the%20Second%20Law%20of%20Thermodynamics",
                "pub_date": null,
                "snippet": "Shannon Measure of Information (SMI) provides a solid and quantitative basis for the interpretation of the thermodynamic entropy. The entropy measures the uncertainty in the distribution of the locations and momenta of all the particles; as well as two corrections due to the uncertainty principle and the indistinguishability of the particles. Finally we show that the H-function as defined by Boltzmann is an SMI but not entropy. Therefore; much of what has been written on the H-theorem is irrelevant to entropy and the Second Law of Thermodynamics.",
                "attribution": "mdpi.com"
              },
              {
                "matched_text": "\u30107\u2020L493-L500\u3011",
                "start_idx": 14452,
                "end_idx": 14465,
                "alt": null,
                "type": "webpage_extended",
                "title": "Entropy, Shannon\u2019s Measure of Information and Boltzmann\u2019s H-Theorem",
                "url": "https://www.mdpi.com/1099-4300/19/2/48#:~:text=In%201948%2C%20Shannon%20sought%20and,both%20information%20theory%20and%20thermodynamics",
                "pub_date": null,
                "snippet": "In 1948, Shannon sought and found a remarkable measure of information, of uncertainty [1 , 82] and unlikelihood. It was not a measure of any information, not any uncertainty about any proposition, and not the unlikelihood about the occurrence of any event. However, because the quantity he found has the same mathematical form as the entropy in statistical mechanics, he called his measure, as allegedly suggested by von Neumann: \u201centropy\u201d. This proved to be a grievous mistake which had caused a great confusion in both information theory and thermodynamics.",
                "attribution": "mdpi.com"
              },
              {
                "matched_text": "\u30107\u2020L456-L464\u3011",
                "start_idx": 14635,
                "end_idx": 14648,
                "alt": null,
                "type": "webpage_extended",
                "title": "Entropy, Shannon\u2019s Measure of Information and Boltzmann\u2019s H-Theorem",
                "url": "https://www.mdpi.com/1099-4300/19/2/48#:~:text=for%20the%20interpretation%20of%20the,the%20Second%20Law%20of%20Thermodynamics",
                "pub_date": null,
                "snippet": "for the interpretation of the thermodynamic entropy. The entropy measures the uncertainty in the distribution of the locations and momenta of all the particles; as well as two corrections due to the uncertainty principle and the indistinguishability of the particles. Finally we show that the H-function as defined by Boltzmann is an SMI but not entropy. Therefore; much of what has been written on the H-theorem is irrelevant to entropy and the Second Law of Thermodynamics.",
                "attribution": "mdpi.com"
              },
              {
                "matched_text": "\u301021\u2020L42-L46\u3011",
                "start_idx": 15504,
                "end_idx": 15516,
                "alt": null,
                "type": "webpage_extended",
                "title": "Landauer's_principle",
                "url": "https://www.chemeurope.com/en/encyclopedia/Landauer%27s_principle.html#:~:text=Landauer%27s%20Principle%2C%20first%20argued%20in,%28Bennett%202003%29%5B2",
                "pub_date": null,
                "snippet": "Landauer's Principle, first argued in 1961[1] by Rolf Landauer of IBM, holds that \"any logically irreversible manipulation of information, such as the erasure of a bit or the merging of two computation paths, must be accompanied by a corresponding entropy increase in non-information bearing degrees of freedom of the information processing apparatus or its environment\". (Bennett 2003)[2].",
                "attribution": "chemeurope.com"
              },
              {
                "matched_text": "\u301021\u2020L62-L68\u3011",
                "start_idx": 15516,
                "end_idx": 15528,
                "alt": null,
                "type": "webpage_extended",
                "title": "Landauer's_principle",
                "url": "https://www.chemeurope.com/en/encyclopedia/Landauer%27s_principle.html#:~:text=Specifically%2C%20each%20bit%20of%20lost,the%20study%20of%20reversible%20computing",
                "pub_date": null,
                "snippet": "Specifically, each bit of lost information will lead to the release of an amount kT ln 2 of heat, where k is the Boltzmann constant and T is the absolute temperature of the circuit. On the other hand, if no information is erased, computation may in principle be achieved which is thermodynamically reversible, and require no release of heat. This has led to considerable interest in the study of reversible computing.",
                "attribution": "chemeurope.com"
              },
              {
                "matched_text": "\u301021\u2020L62-L68\u3011",
                "start_idx": 15648,
                "end_idx": 15660,
                "alt": null,
                "type": "webpage_extended",
                "title": "Landauer's_principle",
                "url": "https://www.chemeurope.com/en/encyclopedia/Landauer%27s_principle.html#:~:text=Specifically%2C%20each%20bit%20of%20lost,the%20study%20of%20reversible%20computing",
                "pub_date": null,
                "snippet": "Specifically, each bit of lost information will lead to the release of an amount kT ln 2 of heat, where k is the Boltzmann constant and T is the absolute temperature of the circuit. On the other hand, if no information is erased, computation may in principle be achieved which is thermodynamically reversible, and require no release of heat. This has led to considerable interest in the study of reversible computing.",
                "attribution": "chemeurope.com"
              },
              {
                "matched_text": "\u301021\u2020L62-L68\u3011",
                "start_idx": 16232,
                "end_idx": 16244,
                "alt": null,
                "type": "webpage_extended",
                "title": "Landauer's_principle",
                "url": "https://www.chemeurope.com/en/encyclopedia/Landauer%27s_principle.html#:~:text=Specifically%2C%20each%20bit%20of%20lost,the%20study%20of%20reversible%20computing",
                "pub_date": null,
                "snippet": "Specifically, each bit of lost information will lead to the release of an amount kT ln 2 of heat, where k is the Boltzmann constant and T is the absolute temperature of the circuit. On the other hand, if no information is erased, computation may in principle be achieved which is thermodynamically reversible, and require no release of heat. This has led to considerable interest in the study of reversible computing.",
                "attribution": "chemeurope.com"
              },
              {
                "matched_text": "\u301057\u2020L152-L160\u3011",
                "start_idx": 16895,
                "end_idx": 16909,
                "alt": null,
                "type": "webpage_extended",
                "title": "Bekenstein bound - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Bekenstein_bound#:~:text=The%20universal%20form%20of%20the,field%20theory%20in%20curved%20spacetime",
                "pub_date": null,
                "snippet": "The universal form of the bound was originally found by Jacob Bekenstein in 1981 as the inequality [ 1 ][ 2 ][ 3 ] [Image 1: {\\displaystyle S\\leq {\\frac {2\\pi kRE}{\\hbar c}},}] where S is the  55, k is the Boltzmann constant , R is the  66 of a sphere  that can enclose the given system, E is the total  68 including any rest masses , \u0127 is the  70, and c is the speed of light . Note that while gravity plays a significant role in its enforcement, the expression for the bound does not contain the  72\u00a0G, and so, it ought to apply to quantum field theory in curved spacetime.",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301057\u2020L139-L147\u3011",
                "start_idx": 17030,
                "end_idx": 17044,
                "alt": null,
                "type": "webpage_extended",
                "title": "Bekenstein bound - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Bekenstein_bound#:~:text=In%20physics%20%2C%20the%20Bekenstein,and%20the%20energy%20are%20finite",
                "pub_date": null,
                "snippet": "In physics , the Bekenstein bound (named after  60) is an upper limit on the thermodynamic entropy  S, or  62 H, that can be contained within a given finite region of space which has a finite amount of energy\u2014or conversely, the maximum amount of information required to perfectly describe a given physical system down to the quantum level.[ 1 ] It implies that the information of a physical system, or the information necessary to perfectly describe that system, must be finite if the region of space and the energy are finite.",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301057\u2020L139-L147\u3011",
                "start_idx": 17324,
                "end_idx": 17338,
                "alt": null,
                "type": "webpage_extended",
                "title": "Bekenstein bound - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Bekenstein_bound#:~:text=In%20physics%20%2C%20the%20Bekenstein,and%20the%20energy%20are%20finite",
                "pub_date": null,
                "snippet": "In physics , the Bekenstein bound (named after  60) is an upper limit on the thermodynamic entropy  S, or  62 H, that can be contained within a given finite region of space which has a finite amount of energy\u2014or conversely, the maximum amount of information required to perfectly describe a given physical system down to the quantum level.[ 1 ] It implies that the information of a physical system, or the information necessary to perfectly describe that system, must be finite if the region of space and the energy are finite.",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301027\u2020L145-L153\u3011",
                "start_idx": 18012,
                "end_idx": 18026,
                "alt": null,
                "type": "webpage_extended",
                "title": "Von Neumann entropy - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Von_Neumann_entropy#:~:text=In%20physics%20%2C%20the%20von,ln%7D%20%7D%5D%20denotes%20the",
                "pub_date": null,
                "snippet": "In physics , the von Neumann entropy, named after  62, is a measure of the statistical uncertainty within a description of a quantum system. It extends the concept of Gibbs entropy  from classical  64 to quantum statistical mechanics , and it is the quantum counterpart of the  66 from classical information theory . For a quantum-mechanical system described by a  68 \u03c1, the von Neumann entropy is[ 1 ] [Image 0: {\\displaystyle S=-\\operatorname {tr} (\\rho \\ln \\rho ),}] where [Image 1: {\\displaystyle \\operatorname {tr} }] denotes the trace and [Image 2: {\\displaystyle \\operatorname {ln} }] denotes the",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301027\u2020L145-L153\u3011",
                "start_idx": 18147,
                "end_idx": 18161,
                "alt": null,
                "type": "webpage_extended",
                "title": "Von Neumann entropy - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Von_Neumann_entropy#:~:text=In%20physics%20%2C%20the%20von,ln%7D%20%7D%5D%20denotes%20the",
                "pub_date": null,
                "snippet": "In physics , the von Neumann entropy, named after  62, is a measure of the statistical uncertainty within a description of a quantum system. It extends the concept of Gibbs entropy  from classical  64 to quantum statistical mechanics , and it is the quantum counterpart of the  66 from classical information theory . For a quantum-mechanical system described by a  68 \u03c1, the von Neumann entropy is[ 1 ] [Image 0: {\\displaystyle S=-\\operatorname {tr} (\\rho \\ln \\rho ),}] where [Image 1: {\\displaystyle \\operatorname {tr} }] denotes the trace and [Image 2: {\\displaystyle \\operatorname {ln} }] denotes the",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301026\u2020L13-L16\u3011",
                "start_idx": 18382,
                "end_idx": 18394,
                "alt": null,
                "type": "webpage_extended",
                "title": "Quantum mutual information - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Quantum_mutual_information#:~:text=Quantum%20mutual%20information%20,1",
                "pub_date": null,
                "snippet": "Quantum mutual information - Wikipedia ... von Neumann, is a measure of correlation between subsystems of quantum state. It is the quantum mechanical analog of Shannon mutual information. Contents. 1 ...",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301035\u2020L136-L144\u3011",
                "start_idx": 19993,
                "end_idx": 20007,
                "alt": null,
                "type": "webpage_extended",
                "title": "W. Ross Ashby, Cybernetics and Requisite Variety (1956)",
                "url": "https://www.panarchy.org/ashby/variety.1956.html#:~:text=One%20is%20that%20it%20offers,concepts%20that%2C%20by%20having%20exact",
                "pub_date": null,
                "snippet": "One is that it offers a single vocabulary and a single set of concepts suitable for representing the most diverse types of systems. Until recently, any attempt to relate the many facts known about, say, servo-mechanisms to what was known about the cerebellum was made unnecessarily difficult by the fact that the properties of servo-mechanisms were described in words redolent of the automatic pilot, or the radio set, or the hydraulic brake, while those of the cerebellum were described in words redolent of the dissecting room and the bedside - aspects that are irrelevant to the similarities between a servo-mechanism and a cerebellar reflex. Cybernetics offers one set of concepts that, by having exact",
                "attribution": "panarchy.org"
              },
              {
                "matched_text": "\u301035\u2020L148-L157\u3011",
                "start_idx": 20007,
                "end_idx": 20021,
                "alt": null,
                "type": "webpage_extended",
                "title": "W. Ross Ashby, Cybernetics and Requisite Variety (1956)",
                "url": "https://www.panarchy.org/ashby/variety.1956.html#:~:text=It%20has%20been%20found%20repeatedly,be%20made%20use%20of%20in",
                "pub_date": null,
                "snippet": "It has been found repeatedly in science that the discovery that two branches are related leads to each branch helping in the development of the other. The result is often a markedly accelerated growth of both. The infinitesimal calculus and astronomy, the virus and the protein molecule, the chromosomes and heredity are examples that come to mind. Neither, of course, can give proofs about the laws of the other, but each can give suggestions that may be of the greatest assistance and fruitfulness. Here I need only mention the fact that cybernetics is likely to reveal a great number of interesting and suggestive parallelisms between machine and brain and society. And it can provide the common language by which discoveries in one branch can readily be made use of in",
                "attribution": "panarchy.org"
              },
              {
                "matched_text": "\u301039\u2020L1-L4\u3011",
                "start_idx": 20312,
                "end_idx": 20322,
                "alt": null,
                "type": "webpage_extended",
                "title": "Norbert Wiener",
                "url": "https://www.informationphilosopher.com/solutions/scientists/wiener/#:~:text=activity,132",
                "pub_date": null,
                "snippet": "activity. Information is information, not matter or energy. No materialism which does not admit this can survive at the present day. > > (Cybernetics, 2nd edition, p.132)",
                "attribution": "informationphilosopher.com"
              },
              {
                "matched_text": "\u301039\u2020L1-L4\u3011",
                "start_idx": 20334,
                "end_idx": 20344,
                "alt": null,
                "type": "webpage_extended",
                "title": "Norbert Wiener",
                "url": "https://www.informationphilosopher.com/solutions/scientists/wiener/#:~:text=activity,132",
                "pub_date": null,
                "snippet": "activity. Information is information, not matter or energy. No materialism which does not admit this can survive at the present day. > > (Cybernetics, 2nd edition, p.132)",
                "attribution": "informationphilosopher.com"
              },
              {
                "matched_text": "\u301036\u2020L1-L4\u3011",
                "start_idx": 21577,
                "end_idx": 21587,
                "alt": null,
                "type": "webpage_extended",
                "title": "W. Ross Ashby, Cybernetics and Requisite Variety (1956)",
                "url": "https://www.panarchy.org/ashby/variety.1956.html#:~:text=11%2F11,apply%20a%20measure%20to%20regulation",
                "pub_date": null,
                "snippet": "11/11. The Law of Requisite Variety [\"Only variety can destroy variety.\"] enables us to apply a measure to regulation.",
                "attribution": "panarchy.org"
              },
              {
                "matched_text": "\u301036\u2020L1-L4\u3011",
                "start_idx": 21926,
                "end_idx": 21936,
                "alt": null,
                "type": "webpage_extended",
                "title": "W. Ross Ashby, Cybernetics and Requisite Variety (1956)",
                "url": "https://www.panarchy.org/ashby/variety.1956.html#:~:text=11%2F11,apply%20a%20measure%20to%20regulation",
                "pub_date": null,
                "snippet": "11/11. The Law of Requisite Variety [\"Only variety can destroy variety.\"] enables us to apply a measure to regulation.",
                "attribution": "panarchy.org"
              },
              {
                "matched_text": "\u301043\u2020L270-L278\u3011",
                "start_idx": 25017,
                "end_idx": 25031,
                "alt": null,
                "type": "webpage_extended",
                "title": "What Is Life? - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/What_Is_Life%3F#:~:text=second%20law%20of%20thermodynamics%20%2C,in%20an%20%20104",
                "pub_date": null,
                "snippet": "second law of thermodynamics , according to which  100 only increases in a closed system  (such as the universe). Schr\u00f6dinger explains that living matter evades the decay to  102 by homeostatically maintaining negative entropy  in an  104.",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301043\u2020L270-L278\u3011",
                "start_idx": 25485,
                "end_idx": 25499,
                "alt": null,
                "type": "webpage_extended",
                "title": "What Is Life? - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/What_Is_Life%3F#:~:text=second%20law%20of%20thermodynamics%20%2C,in%20an%20%20104",
                "pub_date": null,
                "snippet": "second law of thermodynamics , according to which  100 only increases in a closed system  (such as the universe). Schr\u00f6dinger explains that living matter evades the decay to  102 by homeostatically maintaining negative entropy  in an  104.",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301051\u2020L5-L13\u3011",
                "start_idx": 26858,
                "end_idx": 26869,
                "alt": null,
                "type": "webpage_extended",
                "title": "book_X dist.pdf",
                "url": "https://assets.cambridge.org/97805218/02932/frontmatter/9780521802932_frontmatter.pdf#:~:text=biology,computer%20competent%20reader%20who%20is",
                "pub_date": null,
                "snippet": "biology. The genetical information system, because it is linear and digital, resembles the algorithmic language of computers. George Gamow pointed out that the application of Shannon\u2019s information theory breaks genetics and molecular biology out of the descriptive mode into the quantitative mode, and Dr. Yockey develops this theme, discussing how information theory and coding theory can be applied to molecular biology. He discusses how these tools for measuring the information in the sequences of the genome and the proteome are essential for our complete understanding of the nature and origin of life. The author writes for the computer competent reader who is",
                "attribution": "assets.cambridge.org"
              },
              {
                "matched_text": "\u301051\u2020L5-L13\u3011",
                "start_idx": 27093,
                "end_idx": 27104,
                "alt": null,
                "type": "webpage_extended",
                "title": "book_X dist.pdf",
                "url": "https://assets.cambridge.org/97805218/02932/frontmatter/9780521802932_frontmatter.pdf#:~:text=biology,computer%20competent%20reader%20who%20is",
                "pub_date": null,
                "snippet": "biology. The genetical information system, because it is linear and digital, resembles the algorithmic language of computers. George Gamow pointed out that the application of Shannon\u2019s information theory breaks genetics and molecular biology out of the descriptive mode into the quantitative mode, and Dr. Yockey develops this theme, discussing how information theory and coding theory can be applied to molecular biology. He discusses how these tools for measuring the information in the sequences of the genome and the proteome are essential for our complete understanding of the nature and origin of life. The author writes for the computer competent reader who is",
                "attribution": "assets.cambridge.org"
              },
              {
                "matched_text": "\u301028\u2020L28-L36\u3011",
                "start_idx": 29452,
                "end_idx": 29464,
                "alt": null,
                "type": "webpage_extended",
                "title": "New 'assembly theory' unifies physics and biology to explain evolution, complexity | ASU News",
                "url": "https://news.asu.edu/20231005-new-assembly-theory-unifies-physics-and-biology-explain-evolution-complexity#:~:text=This%20research%20builds%20on%20the,derived%20molecules",
                "pub_date": null,
                "snippet": "This research builds on the team's previous work developing assembly theory as an empirically validated approach to life detection, with implications for the search for alien life and efforts to evolve new life forms in the laboratory. In prior work, the team assigned a complexity score to molecules called the molecular assembly index, based on the minimal number of bond-forming steps required to build a molecule. They showed how this index is experimentally measurable and how high values correlate with life-derived molecules.",
                "attribution": "news.asu.edu"
              },
              {
                "matched_text": "\u301028\u2020L36-L44\u3011",
                "start_idx": 29824,
                "end_idx": 29836,
                "alt": null,
                "type": "webpage_extended",
                "title": "New 'assembly theory' unifies physics and biology to explain evolution, complexity | ASU News",
                "url": "https://news.asu.edu/20231005-new-assembly-theory-unifies-physics-and-biology-explain-evolution-complexity#:~:text=The%20new%20study%20introduces%20mathematical,their%20abundance%20and%20assembly%20indices",
                "pub_date": null,
                "snippet": "The new study introduces mathematical formalism around a physical quantity called \"assembly\" that captures how much selection is required to produce a given set of complex objects, based on their abundance and assembly indices.",
                "attribution": "news.asu.edu"
              },
              {
                "matched_text": "\u301028\u2020L40-L48\u3011",
                "start_idx": 30176,
                "end_idx": 30188,
                "alt": null,
                "type": "webpage_extended",
                "title": "New 'assembly theory' unifies physics and biology to explain evolution, complexity | ASU News",
                "url": "https://news.asu.edu/20231005-new-assembly-theory-unifies-physics-and-biology-explain-evolution-complexity#:~:text=,Evolution%20and%20the%20Beyond%20Center",
                "pub_date": null,
                "snippet": "\"Assembly theory provides a completely new lens for looking at physics, chemistry and biology as different perspectives of the same underlying reality,\" explained lead author Sara Walker, a theoretical physicist, origin of life researcher and professor with appointments in Arizona State University's School of Earth and Space Exploration,\u00a0Biosocial Complexity Initiative,\u00a0Center for Social Dynamics and Complexity (CSDC), Biodesign Center for Mechanisms of Evolution and the Beyond Center.",
                "attribution": "news.asu.edu"
              },
              {
                "matched_text": "\u301028\u2020L21-L29\u3011",
                "start_idx": 30500,
                "end_idx": 30512,
                "alt": null,
                "type": "webpage_extended",
                "title": "New 'assembly theory' unifies physics and biology to explain evolution, complexity | ASU News",
                "url": "https://news.asu.edu/20231005-new-assembly-theory-unifies-physics-and-biology-explain-evolution-complexity#:~:text=An%20international%20team%20of%20researchers,physical%20laws%20of%20the%20universe",
                "pub_date": null,
                "snippet": "An international team of researchers has developed a new theoretical framework that bridges physics and biology to provide a unified approach for understanding how complexity and evolution emerge in nature. This new work on \"assembly theory,\" published in Nature, represents a major advance in our fundamental comprehension of biological evolution and how it is governed by the physical laws of the universe.",
                "attribution": "news.asu.edu"
              },
              {
                "matched_text": "\u301028\u2020L58-L64\u3011",
                "start_idx": 30512,
                "end_idx": 30524,
                "alt": null,
                "type": "webpage_extended",
                "title": "New 'assembly theory' unifies physics and biology to explain evolution, complexity | ASU News",
                "url": "https://news.asu.edu/20231005-new-assembly-theory-unifies-physics-and-biology-explain-evolution-complexity#:~:text=,chemistry%2C%20biology%20and%20information%20theory",
                "pub_date": null,
                "snippet": "\"Assembly theory provides an entirely new way to look at the matter that makes up our world, as defined not just by immutable particles but by the memory needed to build objects through selection over time,\" said Professor Lee Cronin, a chemist from the University of Glasgow and co-lead author. \"With further work, this approach has the potential to transform fields from cosmology to computer science. It represents a new frontier at the intersection of physics, chemistry, biology and information theory.\"",
                "attribution": "news.asu.edu"
              },
              {
                "matched_text": "\u301029\u2020L5-L8\u3011",
                "start_idx": 31616,
                "end_idx": 31626,
                "alt": null,
                "type": "webpage_extended",
                "title": "An Overview of the Free Energy Principle and Related Research",
                "url": "https://direct.mit.edu/neco/article/36/5/963/119791/An-Overview-of-the-Free-Energy-Principle-and#:~:text=An%20Overview%20of%20the%20Free,implementing%20perception%20through%20Bayesian",
                "pub_date": null,
                "snippet": "An Overview of the Free Energy Principle and Related Research The free energy principle states that the human brain operates as a Bayesian inference machine, implementing perception through Bayesian ...",
                "attribution": "direct.mit.edu"
              },
              {
                "matched_text": "\u301030\u2020L162-L170\u3011",
                "start_idx": 31728,
                "end_idx": 31742,
                "alt": null,
                "type": "webpage_extended",
                "title": "\n            Answering Schr\u00f6dinger's question: A free-energy formulation - PMC\n        ",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC5857288/#:~:text=The%20free,Homo%20sapiens%2C%20before%20translating%20variational",
                "pub_date": null,
                "snippet": "The free-energy principle (FEP) is a formal model of neuronal processes that is widely recognised in neuroscience as a unifying theory of the brain and biobehaviour. More recently, however, it has been extended beyond the brain to explain the dynamics of living systems, and their unique capacity to avoid decay. The aim of this review is to synthesise these advances with a meta- theoretical ontology of biological systems called variational neuroethology, which integrates the FEP with Tinbergen's four research questions to explain biological systems across spatial and temporal scales. We exemplify this framework by applying it to Homo sapiens, before translating variational",
                "attribution": "pmc.ncbi.nlm.nih.gov"
              },
              {
                "matched_text": "\u301030\u2020L162-L170\u3011",
                "start_idx": 32495,
                "end_idx": 32509,
                "alt": null,
                "type": "webpage_extended",
                "title": "\n            Answering Schr\u00f6dinger's question: A free-energy formulation - PMC\n        ",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC5857288/#:~:text=The%20free,Homo%20sapiens%2C%20before%20translating%20variational",
                "pub_date": null,
                "snippet": "The free-energy principle (FEP) is a formal model of neuronal processes that is widely recognised in neuroscience as a unifying theory of the brain and biobehaviour. More recently, however, it has been extended beyond the brain to explain the dynamics of living systems, and their unique capacity to avoid decay. The aim of this review is to synthesise these advances with a meta- theoretical ontology of biological systems called variational neuroethology, which integrates the FEP with Tinbergen's four research questions to explain biological systems across spatial and temporal scales. We exemplify this framework by applying it to Homo sapiens, before translating variational",
                "attribution": "pmc.ncbi.nlm.nih.gov"
              },
              {
                "matched_text": "\u301025\u2020L53-L60\u3011",
                "start_idx": 34855,
                "end_idx": 34867,
                "alt": null,
                "type": "webpage_extended",
                "title": "It from bit? | plus.maths.org",
                "url": "https://plus.maths.org/content/it-bit-0#:~:text=,no%20questions%2C%20binary%20choices%2C%20bits",
                "pub_date": null,
                "snippet": "\"Every it \u2013 every particle, every field of force, even the space-time continuum itself _ derives its function, its meaning, its very existence entirely... from the apparatus-elected answers to yes or no questions, binary choices, bits.\"",
                "attribution": "plus.maths.org"
              },
              {
                "matched_text": "\u301025\u2020L53-L60\u3011",
                "start_idx": 34892,
                "end_idx": 34904,
                "alt": null,
                "type": "webpage_extended",
                "title": "It from bit? | plus.maths.org",
                "url": "https://plus.maths.org/content/it-bit-0#:~:text=,no%20questions%2C%20binary%20choices%2C%20bits",
                "pub_date": null,
                "snippet": "\"Every it \u2013 every particle, every field of force, even the space-time continuum itself _ derives its function, its meaning, its very existence entirely... from the apparatus-elected answers to yes or no questions, binary choices, bits.\"",
                "attribution": "plus.maths.org"
              },
              {
                "matched_text": "\u301053\u2020L6-L13\u3011",
                "start_idx": 35956,
                "end_idx": 35967,
                "alt": null,
                "type": "webpage_extended",
                "title": "Programming the Universe: A Quantum Computer Scientist Takes ...",
                "url": "https://www.amazon.com/Programming-Universe-Quantum-Computer-Scientist/dp/1400033861#:~:text=The%20universe%20is%20a%20quantum,As",
                "pub_date": null,
                "snippet": "The universe is a quantum computer. This begs the question: What does the universe compute? It computes itself. The universe computes its own behavior. As ...",
                "attribution": "amazon.com"
              },
              {
                "matched_text": "\u301053\u2020L15-L18\u3011",
                "start_idx": 35967,
                "end_idx": 35979,
                "alt": null,
                "type": "webpage_extended",
                "title": "The universe as quantum computer - ResearchGate",
                "url": "https://www.researchgate.net/publication/303418850_The_universe_as_quantum_computer#:~:text=The%20universe%20as%20quantum%20computer,Theoretic",
                "pub_date": null,
                "snippet": "The universe as quantum computer - ResearchGate Lloyd's thesis: 4 The universe is a quantum computer [98, 196, 197]. ... Fundamental Physics and Computation: The Computer-Theoretic ...",
                "attribution": "researchgate.net"
              },
              {
                "matched_text": "\u301055\u2020L1-L4\u3011",
                "start_idx": 36167,
                "end_idx": 36177,
                "alt": null,
                "type": "webpage_extended",
                "title": "Programming the Universe - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Programming_the_Universe#:~:text=proposes%20that%20the%20Universe%20is,fully%20simulated%20using%20a%20quantum",
                "pub_date": null,
                "snippet": "proposes that the Universe is a quantum computer (supercomputer), and advances in the understanding of physics may come from viewing entropy as a phenomenon of information, rather than simply thermodynamics. Lloyd also postulates that the Universe can be fully simulated using a quantum",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u30104\u2020L5-L13\u3011",
                "start_idx": 37625,
                "end_idx": 37635,
                "alt": null,
                "type": "webpage_extended",
                "title": "Amazon.com: EMERGENT INFORMATION: A UNIFIED THEORY OF ...",
                "url": "https://www.amazon.com/Emergent-Information-Unified-Framework-Scientific/dp/9814313483#:~:text=Amazon,13",
                "pub_date": null,
                "snippet": "Amazon.com: EMERGENT INFORMATION: A UNIFIED THEORY OF ... Wolfgang Hofkirchner. EMERGENT INFORMATION: A UNIFIED THEORY OF INFORMATION FRAMEWORK (World Scientific Series in Information Studies, 3). 1st Edition. ISBN-13 ...",
                "attribution": "amazon.com"
              },
              {
                "matched_text": "\u301037\u2020L9-L17\u3011",
                "start_idx": 37822,
                "end_idx": 37833,
                "alt": null,
                "type": "webpage_extended",
                "title": "A Concise Account of Information as Meaning Ascribed to Symbols ...",
                "url": "https://www.mdpi.com/1099-4300/25/1/177#:~:text=A%20Concise%20Account%20of%20Information,here%20are%20in%20line",
                "pub_date": null,
                "snippet": "A Concise Account of Information as Meaning Ascribed to Symbols ... As mathematician and philosopher N. Wiener [15] put it, \u201cInformation is information, not matter or energy.\u201d Therefore, the arguments presented here are in line ...",
                "attribution": "mdpi.com"
              },
              {
                "matched_text": "\u30103\u2020L234-L243\u3011",
                "start_idx": 39528,
                "end_idx": 39541,
                "alt": null,
                "type": "webpage_extended",
                "title": "A grand unified theory for the unification of physics, life, information and cognition (mind) | Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",
                "url": "https://royalsocietypublishing.org/doi/10.1098/rsta.2022.0277#:~:text=The%20modern%20scientific%20world%20view,An%20expanded",
                "pub_date": null,
                "snippet": "The modern scientific world view was built on the incommensurability between cognition (mind) and physics (matter) and later life and physics (the autonomy of biology). Fuelled by Boltzmann's view of the second law of thermodynamics as a \u2018law of disorder\u2019, the idea of \u2018two opposing rivers\u2019, the river of physics \u2018flowing down\u2019 to disorder and the river of life and mind \u2018flowing up\u2019 to higher states of order became a cornerstone of contemporary thinking. The deleterious result of this paradigmatic separation of physics, life and mind has been to considerably incapacitate each by bracketing many of the deepest problems of science, including the very nature of life itself and its cognitive capabilities, outside the theoretical reach of contemporary science. An expanded",
                "attribution": "royalsocietypublishing.org"
              },
              {
                "matched_text": "\u30103\u2020L244-L252\u3011",
                "start_idx": 39541,
                "end_idx": 39554,
                "alt": null,
                "type": "webpage_extended",
                "title": "A grand unified theory for the unification of physics, life, information and cognition (mind) | Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",
                "url": "https://royalsocietypublishing.org/doi/10.1098/rsta.2022.0277#:~:text=view%20of%20physics%2C%20notably%20the,modern%20science%20associated%20with%20it",
                "pub_date": null,
                "snippet": "view of physics, notably the addition of the fourth law of thermodynamics (LMEP), or the law of maximum entropy production, coupled with first law time- translation symmetry, and the self-referencing circularity of the relational ontology of autocatakinetic systems, provides the basis for a grand unified theory unifying physics, life, information and cognition (mind). This dissolves the dysfunctional myth of the two rivers, and solves the previously insoluble problems at the foundations of modern science associated with it.",
                "attribution": "royalsocietypublishing.org"
              },
              {
                "matched_text": "\u30103\u2020L242-L250\u3011",
                "start_idx": 39793,
                "end_idx": 39806,
                "alt": null,
                "type": "webpage_extended",
                "title": "A grand unified theory for the unification of physics, life, information and cognition (mind) | Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",
                "url": "https://royalsocietypublishing.org/doi/10.1098/rsta.2022.0277#:~:text=science%2C%20including%20the%20very%20nature,modern%20science%20associated%20with%20it",
                "pub_date": null,
                "snippet": "science, including the very nature of life itself and its cognitive capabilities, outside the theoretical reach of contemporary science. An expanded view of physics, notably the addition of the fourth law of thermodynamics (LMEP), or the law of maximum entropy production, coupled with first law time- translation symmetry, and the self-referencing circularity of the relational ontology of autocatakinetic systems, provides the basis for a grand unified theory unifying physics, life, information and cognition (mind). This dissolves the dysfunctional myth of the two rivers, and solves the previously insoluble problems at the foundations of modern science associated with it.",
                "attribution": "royalsocietypublishing.org"
              },
              {
                "matched_text": "\u30109\u2020L283-L291\u3011",
                "start_idx": 42716,
                "end_idx": 42729,
                "alt": null,
                "type": "webpage_extended",
                "title": "Information theory - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Information_theory#:~:text=%5D,study%20of%20%20216%20outer",
                "pub_date": null,
                "snippet": "][ 3 ] The theory has also found applications in other areas, including statistical inference ,[ 7 ]  198, neurobiology ,[ 8 ]  200,[ 9 ] signal processing ,[ 2 ]  202, the evolution[ 10 ] and function[ 11 ] of molecular codes (bioinformatics ),  204,[ 12 ] molecular dynamics ,[ 13 ]  206, quantum computing ,  208, intelligence gathering ,  210,[ 14 ] pattern recognition ,  212,[ 15 ] the analysis of music ,[ 16 ][ 17 ]  214,[ 18 ] imaging system design,[ 19 ] study of  216 outer",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301057\u2020L139-L147\u3011",
                "start_idx": 44013,
                "end_idx": 44027,
                "alt": null,
                "type": "webpage_extended",
                "title": "Bekenstein bound - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Bekenstein_bound#:~:text=In%20physics%20%2C%20the%20Bekenstein,and%20the%20energy%20are%20finite",
                "pub_date": null,
                "snippet": "In physics , the Bekenstein bound (named after  60) is an upper limit on the thermodynamic entropy  S, or  62 H, that can be contained within a given finite region of space which has a finite amount of energy\u2014or conversely, the maximum amount of information required to perfectly describe a given physical system down to the quantum level.[ 1 ] It implies that the information of a physical system, or the information necessary to perfectly describe that system, must be finite if the region of space and the energy are finite.",
                "attribution": "en.wikipedia.org"
              },
              {
                "matched_text": "\u301025\u2020L53-L60\u3011",
                "start_idx": 44027,
                "end_idx": 44039,
                "alt": null,
                "type": "webpage_extended",
                "title": "It from bit? | plus.maths.org",
                "url": "https://plus.maths.org/content/it-bit-0#:~:text=,no%20questions%2C%20binary%20choices%2C%20bits",
                "pub_date": null,
                "snippet": "\"Every it \u2013 every particle, every field of force, even the space-time continuum itself _ derives its function, its meaning, its very existence entirely... from the apparatus-elected answers to yes or no questions, binary choices, bits.\"",
                "attribution": "plus.maths.org"
              }
            ],
            "timestamp_": "absolute",
            "message_type": null,
            "is_async_task_result_message": true,
            "b1de6e2_rm": true,
            "async_task_id": "task_5FLsdpdbjGT9fntC7vOEJ7Lv",
            "async_task_title": "Grand Unification of Information Across Disciplines"
          },
          "recipient": "all",
          "channel": "final"
        },
        "parent": "945cef7b-8a77-49cd-bf99-4c16e003333c",
        "children": [
          "815c3cb0-ba36-4eb4-bd35-3fef187f1556"
        ]
      },
      "815c3cb0-ba36-4eb4-bd35-3fef187f1556": {
        "id": "815c3cb0-ba36-4eb4-bd35-3fef187f1556",
        "message": {
          "id": "815c3cb0-ba36-4eb4-bd35-3fef187f1556",
          "author": {
            "role": "user",
            "name": null,
            "metadata": {}
          },
          "create_time": 1738725889.169699,
          "update_time": null,
          "content": {
            "content_type": "text",
            "parts": [
              "hm, I'm still unsatisfied. that all seems very surface level. What I still don't understand is why, physically, do different configurations of matter have vastly different ability to affect the flow energy and matter in the universe? Specifically, why does *knowledge* have such a profound effect on the state of the world. If someone magically got the plans a light speed engine uploaded into their brain, the entire course of the world would change entirely. Less dramatically, scientific theories change the course of humanity all the time, and these are typically inspired ideas coming from one bright mind. Why do universal constructors come about? Physics isn't very helpful whenever we are talking about systems dealing with powerful knowledge, and instead we get new emergent models like psychology. I want a physics of information, or at least to know why its not possible. Who is asking these questions?"
            ]
          },
          "status": "finished_successfully",
          "end_turn": null,
          "weight": 1.0,
          "metadata": {
            "serialization_metadata": {
              "custom_symbol_offsets": []
            },
            "dictation": false,
            "request_id": "90cfca23798baab9-IAD",
            "message_source": null,
            "timestamp_": "absolute",
            "message_type": null
          },
          "recipient": "all",
          "channel": null
        },
        "parent": "79c763e0-d714-4804-b391-d7987e58123b",
        "children": [
          "c9bcbfce-e659-4008-accd-120577bec542"
        ]
      },
      "c9bcbfce-e659-4008-accd-120577bec542": {
        "id": "c9bcbfce-e659-4008-accd-120577bec542",
        "message": {
          "id": "c9bcbfce-e659-4008-accd-120577bec542",
          "author": {
            "role": "assistant",
            "name": null,
            "metadata": {}
          },
          "create_time": 1738725931.295019,
          "update_time": null,
          "content": {
            "content_type": "text",
            "parts": [
              "Below is an expanded, deeper dive that attempts to go beyond enumerating existing schools. It addresses your specific question: **why**, from a physics perspective, does \u201cinformation\u201d (or \u201cknowledge\u201d) so profoundly impact the flow of energy and matter? We will go through historical context, some attempts at a \u201cphysics of knowledge,\u201d partially resolved questions, and open problems in the literature. This time, I\u2019ll highlight the core question: *What is so special about certain informational patterns that they can drastically alter physical processes?* In doing so, I\u2019ll also point to some researchers who are explicitly asking about such \u201cphysics of universal constructors,\u201d emergent agency, and knowledge.\n\n---\n\n## 1. The Core Puzzle: Knowledge Shapes Physical Processes\n\n**Your question in short:** *How is it that certain configurations of matter (like a brain that has discovered a new scientific principle or engineering design) can so radically reshape the flows of matter and energy in the universe?* This puzzle goes beyond standard physics in the sense that typical physical laws (Newton\u2019s laws, quantum mechanics, relativity, etc.) are time-symmetric and do not distinguish \u201cknowledgeable\u201d configurations from random ones. Indeed, from a purely reductionist viewpoint, the universe is just evolving by local interactions of fields and particles under some fundamental laws\u2014there\u2019s no immediate reason to suspect that a blueprint in a mind or in a storage device should cause large-scale changes *unless* we bring in a higher-level viewpoint.\n\n### 1.1. Physics Typically Lacks a \u201cKnowledge\u201d Variable\nClassical physics does not have an internal concept of \u201cinformation\u201d or \u201cknowledge.\u201d Particles have positions and momenta, fields have amplitudes, etc. Even in quantum field theory, \u201cinformation\u201d is typically about *states*, *Hilbert spaces*, and so forth. There is no separate variable for \u201cthe blueprint of the warp drive is in that person\u2019s brain.\u201d Hence, describing the effect of knowledge or ideas on the dynamics of matter must involve *emergent* frameworks: e.g., describing an organism\u2019s behavior in terms of stored genetic information or learned concepts. The challenge is bridging the gap between:\n\n- **Microscopic laws**: local interactions of fundamental constituents.  \n- **Macroscale, knowledge-driven events**: e.g. building a spaceship or reorganizing the economy based on a new idea.\n\n**Thermodynamics** and **statistical mechanics** gave us a partial bridge by linking the microstates to macro variables such as entropy and temperature. But \u201cknowledge\u201d is subtler than just low entropy. \u201cKnowledge\u201d is *functionally relevant* structure\u2014patterns that can *do something*.\n\n---\n\n## 2. Historical Flashpoints in the Physics-of-Knowledge Question\n\n### 2.1. Maxwell\u2019s Demon and Measurement\nIn 1867, Maxwell imagines a \u201cdemon\u201d that obtains knowledge of molecular velocities and uses it to decrease entropy. This scenario forced physicists to consider the **cost of measurement** and **erasure**\u2014in other words, how \u201cinformation about the system\u201d can feed back to reorganize matter/energy flows. The resolution (Szil\u00e1rd, Landauer, Bennett) was that measurement and erasure have thermodynamic costs, saving the second law. This was an early demonstration that *knowledge matters physically*.\n\n**Why?** Because knowledge (information about microstates) allows you to do different \u201cconditional actions\u201d\u2014a valve that opens only if a fast particle is incoming. That \u201cconditional\u201d behavior means the system\u2019s dynamics are no longer symmetrical. The control system can harness random fluctuations for macroscopic work, but the cost is in obtaining (and eventually erasing) that information. Summation: *knowledge is physically potent* because it enables directed action that depends on microstates. So from a physics viewpoint, \u201cknowledge\u201d is effectively a *correlation* between the controller\u2019s state and the environment\u2019s state. Once correlated, the controller can leverage that correlation to bias the flow of energy.\n\n### 2.2. Universal Computation and Universal Construction\n**John von Neumann** (1940s\u20131950s) studied \u201cuniversal constructors\u201d (machines capable of constructing any machine, including copies of themselves, given a suitable blueprint and raw materials). In his cellular automaton model, a universal constructor can read its own description and replicate itself. This was arguably the first explicit model of how *information* (the blueprint) plus raw materials can lead to the open-ended creation of new structures. This points directly to your question: *why does some \u201cknowledge\u201d cause dramatic changes in the world?* Because universal constructors\u2014once you have them\u2014can exploit their knowledge (blueprint) to keep building new things. A single blueprint can scale up *indefinitely*.\n\n### 2.3. Jaynes\u2019s \u201cPredictive\u201d Vision of Statistical Mechanics\nIn 1957, E.T. **Jaynes** introduced the principle of maximum entropy as an inference rule. He explicitly cast thermodynamics as *statistical inference about microstates.* In this viewpoint, if a system \u201cknows\u201d certain constraints, that knowledge changes the assigned distribution. Again, we see that information can shape the predicted or actual macroscopic outcomes. \n\n### 2.4. \u201cIt from Bit\u201d\u2014Wheeler and the Universe as Information\n**John Wheeler** proposed that all physical phenomena might be grounded in *binary questions*\u2014fundamental yes/no measurements. He asked how observers\u2019 measurements could shape reality. While highly philosophical, it was a rallying cry for investigating a \u201cphysics of information\u201d where the presence of observers or knowledge has nontrivial consequences (particularly in quantum measurement).\n\n---\n\n## 3. Modern Directions: Agency, Autonomy, and Emergent Causation\n\nIn recent decades, there\u2019s a renewed push to describe \u201cknowledge,\u201d \u201cagency,\u201d and \u201cgoal-directed behavior\u201d in physical/mathematical terms. Some relevant efforts:\n\n1. **Friston\u2019s Free Energy Principle**  \n   - Argues that living systems \u201cminimize variational free energy,\u201d which can be interpreted as *maintaining accurate internal models (knowledge) of the environment*.  \n   - Once an organism has a sufficiently predictive model, it can *act* to reduce surprise\u2014thereby dramatically reorganizing local resources.  \n   - \u201cKnowledge\u201d in this sense is the *generative model* encoded in the system\u2019s internal states. That model shapes the system\u2019s interactions with the environment.  \n\n2. **Information-Theoretic Approaches to Agency**  \n   - Many researchers (e.g., **Alex Wissner-Gross** on \u201ccausal entropic forces,\u201d **V. Griffith, E. Hoel** on \u201ccausal emergence\u201d) are investigating how maximizing or constraining certain information measures leads to emergent \u201cagent-like\u201d behavior that reorganizes the environment.  \n   - E.g. \u201ccausal emergence\u201d frameworks show that at higher scales, certain patterns have more robust causal power than the micro-details do. This might explain how \u201cknowledge structures\u201d (e.g., beliefs, theories) can have large-scale causal effects, even though physically they\u2019re just patterns of neuronal firing.\n\n3. **Assembly Theory**  \n   - Sara Walker and Lee Cronin\u2019s approach (cited in the earlier answer) tries to quantify how certain \u201cknowledge-driven\u201d configurations can appear. E.g., a molecule with a high assembly index. In principle, if a system \u201cknows\u201d how to build complex objects, you get a combinatorial explosion of possible artifacts.  \n   - This is a step toward a physics that explicitly acknowledges the role of \u201chistorical information\u201d or \u201cconstruction protocols.\u201d\n\n4. **Artificial Life and Open-Ended Evolution**  \n   - Researchers like **Chris Langton, Mark Bedau, Alastair Channon, Tim Taylor** (and many others) ask: how do novel structures keep emerging indefinitely in biological or computational systems? A big part is *information accumulation* over time\u2014organisms discover \u201cnew tricks,\u201d store that knowledge genetically/culturally, and exploit it.  \n   - This can drastically reorganize the environment (think of how photosynthesis changed the Earth\u2019s atmosphere!). So \u201cknowledge\u201d is effectively an internal model or a genotype that encodes solutions. The open question is how to unify such processes with standard physics in a single predictive framework.\n\n5. **Living Technology & Universal Constructor Theory**  \n   - **Chiara Marletto** and **David Deutsch** explore \u201cconstructor theory,\u201d a new fundamental formulation of physics. Instead of laws of motion, they propose laws about *which transformations are possible/impossible*\u2014and universal constructors are physical objects that can carry out any possible transformation if given the right information/recipe.  \n   - Knowledge, in constructor theory, is a *physically instantiated substrate that can act* to bring about transformations (like building a spaceship). This is a direct attempt to embed \u201cknowledge\u201d and \u201cinformation\u201d into fundamental physics. Deutsch especially has argued that knowledge is \u201cphysical and has causal power.\u201d  \n\n**Hence, the short answer** to \u201cWho\u2019s asking these questions?\u201d:  \n- Karl Friston and colleagues (in theoretical neuroscience/biology).  \n- David Deutsch and Chiara Marletto (constructor theory, quantum computation).  \n- Sara Walker, Lee Cronin, Addy Pross (origins of life, assembly theory).  \n- James Ladyman, Karoline Wiesner (the philosophy of information, complexity).  \n- The Santa Fe Institute crowd (G. West, J. Goldenfeld, Jessica Flack, etc.) approach emergent phenomena, collective intelligence, and how knowledge shapes system dynamics.  \n\n---\n\n## 4. Mechanisms By Which Knowledge Reorganizes Matter/Energy\n\nWhat physically changes when a system \u201chas knowledge\u201d?\n\n1. **Feedback Control Loops**  \n   - If a system can measure the environment (i.e. \u201cgain information\u201d) and then condition its actions on that information, it can harness energy flows more effectively. For instance, a steam engine with a governor. More advanced examples are living organisms employing signals to gather and channel resources at large scale.  \n\n2. **Conditional Probability and Directed Action**  \n   - Imagine an enormous branching of possible actions. Knowledge helps select a branch that yields dramatic macro changes. E.g., constructing a rocket (versus flailing randomly). So knowledge acts like a selection function in state space, guiding the system along improbable pathways by leveraging correlations.  \n\n3. **Replication of Knowledge**  \n   - Knowledge can replicate itself (culture, language, DNA replication). This leads to exponential scaling in its ability to reorganize matter. That is, once you discover the blueprint for a warp drive, you can copy it to many minds or computers, each of which can act.  \n\n4. **Reduced Entropy Production Locally / Increased Globally**  \n   - By harnessing knowledge, systems can do \u201cuseful work\u201d in a local sense (build cities, spaceships). Thermodynamics ensures total entropy of universe still increases, but you get pockets of decreased entropy or highly ordered structure, compensated by bigger increases in the surroundings. Knowledge accelerates these processes because it guides energy usage toward highly specific states.\n\nIn standard physics, none of these points are contradictory. They just require that the system *possesses an internal model that is correlated with external states*. That correlation, once embedded, changes the system\u2019s conditional dynamics. So from the micro-level perspective, it\u2019s all just interactions, but at a macro level we see powerful \u201cknowledge-driven\u201d processes.\n\n---\n\n## 5. Why a \u201cPhysics of Information\u201d Is Hard (But Ongoing)\n\n**Challenges:**  \n- We typically treat \u201cknowledge\u201d as a high-level, semantic concept. The semantic and pragmatic layers of information are more than just Shannon bits. Yet standard physics is about microstates and wavefunctions, not about meaning or reference.  \n- We want predictions about the *emergence* of new knowledge structures. This is akin to the \u201chard problem\u201d of open-ended evolution or how minds discover new ideas. Standard physics can describe how electrons move in brains, but not easily \u201cwhich idea\u201d they represent.  \n- There\u2019s no universal consensus on how to define \u201cknowledge\u201d physically. Some define it as a stable correlation that can be *used* by a system to maintain or grow itself. Others define it in terms of *counterfactuals* (constructor theory), or *mutual information* (cybernetics).\n\n**Ongoing**: The researchers mentioned are trying to formalize these. For instance, in constructor theory, knowledge is \u201cinformation that can act as a constructor for transformations.\u201d In the free-energy principle, knowledge is \u201can internal generative model that reduces surprise.\u201d These are stabs at the same puzzle.\n\n---\n\n## 6. Summary and Outlook\n\n1. **The puzzle**: Certain informational patterns, or \u201cknowledge,\u201d in a physical system cause disproportionate changes in the environment (warp drives, nuclear bombs, advanced biotech). This is because those patterns *encode instructions or decisions* that can systematically harness or direct matter/energy flows.  \n2. **Mechanism**: Once a system has the relevant correlations (knows the instructions), it can *select* among many possible actions or transformations, steering the system along a trajectory that wouldn\u2019t occur otherwise without that knowledge.  \n3. **Active research**: Deutsch & Marletto\u2019s constructor theory, Walker & Cronin\u2019s assembly theory, Friston\u2019s free energy principle, and other frameworks explicitly incorporate knowledge or model-building. They go beyond typical physics by focusing on *which transformations are possible given that an agent can store and replicate information*.  \n4. **Why not standard physics?**: Standard physics is fully compatible but doesn\u2019t by itself provide an *organizational vantage* for emergent \u201cknowledge.\u201d The phenomena require higher-level structures (control, feedback, replication, code) that only become evident at macroscopic, coarse-grained scales.  \n5. **Future**: A truly comprehensive \u201cphysics of knowledge\u201d would unify quantum/classical laws with these emergent phenomena of agency and self-reference. It remains an open frontier, bridging physics, complexity science, AI, and philosophy of mind. There is no final solution, but the field is vibrant.\n\n**Hence**: if you want a single banner under which people ask precisely *\u201cwhy knowledge changes the world physically?\u201d*, you\u2019ll find it in **Constructor Theory**, **Free Energy Principle**-based theories of life and mind, **information-driven theories of agency**, or **open-ended evolution** research in Artificial Life. They are all trying to embed the idea that certain patterns of information (knowledge) have causal power that reorients the flows of matter and energy in ways standard physics does not trivially highlight, even though it does not contradict it."
            ]
          },
          "status": "finished_successfully",
          "end_turn": true,
          "weight": 1.0,
          "metadata": {
            "finish_details": {
              "type": "stop",
              "stop_tokens": [
                200002
              ]
            },
            "is_complete": true,
            "citations": [],
            "content_references": [],
            "message_type": null,
            "model_slug": "o1",
            "default_model_slug": "o1",
            "parent_id": "fa003f87-53cc-48c5-8bc8-fc714866a871",
            "request_id": "90cfca23798baab9-IAD",
            "timestamp_": "absolute"
          },
          "recipient": "all",
          "channel": null
        },
        "parent": "815c3cb0-ba36-4eb4-bd35-3fef187f1556",
        "children": []
      }
    },
    "moderation_results": [],
    "current_node": "c9bcbfce-e659-4008-accd-120577bec542",
    "plugin_ids": null,
    "conversation_id": "67a2b822-ac9c-8005-bff7-d4c0f8263805",
    "conversation_template_id": null,
    "gizmo_id": null,
    "gizmo_type": null,
    "is_archived": false,
    "is_starred": null,
    "safe_urls": [],
    "blocked_urls": [],
    "default_model_slug": "o1",
    "conversation_origin": null,
    "voice": null,
    "async_status": null,
    "disabled_tool_ids": [],
    "is_do_not_remember": null,
    "memory_scope": "global_enabled",
    "sugar_item_id": null,
    "id": "67a2b822-ac9c-8005-bff7-d4c0f8263805"
  }
]