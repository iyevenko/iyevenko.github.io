[
  {
    "title": "Copying in Information Theory",
    "create_time": 1729205906.705413,
    "update_time": 1729206048.423996,
    "mapping": {
      "de8d8eee-b9a3-430d-9c50-200ed2f6ded5": {
        "id": "de8d8eee-b9a3-430d-9c50-200ed2f6ded5",
        "message": {
          "id": "de8d8eee-b9a3-430d-9c50-200ed2f6ded5",
          "author": {
            "role": "system",
            "name": null,
            "metadata": {}
          },
          "create_time": null,
          "update_time": null,
          "content": {
            "content_type": "text",
            "parts": [
              ""
            ]
          },
          "status": "finished_successfully",
          "end_turn": true,
          "weight": 0.0,
          "metadata": {
            "is_visually_hidden_from_conversation": true
          },
          "recipient": "all",
          "channel": null
        },
        "parent": "aaa1a207-649a-42e9-ba3a-f1fb10257cb0",
        "children": [
          "aaa25813-9e92-4fe7-84c3-7d2c36bcb59c",
          "aaa2f3c2-e547-4767-baa4-60aa4ccd1884"
        ]
      },
      "aaa1a207-649a-42e9-ba3a-f1fb10257cb0": {
        "id": "aaa1a207-649a-42e9-ba3a-f1fb10257cb0",
        "message": null,
        "parent": null,
        "children": [
          "de8d8eee-b9a3-430d-9c50-200ed2f6ded5"
        ]
      },
      "aaa25813-9e92-4fe7-84c3-7d2c36bcb59c": {
        "id": "aaa25813-9e92-4fe7-84c3-7d2c36bcb59c",
        "message": {
          "id": "aaa25813-9e92-4fe7-84c3-7d2c36bcb59c",
          "author": {
            "role": "user",
            "name": null,
            "metadata": {}
          },
          "create_time": 1729205906.708029,
          "update_time": null,
          "content": {
            "content_type": "text",
            "parts": [
              "Abstract\nIn many real-world systems, information can be transmitted in two qualitatively different ways: by copying or by transformation. Copying occurs when messages are transmitted without modification, e.g. when an offspring receives an unaltered copy of a gene from its parent. Transformation occurs when messages are modified systematically during transmission, e.g. when mutational biases occur during genetic replication. Standard information-theoretic measures do not distinguish these two modes of information transfer, although they may reflect different mechanisms and have different functional consequences. Starting from a few simple axioms, we derive a decomposition of mutual information into the information transmitted by copying versus the information transmitted by transformation. We begin with a decomposition that applies when the source and destination of the channel have the same set of messages and a notion of message identity exists. We then generalize our decomposition to other kinds of channels, which can involve different source and destination sets and broader notions of similarity. In addition, we show that copy information can be interpreted as the minimal work needed by a physical copying process, which is relevant for understanding the physics of replication. We use the proposed decomposition to explore a model of amino acid substitution rates. Our results apply to any system in which the fidelity of copying, rather than simple predictability, is of critical relevance.\n\nSignificance statement\nAnalysing and understanding the flow of information is crucial in many fields, from biology to physics to artificial intelligence. In many situations, it is crucial to disentangle how much information is transmitted by exact copying and how much information is transmitted by systematic transformations, a distinction that is not captured by standard information-theoretic measures. Here, we derive such a decomposition by starting from a few simple and intuitive assumptions. Our decomposition is easy to compute and has fundamental interpretations in terms of the thermodynamic costs of physical replication. Our measures apply to any information-processing scenario in which the faithfulness of the copy, not just the overall amount of mutual information, is of functional importance.\n\n1. Introduction\nShannon\u2019s information theory provides a powerful set of tools for quantifying and analysing information transmission. A particular measure of interest is mutual information (MI), which is the most common way of quantifying the amount of information transmitted from a source to a destination. MI has fundamental interpretations and operationalizations in a variety of domains, ranging from telecommunications [1,2] to gambling and investment [3\u20135], biological evolution [6], statistical physics [7,8] and many others. Nonetheless, it has long been observed [9,10] that MI does not distinguish between a situation in which the destination receives a copy of the source message versus one in which the destination receives some systematically transformed version of the source message (where \u2018systematic\u2019 refers to transformations that do not arise purely from noise).\n\nAs an example of where this distinction matters, consider the transmission of genetic information during biological reproduction. When this process is modelled as a communication channel from parent to offspring, the amount of transmitted genetic information is often quantified by MI [11\u201315]. During replication, however, genetic information is not only copied but can also undergo systematic transformations in the form of non-random mutational biases. For instance, in the DNA of most organisms, \ud835\ude70\u2194\ud835\ude76\n and \ud835\ude72\u2194\ud835\ude83\n mutations occur more frequently than \ud835\ude70\u2194\ud835\ude72\n, \ud835\ude70\u2194\ud835\ude83\n, \ud835\ude76\u2194\ud835\ude72\n and \ud835\ude76\u2194\ud835\ude83\n mutations [16\u201318]. That means that some information about parent nucleotides is preserved even when those nucleotides undergo mutations. MI does not distinguish which part of genetic information is transmitted by exact copying and which part is transmitted by mutational biases. However, these two modes of information transmission are driven by different mechanisms and have dramatically different evolutionary and functional implications, given that mutations are more likely to lead to deleterious consequences.\n\nThe goal of this paper is to find a general decomposition of the information transmitted by a channel into contributions from copying versus from transformation. In figure 1, we provide a schematic that illustrates the problem. Essentially, we seek a decomposition of transmitted information into copy and transformation that distinguishes the example provided in figure 1a, where the copy is perfect, from the one provided in figure 1b, where the message has been systematically scrambled, from the one provided in figure 1c, where the channel is completely noisy. Of course, we also want such a decomposition to apply in less extreme situations, where part of the information is copied and part is transformed.\n\nFigure 1. \nFigure 1. An illustration of the problem of copy and transformation. Consider three channels, each of which can transmit two messages, indicated by cat and snake (e.g. alarm calls in an animal communication system). In all panels, the rows indicate the message selected at the source, the columns indicate the message received at the destination, and the shade of the respective square indicates the conditional probability of the destination message given source message. For the channel in (a), all information is copied: the channel maps cat \u2192 cat and snake \u2192 snake with probability 1. For the channel in (b), all information is transformed: the channel maps cat \u2192 snake and snake \u2192 cat with probability 1. Note that, for any source distribution, the mutual information between source and destination is the same in (a) and (b). The channel in (c) is completely noisy: the probability of receiving a given message at the destination does not depend on the message selected at the source, and the mutual information between source and destination is 0. Observe that transformation is different from noise, in that it still involves the transmission of information.\n\nDownload figureOpen in new tabDownload PowerPoint\nThe distinction between copying and transformation is important in many other domains beyond the case of biological reproduction outlined above. For example, in many models of animal communication and language evolution, agents exchange signals across noisy channels and then use these signals to try to agree on common referents in the external world [10,19\u201325]. In such models, successful communication occurs when information is transmitted by copying; if signals are systematically transformed\u2014e.g. by scrambling\u2014the agents will not be mutually intelligible, even though MI between them may be high. As another example, the distinction between copying and transformation may be relevant in the study of information flow during biological development, where recent work has investigated the ability of regulatory networks to decode development signals, such as positional information, from gene expression patterns [26]. In this scenario, information is copied when developmental signals are decoded correctly, and transformed when they are systematically decoded in an incorrect manner. Yet other examples are provided by Markov chain models, which are commonly used to study computation and other dynamical processes in physics [27], biology [28] or sociology [29], among other fields. In fact, a Markov chain can be seen as a communication channel in which the system state transmits information from the past into the future. In this context, copying occurs when the system maintains its state constant over time (remains in fixed points) and transformation occurs when the state undergoes systematic changes (e.g. performs some kind of non-trivial computations).\n\nInterestingly, while the distinction between copy and transformation information seems natural, it has not been previously considered in the information-theoretic literature. This may be partly due to the different roles that information theory has historically played: on the one hand, a field of applied mathematics concerned with the engineering problem of optimizing information transmission (its original purpose); on the other, a set of quantitative tools for describing and analysing intrinsic properties of real-world systems. Because of its origins in engineering, much of information theory\u2014including Shannon\u2019s channel-coding theorem, which established MI as a fundamental measure of transmitted information [2,30,31]\u2014is formulated under the assumption of an external agent who can appropriately encode and decode information for transmission across a given communication channel, in this way accounting for any transformations performed by the channel. However, in many real-world systems, there is no additional external agent who codes for the channel [10,32], and one is interested in quantifying the ability of a channel to copy information without any additional encoding or decoding. This latter problem is the main subject of this paper.\n\nA final word is required to motivate our information-theoretic approach. It is standard to characterize the ability of a channel to copy messages via the \u2018probability of error\u2019 [2], which we indicate as \u03b5. In particular, \u03b5 is the probability that the destination receives a different message from the one that was sent by the source, while 1 \u2212 \u03b5 is the probability that the destination receives the same message as was sent by the source. However, for our purposes, this approach is insufficient. First of all, while 1 \u2212 \u03b5 quantifies the propensity of a channel to copy information, \u03b5 does not quantify the propensity to transmit information by transformation, since \u03b5 increases both in the presence of transformation and in the presence of noise (in other words, \u03b5 is high both in a channel like figure 1b and in a channel like figure 1c). Among other things, this means that 1 \u2212 \u03b5 and \u03b5 cannot be used to compute a channel\u2019s \u2018copying efficiency\u2019 (i.e. which portion of the total information transmitted across a channel is copied). Second, and more fundamentally, \u03b5 and 1 \u2212 \u03b5 are not information-theoretic quantities, in the sense that they do not measure an amount of information. For instance, 1 \u2212 \u03b5 is bounded between 0 and 1 for all channels, whether considering a simple binary channel or a high-speed fibre-optic line. In the language of physics, one might say that \u03b5 is an intensive property, rather than an extensive one that scales with the size of the channel. We instead seek measures which quantify the amount of copied and transformed information, and which can grow as the capacity of the channel under consideration increases.\n\nIn this paper, we present a decomposition of information that distinguishes copied from transformed information. We derive our decomposition by proposing four natural axioms that copy and transformation information should satisfy, and then identifying the unique measure that satisfies these axioms. Our resulting measure is easy to compute and can be used to decompose either the total MI flowing across a channel, or the specific MI corresponding to a given source message, or an even more general measure of acquired information called Bayesian surprise.\n\nThe paper is laid out as follows. We present our approach in the next section. In \u00a73, we show that, while our basic decomposition is defined for discrete-state channels where the source and destination share the same set of possible messages (so that the notion of \u2018exact copy\u2019 is simple to define), our measures can be generalized to channels with different source and destination messages, to continuous-valued channels, and to other definitions of copying. We also discuss how our approach relates to rate distortion in information theory [2]. In \u00a74, we show that our measure can be used to quantify the thermodynamic efficiency of physical copying processes, a central topic in biological physics. In \u00a75, we demonstrate our measures on a real-world dataset of amino acid substitution rates.\n\n2. Copy and transformation information\n2.1. Preliminaries\nWe briefly present some basic concepts from information theory that will be useful for our further developments.\n\nWe use the random variables X and Y to indicate the source and destination, respectively, of a communication channel (as defined in detail below). We assume that the source X and destination Y both take outcomes from the same countable set \ud835\udc9c\n. We use \u0394 to indicate the set of all probability distributions whose support is equal to or a subset of \ud835\udc9c\n. We use notation like pY, qY, \u2026 \u2208 \u0394 to indicate marginal distributions over Y, and pY|x, qY|x, \u2026 \u2208 \u0394 to indicate conditional distributions over Y, given the event X = x. Where clear from context, we will simply write p(y), q(y), \u2026 and p(y|x), q(y|x), \u2026 , and drop the subscripts.\n\nFor some distribution p over random variable X, we write the Shannon entropy as \ud835\udc3b\u2061(\ud835\udc5d\u2061(\ud835\udc4b)):=\u2212\u2211\ud835\udc65\ud835\udc5d\u2061(\ud835\udc65)\u2062log\u2061\ud835\udc5d\u2061(\ud835\udc65)\n, or simply H(X). For any two distributions s and q over the same set of outcomes, the Kullback\u2013Leibler (KL) divergence is defined as\n\n\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc60\u2225\ud835\udc5e):= \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062log\u2061\n\ud835\udc60\u2061(\ud835\udc65)\n\ud835\udc5e\u2061(\ud835\udc65)\n \n.\n2.1\nKL is non-negative and equal to 0 if and only if s(x) = q(x) for all x. It is infinite when the support of s is not a subset of the support of q. In this paper, we will also make use of the KL between Bernoulli distributions\u2014that is, distributions over two states of the type (a, 1 \u2212 a)\u2014which is sometimes called \u2018binary KL\u2019. We will use the notation \ud835\uddbd\u2061(\ud835\udc4e,\ud835\udc4f)\n to indicate the binary KL,\n\ud835\uddbd\u2061(\ud835\udc4e,\ud835\udc4f):=\ud835\udc4e\u2062log\u2061\n\ud835\udc4e\n\ud835\udc4f\n \n+(1\u2212\ud835\udc4e)\u2062log\u2061\n1\u2212\ud835\udc4e\n1\u2212\ud835\udc4f\n \n.\n2.2\nWe will in general assume that logs are in base 2 (so information is measured in bits), unless otherwise noted.\nIn information theory, a communication channel specifies the conditional probability distribution of receiving different messages at a destination given messages transmitted by a source. Let pY|X(y|x) indicate such a conditional probability distribution. The amount of intrinsic noise in the channel, given some probability distribution of source messages \ud835\udc60\ud835\udc4b\u2061(\ud835\udc65)\n, is the conditional Shannon entropy\n\ud835\udc3b\u2061(\ud835\udc4c\u2009|\u2009\ud835\udc4b):=\u2212 \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062 \n\u2211\n\ud835\udc66\n \ud835\udc5d\u2061(\ud835\udc66|\ud835\udc65)\u2062log\u2061\u2009\ud835\udc5d\u2061(\ud835\udc66|\ud835\udc65)\n. The amount of information transferred across a communication channel is quantified using the MI between the source and the destination [2],\n\n\ud835\udc3c\ud835\udc5d\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b):= \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062 \n\u2211\n\ud835\udc66\n \ud835\udc5d\u2061(\ud835\udc66|\ud835\udc65)\u2062log\u2061\n\u2009\ud835\udc5d\u2061(\ud835\udc66|\ud835\udc65)\n\ud835\udc5d\u2061(\ud835\udc66)\n \n,\n2.3\nwhere p(y) is the marginal probability of receiving message y at the destination, defined as\n\ud835\udc5d\u2061(\ud835\udc66):= \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062\ud835\udc5d\u2061(\ud835\udc66|\ud835\udc65).\n2.4\nWhen writing Ip (Y : X), we will omit the subscript p indicating the channel where it is clear from context. MI is a fundamental measure of information transmission and can be operationalized in numerous ways [2]. It is non-negative, and large when (on average) the uncertainty about the message at the destination decreases by a large amount, given the source message. MI can also be written as a weighted sum of so-called specific MI1 terms [33\u201335], one for each outcome of X,\n\ud835\udc3c\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b)= \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062\ud835\udc3c\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b=\ud835\udc65),\n2.5\nwhere the specific MI for outcome x is given by\n\ud835\udc3c\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b=\ud835\udc65):= \n\u2211\n\ud835\udc66\n \ud835\udc5d\u2061(\ud835\udc66|\ud835\udc65)\u2062log\u2061\n\u2009\ud835\udc5d\u2061(\ud835\udc66|\ud835\udc65)\n\ud835\udc5d\u2061(\ud835\udc66)\n \n=\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c).\n2.6\nEach I(Y : X = x) indicates the contribution to MI arising from the particular source message x. We will sometimes use the term total mutual information (total MI) to refer to equation (2.3), so as to distinguish it from specific MI.\nSpecific MI also has an important Bayesian interpretation. Consider an agent who begins with a set of prior beliefs about Y, as specified by the prior distribution pY(y). The agent then updates their beliefs conditioned on the event X = x, resulting in the posterior distribution pY|x. The KL divergence between the posterior and the prior, \ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n (equation (2.6)), is called Bayesian surprise [36], and quantifies the amount of information acquired by the agent. It reaches its minimum value of zero, indicating that no information is acquired, if and only if the prior and posterior distributions match exactly. Bayesian surprise plays a fundamental role in Bayesian theory, including in the design of optimal experiments [37\u201340] and the selection of \u2018non-informative priors\u2019 [41,42]. Specific MI is a special case of Bayesian surprise, when the prior pY is the marginal distribution at the destination, as determined by a choice of source distribution sX and channel pY|X according to equation (2.4). In general, however, Bayesian surprise may be defined for any desired prior pY and posterior distribution pY|x, without necessarily making reference to a source distribution sX and communication channel pY|X.\n\nBecause Bayesian surprise is a general measure that includes specific MI as a special case, we will formulate our analysis of copy and transformation information in terms of Bayesian surprise, \ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n. Note that, while the notation pY|x implies conditioning on the event X = x, formally pY|x can be any distribution whatsoever. Thus, we do not technically require that there exists some full joint or conditional probability distribution over X and Y. Throughout the paper, we will refer to the distributions pY|x and pY as the \u2018posterior\u2019 and \u2018prior\u2019.\n\nProofs and derivations are contained in the electronic supplementary material.\n\n2.2. Axioms for copy information\nWe propose that any measure of copy information should satisfy a set of four axioms. Our set-up is motivated in the following way. First, our decomposition should apply at the level of the individual source message, i.e. we wish to be able to decompose each specific MI term (or, more generally, Bayesian surprise) into a non-negative (specific) copy information term and a non-negative (specific) transformation information term. Second, we postulate that if there are two channels with the same marginal distribution at the destination, then the channel with the larger pY|X(x|x) (probability of destination getting message x when the source transmits message x) should have larger copy information for source message x (this is, so to speak, our \u2018central axiom\u2019). This postulate can also be interpreted in a Bayesian way. Imagine two Bayesian agents with the same prior distribution over beliefs, pY, who update their beliefs conditioned on the event X = x. We postulate that the agent with the larger posterior probability on Y = x should have greater copy information.\n\nFormally, we assume that each copy information term is a real-valued function of the posterior distribution, the prior distribution and the source message x, written generically as F(pY|x, pY, x). Given any measure of copy information F, the transformation information associated with message x is then the remainder of \ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n beyond F,\n\n\ud835\udc39\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65,\ud835\udc5d\ud835\udc4c,\ud835\udc65):=\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\u2212\ud835\udc39\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65,\ud835\udc5d\ud835\udc4c,\ud835\udc65).\n2.7\nWe now propose a set of axioms that any measure of copy information F should satisfy.\nFirst, we postulate that copy information should be bounded between 0 and the Bayesian surprise, \ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n. Given equation (2.7), this guarantees that both F and \ud835\udc39\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n are non-negative.\n\nAxiom 2.1.\n\ud835\udc39\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65,\ud835\udc5d\ud835\udc4c,\ud835\udc65)\u22650.\n\nAxiom 2.2.\n\ud835\udc39\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65,\ud835\udc5d\ud835\udc4c,\ud835\udc65)\u2264\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n.\n\nThen, we postulate that copy information for source message x should increase monotonically as the posterior probability of x increases, assuming the prior distribution is held fixed (this is the \u2018central axiom\u2019 mentioned above).\n\nAxiom 2.3.\nIf pY|x(x) \u2264 qY|x(x), then F(pY|x, pY, x) \u2264 F(qY|x, pY, x).\n\nIn electronic supplementary material, section B, we show that any measure of copy information that satisfies the above three axioms must obey F(pY|x, pY, x) = 0 whenever pY|x(x) \u2264 pY(x). We also show that one particular measure of copy information, which is called \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n and is discussed in the next section, is the largest measure that satisfies the above three axioms. However, the three axioms do not uniquely determine what happens when pY|x(x) > pY(x). This means that \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n is not unique, and, in fact, there are some trivial measures (such as F(pY|x, pY, x) = 0 for all pY|x, pY and x) that also satisfy the above axioms. Such trivial cases are excluded by our final axiom, which states that for all prior distributions and all posterior probabilities pY|x(x) > pY(x), there are posterior distributions that contain only copy information. As we will see below, \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n is the unique satisfying measure once this axiom is added.\n\nAxiom 2.4.\nFor any pY and c \u2208 [pY(x), 1], there exists a posterior distribution pY|x such that pY|x(x) = c and \ud835\udc39\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65,\ud835\udc5d\ud835\udc4c,\ud835\udc65)=\n \ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n.\n\n2.3. The measure \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\nWe now present \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n, the unique measure that satisfies the four copy information axioms proposed in the last section. Given a prior distribution pY, posterior distribution pY|x and source message x, this measure is defined as\n\n\ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)={ \n\ud835\uddbd\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2061(\ud835\udc65),\ud835\udc5d\ud835\udc4c\u2061(\ud835\udc65))\tif\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2061(\ud835\udc65)>\ud835\udc5d\ud835\udc4c\u2061(\ud835\udc65)\n0\totherwise,\n \u2009\n2.8\nwhere we have used the notation of equation (2.2). We now state the main result of our paper, which is as follows.\nTheorem 2.5.\n\ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n is the unique measure which satisfies axioms 2.1\u20132.4.\n\nIn electronic supplementary material, section A, we demonstrate that \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n satisfies all the axioms, and in electronic supplementary material, section B, we prove that it is the only measure that satisfies them. We further show that if one drops axiom 2.4, then \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n is the largest possible measure that can satisfy the remaining axioms.\n\nGiven the definition of \ud835\udc39\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n in equation (2.7), \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n also defines a non-negative measure of transformation information, which we call \ud835\udc37\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\n,\n\n\ud835\udc37\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)=\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\u2212\ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c).\n2.4. Decomposing mutual information\nWe now show that \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n and \ud835\udc37\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\n allow for a decomposition of MI into MI due to copying and MI due to transformation. Recall that MI can be written as an expectation over specific MI terms, as shown in equation (2.6). Each specific MI term can be seen as a Bayesian surprise, where the prior distribution is the marginal distribution at the destination (see equation (2.4)), and the posterior distribution is the conditional distribution of destination messages given a particular source message. Thus, our definitions of \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n and \ud835\udc37\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\n provide a non-negative decomposition of each specific MI term,\n\n\ud835\udc3c\ud835\udc5d\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b=\ud835\udc65)=\ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)+\ud835\udc37\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c).\n2.9\nIn consequence, they also provide a non-negative decomposition of the total MI into two non-negative terms: the total copy information and the total transformation information,\n\ud835\udc3c\ud835\udc5d\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b)=\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc5d\u2061(\ud835\udc4b\u2192\ud835\udc4c)+\ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc5d\u2061(\ud835\udc4b\u2192\ud835\udc4c),\nwhere \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc5d\u2061(\ud835\udc4b\u2192\ud835\udc4c)\n and \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc5d\u2061(\ud835\udc4b\u2192\ud835\udc4c)\n are given by\n\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc5d\u2061(\ud835\udc4b\u2192\ud835\udc4c):= \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062\ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n2.10\nand\u2009\u2009\ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc5d\u2061(\ud835\udc4b\u2192\ud835\udc4c):= \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062\ud835\udc37\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c).\n2.11\n(When writing \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n and \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n, we will often omit the subscript p when the channel is clear from the context.) By a simple manipulation, we can also decompose the marginal entropy of the destination H(Y) into three non-negative components,\n\ud835\udc3b\u2061(\ud835\udc4c)=\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\u2061(\ud835\udc4b\u2192\ud835\udc4c)+\ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\u2061(\ud835\udc4b\u2192\ud835\udc4c)+\ud835\udc3b\u2061(\ud835\udc4c|\ud835\udc4b).\n2.12\nThus, given a channel from X to Y, the uncertainty in Y can be written as the sum of the copy information from X, the transformed information from X and the intrinsic noise in that channel from X to Y.\nFor illustration purposes, we plot the behaviour of \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n and \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n in the classical binary symmetric channel (BSC) in figure 2 (see caption for details). More detailed analysis of copy and transformation information in the BSC is discussed in electronic supplementary material, section E.\n\nFigure 2. \nFigure 2. The binary symmetric channel (BSC) with a uniform source distribution. We plot values of the MI I(Y : X), copy information \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\u2061(\ud835\udc4b\u2192\ud835\udc4c)\n (equation (2.10)) and transformation information \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\u2061(\ud835\udc4b\u2192\ud835\udc4c)\n (equation (2.11)) for the BSC along the whole range of error probabilities \u03b5 \u2208 [0, 1]. When \u03b5 \u2264 1/2, all mutual information is \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n (blue shading); when \u03b5 \u2265 1/2, all mutual information is \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n (orange shading). (Online version in colour.)\n\nDownload figureOpen in new tabDownload PowerPoint\nIt is worthwhile pointing out several important differences between our proposed measures and MI.\n\nFirst, in the definitions of \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\u2061(\ud835\udc4b\u2192\ud835\udc4c)\n and \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\u2061(\ud835\udc4b\u2192\ud835\udc4c)\n, the notation X \u2192 Y indicates that X is the source and Y is the destination. This is necessary because, unlike MI, \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n and \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n are in general non-symmetric, so it is possible that \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\u2061(\ud835\udc4b\u2192\ud835\udc4c)\u2260\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\u2061(\ud835\udc4c\u2192\ud835\udc4b)\n, and similarly for \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n. We also note that the above forms of \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n and \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n, where they are written as sums over the individual source message, are sometimes referred to as the trace-like forms in the literature, and are commonly desired characteristics of information-theoretic functionals [43,44].\n\nSecond, \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n and \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n do not obey the data-processing inequality [2], and can either decrease or increase as the destination undergoes further operations. In this respect, they are different from MI (the sum of \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n and \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n). As an example, consider the case where channel pY|X first transforms source message X into an encrypted message Y, and then another channel pX\u2032|Y decrypts Y back into a copy of X (so X\u2032 = X). In this example, \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\u2061(\ud835\udc4b\u2192\ud835\udc4b\u2032)>\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\u2061(\ud835\udc4b\u2192\ud835\udc4c)\n even though the Markov condition X \u2212 Y \u2212 X\u2032 holds.\n\nFinally, unlike MI, \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n and \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n are generally non-additive when multiple independent channels are concatenated. As an example, imagine that the source messages are bit strings of length n, which are transmitted through a product of n independent channels, p(y|x) = \u220fi pi(yi|xi). If the source bits are independent, s(x) = \u220fi si(xi), it is straightforward to show that the MI between X and Y has the additive form \ud835\udc3c\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b)=\u2211\ud835\udc56\ud835\udc3c\u2061(\ud835\udc4c\ud835\udc56\u2009:\u2009\ud835\udc4b\ud835\udc56)\n. However, \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n will generally not have this additive form, because copy information is defined in terms of the probability of exactly copying the entire source message (e.g. the entire n-bit-long string). Imagine that, in the above example, one of the bit-wise channels carries out a bit flip, pi(xi|yi) = 1 \u2212 \u03b4(xi, yi). In that case, the probability of receiving an exact copy of the source message at the destination is zero, and therefore \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n is also zero regardless of the nature of the other bit-wise channels pj for j \u2260 i. If desired, it is possible to derive an additive version of \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n by generalizing our measure with an appropriate \u2018loss function\u2019, as discussed in more detail in \u00a73 and electronic supplementary material, section C.3.\n\n2.5. Copying efficiency\nOur approach provides a way to quantify which portion of the information transmitted across a channel is due to copying rather than transformation, which we refer to as \u2018copying efficiency\u2019. Copying efficiency is defined at the level of individual source messages as\n\n\ud835\udf02\ud835\udc5d\u2061(\ud835\udc65):=\n\ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n \n\u2208[0,1],\n2.13\nwhere the bounds come directly from axioms 2.1 and 2.2. It can also be defined at the level of a channel as a whole as\n\ud835\udf02\ud835\udc5d:=\n\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\u2061(\ud835\udc4b\u2192\ud835\udc4c)\n\ud835\udc3c\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b)\n \n\u2208[0,1].\n2.14\nThe bounds follow simply given the above results.\nFor equations (2.13) and (2.14) to be useful efficiency measures, there should exist channels which are either \u2018completely inefficient\u2019 (have efficiency 0) or \u2018maximally efficient\u2019 (achieve efficiency 1). For the case of equation (2.13), the bounds can be saturated because of axiom 2.4, which guarantees that, for any source message x, prior pY and desired posterior probability pY|x(x) \u2265 pY(x), there exists a posterior pY|x such that the Bayesian surprise \ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n is composed entirely of copy information (for example, see electronic supplementary material, Eq. (A.1)).\n\nOne can show that the bounds in equation (2.14) can also be saturated. First, it can be verified that completely inefficient channels exist, since any channel which has pY|x(x) \u2264 pY(x) for all \ud835\udc65\u2208\ud835\udc9c\n will have \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\u2061(\ud835\udc4b\u2192\ud835\udc4c)=0\n (note that such channels exist at all levels of MI). We also show that maximally efficient channels exist, using the following result, which is proved in electronic supplementary material, section D.\n\nProposition 2.6.\nFor any source distribution sX with H(X) < \u221e, there exist channels pY|x for all levels of mutual information Ip (Y : X) \u2208 [0, H(X)] such that \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc5d\u2061(\ud835\udc4b\u2192\ud835\udc4c)=\ud835\udc3c\ud835\udc5d\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b)\n.\n\nProposition 2.6 shows that it is possible to achieve all values of total copy information, which is defined at the level of a channel. Note that this proposition does not follow immediately from axiom 2.4, which is a statement about copy information at the level of a prior pY and posterior pY|x, where no particular relationship between pY and pY|x is assumed.\n\n3. Generalization and relation to rate distortion\nWe now show that \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n can be written as a particular element among a broad family of copy information measures, which generalize the formal definition of what is meant by \u2018copying\u2019.\n\nAs we showed above, \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n is the unique measure that satisfies the four axioms proposed in \u00a72.2. In particular, it satisfies axiom 2.3, which states that, given the same prior pY, copy information should be larger for qY|x than for pY|x whenever qY|x(x) \u2265 pY|x(x). It also satisfies axiom 2.4, which states that there exist posterior distributions that have only copy information for all possible pY|x(x) \u2208 [pY(x), 1].\n\nThese axioms are based on one particular definition of copying, which states that copying occurs when the source and destination messages match perfectly. In fact, this can be generalized to other definitions of copying and transformation by using a loss function \u2113(x, y), which quantifies the dissimilarity between source message x and destination message y. For a given loss function, \u2113(x, y) = 0 indicates that x and y should be considered a perfect copy of each other, while \u2113(x, y) > 0 indicates that x and y should be considered as somewhat different. Importantly, \u2113(x, y) can quantify similarity in a graded manner, so that \u2113(x, y\u2032) > \u2113(x, y) indicates that y is closer to being a copy of x than y\u2032 (even though neither y nor y\u2032 may be a perfect copy of x).\n\nGiven an externally specified loss function \u2113(x, y), one can define axioms 2.3 and 2.4 in a generalized manner. The generalized version of axiom 2.3 states that posterior distribution qY|x should have higher copy information than pY|x whenever its expected loss is lower:\n\nAxiom 3.1*.\nIf \ud835\udd3c\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2061[\u2113\u2061(\ud835\udc65,\u2009\ud835\udc4c)]\u2265\ud835\udd3c\ud835\udc5e\ud835\udc4c|\ud835\udc65\u2061[\u2113\u2061(\ud835\udc65,\u2009\ud835\udc4c)],\n then F(pY|x, pY, x) \u2264 F(qY|x, pY, x).\n\nThe generalized version of axiom 2.4 states that, at all values of the expected loss which are lower than the expected loss achieved by pY, there are channels which transmit information only by copying.\n\nAxiom 3.2*.\nFor any pY and \ud835\udc50\u2208[min\ud835\udc66\u2061\u2113\u2061(\ud835\udc65,\u2009\ud835\udc66),\u2009\ud835\udd3c\u2009\ud835\udc5d\ud835\udc4c\u2061[\u2113\u2061(\ud835\udc65,\u2009\ud835\udc4c)]]\n, there exists a posterior distribution pY|x such that \ud835\udd3c\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2061[\u2113\u2061(\ud835\udc65,\u2009\ud835\udc4c)]=\ud835\udc50\n and \ud835\udc39\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65,\ud835\udc5d\ud835\udc4c,\u2009\ud835\udc65)=\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n.\n\nNote that, in defining axiom 3.2*, we used that min y\u2113(x, y) is the lowest expected loss that can be achieved by any posterior distribution.\n\nEach particular loss function induces its own measure of copy information. In fact, as we show in electronic supplementary material, section C.1, there is a unique measure of copy information which satisfies axioms 2.1 and 2.2, as defined in \u00a72.2, plus the generalized axioms 3.1* and 3.2*, as defined here in terms of the loss function \u2113(x, y). This generalized measure of copy information has the following form:\n\n\ud835\udc3a\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c):= \nmin\n\ud835\udc5f\ud835\udc4c\n \u2061\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc5f\ud835\udc4c\u2225\ud835\udc5d\ud835\udc4c)\n3.1\ns.t.\ud835\udd3c\ud835\udc5f\ud835\udc4c\u2061[\u2113\u2061(\ud835\udc65,\u2009\ud835\udc4c)]\u2264\ud835\udd3c\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2061[\u2113\u2061(\ud835\udc65,\u2009\ud835\udc4c)].\n3.2\nRecall that the KL divergence \ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc5f\ud835\udc4c\u2225\ud835\udc5d\ud835\udc4c)\n reflects the amount of information acquired by an agent in going from prior distribution pY to posterior distribution rY. Thus, \ud835\udc3a\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n quantifies the minimum information that must be acquired by an agent in order to match the copying performance of the actual posterior pY|x, as measured by the expected loss.\nEquation (3.1) is an instance of a \u2018minimum cross-entropy\u2019 problem, which is closely related to the \u2018maximum entropy\u2019 principle [45\u201347]. The distribution that optimizes equation (3.1) can be written in a simple form [48, pp. 299\u2013300],\n\n\ud835\udc64\u2061(\ud835\udc66)=\n1\n\ud835\udc4d\u2061(\ud835\udf06)\n \n\u2062\ud835\udc5d\ud835\udc4c\u2061(\ud835\udc66)\u2009e\u2212\ud835\udf06\u2062\u2113\u2061(\ud835\udc65,\ud835\udc66),\nwhere \u03bb \u2265 0 is a Lagrange multiplier chosen so that the constraint in equation (3.1) is satisfied, and \ud835\udc4d\u2061(\ud835\udf06)=\u2211\ud835\udc66\ud835\udc5d\ud835\udc4c\u2061(\ud835\udc66)\u2062\ud835\udc52\u2212\ud835\udf06\u2062\u2113\u2061(\ud835\udc65,\ud835\udc66)\n is a normalization constant. Note that, whenever \ud835\udd3c\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2061[\u2113\u2061(\ud835\udc65,\ud835\udc4c)]\u2265\ud835\udd3c\u2009\ud835\udc5d\ud835\udc4c\u2061[\u2113\u2061(\ud835\udc65,\ud835\udc4c)]\n, \u03bb = 0 and wY = pY [48]. Otherwise, \u03bb > 0 and the constraint in equation (3.1) will be tight up to equality. In practice, equation (3.1) can be solved by sweeping across the one-dimensional space of possible \u03bb \u2265 0 values (it can also be solved by standard convex optimization techniques). Once \u03bb is determined, the value of copy information is given by\n\ud835\udc3a\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65=\u2212\ud835\udf06\u2062\ud835\udd3c\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2061[\u2113\u2061(\ud835\udc65,\ud835\udc4c)]\u2212log\u2061\ud835\udc4d\u2061(\ud835\udf06).\nIt can be verified that \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n, the measure derived above, corresponds to the special case \u2113(x, y) := 1 \u2212 \u03b4(x, y), which is called \u20180\u20131 loss\u2019 in statistics [49] and \u2018Hamming distortion\u2019 in information theory [2] (see electronic supplementary material, section C.2).\nThe generalized measure \ud835\udc3a\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n has many similarities to \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n. Like \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n, it naturally leads to a non-negative measure of generalized transformation information,\n\n\ud835\udc3a\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)=\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\u2212\ud835\udc3a\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c).\n3.3\n\ud835\udc3a\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n can also be used to decompose total MI into (generalized) total copy and transformation information, akin to equations (2.10) and (2.11). Finally, one can use \ud835\udc3a\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n to define a generalized measure of copying efficiency, following the approach described in \u00a72.5.\nWhile we believe \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n, as defined via the 0\u20131 loss function, is a simple and reasonable choice in a variety of applications; in some cases, it may also be useful to consider other loss functions. One important example is when the source and destination have different sets of outcomes. Recall that \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n assumes that the source and destination share the same set of possible outcomes, \ud835\udc9c\n. When this assumption does not hold, generalized measures of copy and transformation information can still be defined, as long as an appropriate loss function \u2113\u2009:\u2009\ud835\udcb3\u00d7\ud835\udcb4\u2192\u211d\n is provided (where \ud835\udcb3\n and \ud835\udcb4\n indicate the outcomes of the source and destination, respectively).\n\nAnother important use case occurs when the loss function specifies continuously varying degrees of functional similarity between source and destination messages. For example, imagine that pY|X is an image compression algorithm which maps raw images X to compressed outputs Y. Research in computer vision has developed sophisticated loss functions for image compression which correlate strongly with human perceptual judgements [50]. By defining copy information in terms of such a loss function, one could measure how much perceptual information is copied by a particular image compression algorithm.\n\nOur generalized approach can also be used to define copy and transformation information for random variables with continuous-valued outcomes. The 0\u20131 loss function, as used in \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n, is not very meaningful for continuous-valued outcomes, since it depends on a measure-0 property of pY|x. A more natural measure of copy information is produced by the squared-error loss function \u2113(x, y): = (x \u2212 y)2, giving\n\nmin\n\ud835\udc5f\ud835\udc4c\n \u2061\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc5f\ud835\udc4c\u2225\ud835\udc5d\ud835\udc4c)s.t.\u2009\ud835\udd3c\ud835\udc5f\ud835\udc4c\u2061[(\ud835\udc4c\u2212\ud835\udc65)2]\u2264\ud835\udd3c\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2061[(\ud835\udc4c\u2212\ud835\udc65)2].\nThis particular optimization problem has been investigated in the maximum entropy literature, and has been shown to be particularly tractable when pY belongs to an exponential family [51\u201353].\nFinally, it is also possible to generalize this approach to vector-valued loss functions \u2113\u2009:\u2009\ud835\udcb3\u00d7\ud835\udcb4\u2192\u211d\ud835\udc5b\n, which allow one to specify dissimilarity in a multi-dimensional way. We discuss the relevant axioms and resulting copy information measure for vector-valued loss functions in electronic supplementary material, section C.3. We also demonstrate that vector-valued loss functions can be used to define measures of copy and transformation information that are additive for independent channels, in the sense discussed in \u00a72.4.\n\nAfter what we have discussed so far, it is natural to briefly review the similarities between our generalized approach and rate-distortion theory [2]. In rate-distortion theory, one is given a distribution over source messages sX and a \u2018distortion function\u2019 \u2113(x, y) which specifies the loss incurred when source message x is encoded with destination message y. The problem is to find the channel rY|X which minimizes MI without exceeding some constraint on the expected distortion,\n\nmin\n\ud835\udc5f\ud835\udc4c|\ud835\udc4b\n \u2061\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc5f\ud835\udc4c|\ud835\udc4b\u2225\ud835\udc5f\ud835\udc4c)s.t.\ud835\udd3c\ud835\udc5f\u2061[\u2113\u2061(\ud835\udc4b,\u2009\ud835\udc4c)]\u2264\ud835\udefc,\n3.4\nwhere \u03b1 is an externally determined parameter. The prototypical application of rate distortion is compression, i.e. to find a compression channel rY|X that has both low MI and low expected distortion. As can be seen by comparing equations (3.1) and (3.4), the optimization problem considered in our definition of generalized copy information and the optimization found in rate distortion are quite similar: they both involve minimizing a KL divergence subject to an expected loss constraint. Nonetheless, there are some important differences. First and foremost, the goals of the two approaches are different. In our approach, the aim is to decompose the information transmitted by a fixed externally specified channel into copy and transformation. In rate distortion, there is no externally specified channel and the aim is instead to find an optimal channel de novo. Second, our approach is motivated by a set of axioms which postulate how a measure of copy information should behave, rather than from channel-coding considerations which are used to derive the optimization problem in rate distortion [2]. Lastly, copy information is defined in a point-wise manner for each source message x, rather than for an entire set of source messages at once, as is rate distortion.\nWe finish by noting that one can also define equation (3.1) in a channel-wise manner (by minimizing \ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc5f\ud835\udc4c|\ud835\udc4b\u2225\ud835\udc5f\ud835\udc4c)\n, as in equation (3.4)) rather than a pointwise manner (minimize \ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc5f\ud835\udc4c|\ud835\udc4b=\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n, as in equation (3.1)). Under that formulation, one could no longer decompose specific MI into non-negative copy and information terms, though total MI could still be decomposed in that way. Interestingly, this alternative formulation would become equivalent to the so-called minimum information principle, a previous proposal for quantifying how much information about source messages is carried by different properties of destination messages [54].\n\n4. Thermodynamic costs of copying\nGiven the close connection between information theory and statistical physics, many information-theoretic quantities can be interpreted in thermodynamic terms [8]. As we show here, this includes our proposed measure of copy information, \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n. Specifically, we will show that \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n reflects the minimal amount of thermodynamic work necessary to copy a physical entity such as a polymer molecule. This latter example emphasizes the difference between information transfer by copying versus by transformation in a fundamental, biologically inspired physical set-up.\n\nConsider a physical system coupled to a heat bath at temperature T, and which is initially in an equilibrium distribution \ud835\udf0b\u2061(\ud835\udc56)\u221d\ud835\udc52\u2212\ud835\udc38\u2061(\ud835\udc56)/(\ud835\udc58\u2062\ud835\udc47)\n with respect to some Hamiltonian E (k is Boltzmann\u2019s constant). Now imagine that the system is driven to some non-equilibrium distribution p by a physical process, and that by the end of the process the Hamiltonian is again equal to E. The minimal amount of work required by any such process is related to the KL divergence between p and \u03c0 [55],\n\n\ud835\udc4a\u2265\ud835\udc58\u2062\ud835\udc47\u2009\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc5d\u2225\ud835\udf0b).\n4.1\nThe limit is achieved by thermodynamically reversible processes. (In this subsection, in accordance with the convention in physics, we assume that all logarithms are in base e, so information is measured in nats.)\nRecent work has analysed the fundamental thermodynamic constraints on copying in a physical system, for example for an information-carrying polymer like DNA [56,57]. Here, we will generally follow the model described in [56], while using our notation and omitting some details that are irrelevant for our purposes (such as the microstate/macrostate distinction). In this model, the source X represents the state of the original system (e.g. the polymer to be copied), and the destination Y represents the state of the replicate (e.g. the polymer produced by the copying mechanism). We make several assumptions. First, the source X is not modified during the copying process. Second, X and Y have the same Hamiltonian before and after the copying process. Finally, we follow [56] in assuming that Y is a persistent copy of X, meaning that before and after the copying process Y is physically separated from X and there is no interaction energy between them. This does not preclude X and Y from coming into contact and interacting energetically during intermediate stages of the copying process (for instance by template binding). The assumption of persistent copying means that there are no unaccounted energetic costs involved in preparing the copying system and transporting the produced replicate (e.g. moving the replicate Y to a daughter cell).\n\nAssume that Y starts in the equilibrium distribution, indicated as \u03c0Y (note that, by our persistent copy assumption, the equilibrium distribution cannot depend on the state of X). Let pY|x(x) indicate the conditional distribution of replicates after the end of the copying process, where x is the state of the original system X. Following equation (4.1), the minimal work required to bring Y out of equilibrium and produce replicates according to pY|x(x) is given by\n\n\ud835\udc4a\u2061(\ud835\udc65)\u2265\ud835\udc58\u2062\ud835\udc47\u2009\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udf0b\ud835\udc4c).\n4.2\nNote that equation (4.2) specifies the minimal work required to create the overall distribution pY|x. However, in many real-world scenarios, probably including DNA copying, the primary goal is to create exact copies of the original state, not transformed versions of it (such as non-random mutations). That means that, for a given source state x, the quality of the replication process can be quantified by the probability of making an exact copy, pY|x(x). We can now ask: what is the minimal work required by a physical replication process whose probability of making exact copies is at least as large as pY|x(x)? To make the comparison fair, we require that the process begins and ends with the same equilibrium distribution, \u03c0Y. The answer is given by the minimum of the RHS of equation (4.2) under a constraint on the exact-copy yield, which is exactly proportional to \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n,\n\n\ud835\udc4aexact\nmin\u2061(\ud835\udc65)=\ud835\udc58\u2062\ud835\udc47\u2009[ \nmin\n\ud835\udc5f\ud835\udc4c:\ud835\udc5f\ud835\udc4c\u2061(\ud835\udc65)\u2265\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2061(\ud835\udc65)\n \u2061\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc5f\ud835\udc4c\u2225\ud835\udf0b\ud835\udc4c)]\n4.3\n=\ud835\udc58\u2062\ud835\udc47\u2009\ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udf0b\ud835\udc4c),\n4.4\nwhere equation (4.4) follows from electronic supplementary material, section C.2. The additional work that is expended by the replication process is then lower bounded by a quantity proportional to \ud835\udc37\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\n,\n\ud835\udc4a\u2061(\ud835\udc65)\u2212\ud835\udc4aexact\nmin\u2061(\ud835\udc65)\u2265\ud835\udc58\u2062\ud835\udc47\u2062\ud835\udc37\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udf0b\ud835\udc4c).\n4.5\nThis shows formally the intuitive idea that transformation information contributes to thermodynamic costs but not to the accuracy of correct copying.\nIn most cases, a replication system is designed for copying not just one source state x, but an entire ensemble of source states (for example, the DNA replication system can copy a huge ensemble of source DNA sequences, not just one). Assume that X is distributed according to some sX(x). Across this ensemble of source states, the minimal amount of expected thermodynamic work required to produce replicates according to conditional distribution pY|X is given by\n\n\u27e8\ud835\udc4a\u27e9\u2265\ud835\udc58\u2062\ud835\udc47\u2009 \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udf0b\ud835\udc4c)\n4.6\n=\ud835\udc58\u2062\ud835\udc47\u2062[\ud835\udc3c\ud835\udc5d\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b)+\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc5d\ud835\udc4c\u2225\ud835\udf0b\ud835\udc4c)].\n4.7\nSince KL is non-negative, the minimum expected work is lowest when the equilibrium distribution \u03c0Y matches the marginal distribution of replicates, \ud835\udc5d\ud835\udc4c\u2061(\ud835\udc66)=\u2211\ud835\udc65\ud835\udc60\u2061(\ud835\udc65)\u2062\ud835\udc5d\u2061(\ud835\udc66|\ud835\udc65)\n. Using similar arguments as above, we can ask about the minimum expected work required to produce replicates, assuming each source state x achieves an exact-copy yield of at least pY|x(x). This turns out to be the expectation of equation (4.4),\n\u27e8\ud835\udc4aexact\nmin\u27e9=\ud835\udc58\u2062\ud835\udc47\u2009 \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062\ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2061(\ud835\udc65)\u2225\ud835\udf0b\ud835\udc4c)\n4.8\n=\ud835\udc58\u2062\ud835\udc47\u2062[\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc5d\u2061(\ud835\udc4b\u2192\ud835\udc4c)+\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc5d\ud835\udc4c\u2225\ud835\udf0b\ud835\udc4c)].\n4.9\nThe additional expected work that is needed by the replication process, above and beyond an optimal process that achieves the same exact-copy yield, is lower bounded by the transformation information,\n\u27e8\ud835\udc4a\u27e9\u2212\u27e8\ud835\udc4aexact\nmin\u27e9\u2265\ud835\udc58\u2062\ud835\udc47\u2062\ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc5d\u2061(\ud835\udc4b\u2192\ud835\udc4c).\n4.10\nWhen the equilibrium distribution \u03c0Y matches the marginal distribution pY, \u27e8\ud835\udc4aexact\nmin\u27e9\n is exactly equal to \ud835\udc58\u2062\ud835\udc47\u2062\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n. Furthermore, in this special case, the thermodynamic efficiency of exact copying, defined as the ratio of minimal work to actual work, becomes equal to the information-theoretic copying efficiency of p, as defined in equation (2.14),\n\u27e8\ud835\udc4aexact\nmin\u27e9\n\u27e8\ud835\udc4a\u27e9\n \n=\n\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc5d\u2061(\ud835\udc4b\u2192\ud835\udc4c)\n\ud835\udc3c\ud835\udc5d\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b)\n \n=\ud835\udf02\ud835\udc5d.\n4.11\nAs can be seen, standard information-theoretic measures, such as equation (4.2), bound the minimal thermodynamic costs of transferring information from one physical system to another, whether that transfer happens by copying or by transformation. However, as we have argued above, the difference between copying and transformation is essential in many biological scenarios, as well as other domains. In such cases, \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n arises naturally as the minimal thermodynamic work required to replicate information by copying.\n\nConcerning the example of DNA copying that we discussed throughout this section, our results should be interpreted with some care. We have generally imagined that the source system represents the state of an entire polymer, e.g. the state of an entire DNA molecule, and that the probability of exact copying refers to the probability that the entire sequence is reproduced without any errors. Alternatively, one can use the same framework to consider the probability of copying a single monomer in a long polymer (assuming that the thermodynamics of polymerization can be disregarded), as might be represented for instance by a single-nucleotide DNA substitution matrix [17], as analysed in the last section. Generally speaking, \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n computed at the level of single monomers will be different from \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n computed at the level of entire polymers, since the probability of exact copying means different things in these two formulations.\n\n5. Copy and transformation in amino acid substitution matrices\nIn the previous section, we saw how \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n and \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n arise naturally when studying the fundamental limits on the thermodynamics of copying, which includes the special case of replicating information-bearing polymers. Here, we demonstrate how these measures can be used to characterize the information-transmission properties of a real-world biological replication system, as formalized by a communication channel pY|X from parent to offspring [17,58]. In this context, we show how \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n can be used to quantify precisely how much information is transmitted by copying, without mutations. At the same time, we will use \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n to quantify how much information is transmitted by transformation, that is, by systematic non-random mutations that carry information but do not preserve the identity of the original message [16\u201318]. We also quantify the effect of purely random mutations, which correspond to the conditional entropy of the channel, H(Y|X).\n\nWe demonstrate these measures on empirical data of point accepted mutations (PAMs) of amino acids. PAM data represent the rates of substitutions between different amino acids during the course of biological evolution, and have various applications, including evolutionary modelling, phylogenetic reconstructions and protein alignment [58]. We emphasize that amino acid PAM matrices do not reflect the direct physical transfer of information from protein to protein, but rather the effects of underlying processes of DNA-based replication and selection, followed by translation.\n\nFormally, an amino acid PAM matrix Q is a continuous-time rate matrix. Qyx represents the instantaneous rates of substitutions from amino acid x to amino acid y, where both x and y belong to \ud835\udc9c={1,\u2009\u2026,20}\n, representing the 20 standard amino acids. We performed our analysis on a particular PAM matrix Q which was published by Le & Gascuel [58] (this matrix was provided by the pyvolve Python package [59]). We calculated a discrete-time conditional probability distribution pY|X from this matrix by computing the matrix exponential pY|X = exp(\u03c4Q). Thus, p(y|x) represents the probability that amino acid x is replaced by amino acid y over time scale \u03c4. For simplicity, we used time scale \u03c4 = 1. We used the stationary distribution of Q as the source distribution sX, which correlates closely with empirically observed amino acid frequencies [58, fig. 1]. Using the decomposition presented in equation (2.11), we arrived at the following values for the communication channel described by the conditional probabilities pY|X:\n\n\ud835\udc3c\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b)=\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\u2061(\ud835\udc4b\u2192\ud835\udc4c)+\ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\u2061(\ud835\udc4b\u2192\ud835\udc4c)\u22481.2\u2009bits,\nwhere\n\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\u2061(\ud835\udc4b\u2192\ud835\udc4c)\t= \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062\ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\u22480.88\u2009bits\nand\ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\u2061(\ud835\udc4b\u2192\ud835\udc4c)\t= \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062\ud835\udc37\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\u22480.32\u2009bits.\n \u2009\nWe also computed the intrinsic noise for this channel (see equation (2.12)),\n\ud835\udc3b\u2061(\ud835\udc4c|\ud835\udc4b)= \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062\ud835\udc3b\u2061(\ud835\udc4c|\ud835\udc4b=\ud835\udc65)\u22482.97\u2009bits.\nFinally, we computed the specific copy and transformation information, \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n and \ud835\udc37\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\n, for different amino acids. The results are shown in figure 3. We remind the reader that the sum of \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n and \ud835\udc37\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n for each amino acid x\u2014that is, the total height of the stacked bar plots in the figure\u2014is equal to the specific MI I(Y : X = x) for that x, as explained in the decomposition of equation (2.9).\nFigure 3. \nFigure 3. Copy and transformation information for different amino acids, based on an empirical PAM matrix [58]. We show magnitude of \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n in blue; on top of this is the amount of transformation information in orange. The sum of both is the specific MI for each amino acid, according to the decomposition given in equation (2.9). (Online version in colour.)\n\nDownload figureOpen in new tabDownload PowerPoint\nWhile we do not dive deeply in the biological significance of these results, we highlight several interesting findings. First, for this PAM matrix and time scale (\u03c4 = 1), a considerable fraction of the information (\u22481/4) is transmitted not by copying but by non-random mutations. Generally, such non-random mutations represent underlying physical, genetic and biological constraints that allow some pairs of amino acids to substitute each other more readily than other pairs.\n\nSecond, we observe considerable variation in the amount of specific MI, copy information and transformation between different amino acids, as well as different ratios of copy information to transformation information. In general, amino acids with more copy information are conserved unchanged over evolutionary time scales. At the same time, it is known that conserved amino acids tend to be \u2018outliers\u2019 in terms of their physio-chemical properties (such as hydrophobicity, volume, polarity, etc.), since mutations to such outliers are likely to alter protein function in deleterious ways [60,61]. To analyse this quantitatively, we used Miyata\u2019s measure of distance between amino acids, which is based on differences in volume and polarity [62]. For each amino acid, we quantified its degree of \u2018outlierness\u2019 in terms of its mean Miyata distance to all 19 other amino acids. The Spearman rank correlation between this outlierness measure and copy information (as shown in figure 3) was 0.57 (p = 0.009). On the other hand, the rank correlation between outlierness and transformation information was 0.22 (p = 0.352). Similar results were observed for other chemically motivated measures of amino acid distance, such as Grantham\u2019s distance [63] and Sneath\u2019s index [64]. This demonstrates that amino acids with unique chemical characteristics tend to have more copy information, but not more transformation information.\n\n6. Discussion\nAlthough MI is a very common and successful measure of transmitted information, it is insensitive to the distinction between information that is transmitted by copying versus information that is transmitted by transformation. Nonetheless, as we have argued, this distinction is of fundamental importance in many real-world systems.\n\nIn this paper, we propose a rigorous and practical way to decompose specific MI, and more generally Bayesian surprise, into two non-negative terms corresponding to copy and transformation, \ud835\udc3c=\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2+\ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n. We derive our decomposition using an axiomatic framework: we propose a set of four axioms that any measure of copy information should obey, and then identify the unique measure that satisfies those axioms. At the same time, we show that our measure of copy information is one of a family of functionals, each of which corresponds to a different way of quantifying error in transmission. We also demonstrate that our measures have a natural interpretation in thermodynamic terms, which suggests novel approaches for understanding the thermodynamic efficiency of biological replication processes, in particular DNA and RNA duplication. Finally, we demonstrate our results on real-world biological data, exploring copy and transformation information of amino acid substitution rates. We find significant variation among the amount of information transmitted by copying versus transformation among different amino acids.\n\nSeveral directions for future work present themselves.\n\nFirst, there is a large range of practical and theoretical application of our measures, from analysis of biological and neural information transmission to the study of the thermodynamics of self-replication, a fundamental and challenging problem in biophysics [65].\n\nSecond, we suspect our measures of copy and transformation information have further connections to existing formal treatments in information theory, in particular, rate-distortion theory [2], whose connections we started to explore here. We also believe that our decomposition may be generalizable beyond Bayesian surprise and MI to include other information-theoretic measures, including conditional MI and multi-information. Decomposing conditional MI is of particular interest, since it will permit a decomposition of the commonly used transfer entropy [66] measure into copy and transformation components, thus separating two different modes of dynamical information flow between systems.\n\nFinally, we point out that our proposed decomposition has some high-level similarities to other recent proposals for information-theoretic decomposition, such as the \u2018partial information decomposition\u2019 of multivariate information into redundant and synergistic components [67], integrated information decompositions [68,69] and decompositions of MI into \u2018semantic\u2019 (valuable) and \u2018non-semantic\u2019 (non-valuable) information [70]. We also mention another recent proposal for an alternative information-theoretic notion of \u2018copying\u2019 [71], in which copying is said to occur in a multivariate system when information that is present in one variable spreads to other variables (regardless of any transformations that information may undergo). Further research should explore if and how the decomposition proposed in this paper relates to these other approaches."
            ]
          },
          "status": "finished_successfully",
          "end_turn": null,
          "weight": 1.0,
          "metadata": {
            "serialization_metadata": {
              "custom_symbol_offsets": []
            },
            "request_id": "8d43e4b12864abd6-YYZ",
            "message_source": null,
            "timestamp_": "absolute",
            "message_type": null
          },
          "recipient": "all",
          "channel": null
        },
        "parent": "de8d8eee-b9a3-430d-9c50-200ed2f6ded5",
        "children": [
          "59e65112-2059-4102-a77b-868cf43497b4"
        ]
      },
      "59e65112-2059-4102-a77b-868cf43497b4": {
        "id": "59e65112-2059-4102-a77b-868cf43497b4",
        "message": {
          "id": "59e65112-2059-4102-a77b-868cf43497b4",
          "author": {
            "role": "tool",
            "name": "a8km123",
            "metadata": {}
          },
          "create_time": 1729205917.383386,
          "update_time": null,
          "content": {
            "content_type": "text",
            "parts": [
              "**Understanding the context**\n\nThe assistant is piecing together a response by examining the user's lengthy article on \"Copy and transformation in information theory: an axiomatic approach\" as a starting point.\n\n**Understanding copying**\n\nI'm looking into an alternative notion of copying in a multivariate system, where information diffuses across variables without considering transformations.\n\n**Thinking through possibilities**\n\nI'm considering various ways to engage with the article, including summarizing, applying methods, exploring connections, or expanding on research suggestions. The goal is to determine the most effective response to the user's query.\n\n**Piecing together connections**\n\nThe assistant considers summarizing the article or linking it to existing decompositions like partial information and integrated information.\n\n**Mapping out connections**\n\nI\u2019m tasked with summarizing the proposed decomposition and exploring its links to partial information decomposition (PID), integrated information decompositions, and similar approaches. Further research is emphasized to refine these connections."
            ]
          },
          "status": "finished_successfully",
          "end_turn": null,
          "weight": 0.0,
          "metadata": {
            "finished_text": "Thought for 9 seconds",
            "initial_text": "Thinking",
            "parent_id": "aaa25813-9e92-4fe7-84c3-7d2c36bcb59c",
            "request_id": "8d43e4b12864abd6-YYZ",
            "timestamp_": "absolute",
            "message_type": null
          },
          "recipient": "all",
          "channel": null
        },
        "parent": "aaa25813-9e92-4fe7-84c3-7d2c36bcb59c",
        "children": [
          "5587f404-126e-4d38-b68e-b30601e563a4"
        ]
      },
      "5587f404-126e-4d38-b68e-b30601e563a4": {
        "id": "5587f404-126e-4d38-b68e-b30601e563a4",
        "message": {
          "id": "5587f404-126e-4d38-b68e-b30601e563a4",
          "author": {
            "role": "assistant",
            "name": null,
            "metadata": {}
          },
          "create_time": 1729205917.383503,
          "update_time": null,
          "content": {
            "content_type": "text",
            "parts": [
              ""
            ]
          },
          "status": "in_progress",
          "end_turn": null,
          "weight": 1.0,
          "metadata": {
            "citations": [],
            "content_references": [],
            "gizmo_id": null,
            "message_type": null,
            "model_slug": "o1-preview",
            "default_model_slug": "o1-preview",
            "parent_id": "59e65112-2059-4102-a77b-868cf43497b4",
            "finish_details": {
              "type": "interrupted"
            },
            "request_id": "8d43e4b12864abd6-YYZ",
            "timestamp_": "absolute"
          },
          "recipient": "all",
          "channel": null
        },
        "parent": "59e65112-2059-4102-a77b-868cf43497b4",
        "children": []
      },
      "aaa2f3c2-e547-4767-baa4-60aa4ccd1884": {
        "id": "aaa2f3c2-e547-4767-baa4-60aa4ccd1884",
        "message": {
          "id": "aaa2f3c2-e547-4767-baa4-60aa4ccd1884",
          "author": {
            "role": "user",
            "name": null,
            "metadata": {}
          },
          "create_time": 1729206000.518278,
          "update_time": null,
          "content": {
            "content_type": "text",
            "parts": [
              "Please read the following paper and explain how they introduce copying to information theory. Then, tell me everything you know about physical copying and how it has been formulated to relate to info theory.\n---\nAbstract\nIn many real-world systems, information can be transmitted in two qualitatively different ways: by copying or by transformation. Copying occurs when messages are transmitted without modification, e.g. when an offspring receives an unaltered copy of a gene from its parent. Transformation occurs when messages are modified systematically during transmission, e.g. when mutational biases occur during genetic replication. Standard information-theoretic measures do not distinguish these two modes of information transfer, although they may reflect different mechanisms and have different functional consequences. Starting from a few simple axioms, we derive a decomposition of mutual information into the information transmitted by copying versus the information transmitted by transformation. We begin with a decomposition that applies when the source and destination of the channel have the same set of messages and a notion of message identity exists. We then generalize our decomposition to other kinds of channels, which can involve different source and destination sets and broader notions of similarity. In addition, we show that copy information can be interpreted as the minimal work needed by a physical copying process, which is relevant for understanding the physics of replication. We use the proposed decomposition to explore a model of amino acid substitution rates. Our results apply to any system in which the fidelity of copying, rather than simple predictability, is of critical relevance.\n\nSignificance statement\nAnalysing and understanding the flow of information is crucial in many fields, from biology to physics to artificial intelligence. In many situations, it is crucial to disentangle how much information is transmitted by exact copying and how much information is transmitted by systematic transformations, a distinction that is not captured by standard information-theoretic measures. Here, we derive such a decomposition by starting from a few simple and intuitive assumptions. Our decomposition is easy to compute and has fundamental interpretations in terms of the thermodynamic costs of physical replication. Our measures apply to any information-processing scenario in which the faithfulness of the copy, not just the overall amount of mutual information, is of functional importance.\n\n1. Introduction\nShannon\u2019s information theory provides a powerful set of tools for quantifying and analysing information transmission. A particular measure of interest is mutual information (MI), which is the most common way of quantifying the amount of information transmitted from a source to a destination. MI has fundamental interpretations and operationalizations in a variety of domains, ranging from telecommunications [1,2] to gambling and investment [3\u20135], biological evolution [6], statistical physics [7,8] and many others. Nonetheless, it has long been observed [9,10] that MI does not distinguish between a situation in which the destination receives a copy of the source message versus one in which the destination receives some systematically transformed version of the source message (where \u2018systematic\u2019 refers to transformations that do not arise purely from noise).\n\nAs an example of where this distinction matters, consider the transmission of genetic information during biological reproduction. When this process is modelled as a communication channel from parent to offspring, the amount of transmitted genetic information is often quantified by MI [11\u201315]. During replication, however, genetic information is not only copied but can also undergo systematic transformations in the form of non-random mutational biases. For instance, in the DNA of most organisms, \ud835\ude70\u2194\ud835\ude76\n and \ud835\ude72\u2194\ud835\ude83\n mutations occur more frequently than \ud835\ude70\u2194\ud835\ude72\n, \ud835\ude70\u2194\ud835\ude83\n, \ud835\ude76\u2194\ud835\ude72\n and \ud835\ude76\u2194\ud835\ude83\n mutations [16\u201318]. That means that some information about parent nucleotides is preserved even when those nucleotides undergo mutations. MI does not distinguish which part of genetic information is transmitted by exact copying and which part is transmitted by mutational biases. However, these two modes of information transmission are driven by different mechanisms and have dramatically different evolutionary and functional implications, given that mutations are more likely to lead to deleterious consequences.\n\nThe goal of this paper is to find a general decomposition of the information transmitted by a channel into contributions from copying versus from transformation. In figure 1, we provide a schematic that illustrates the problem. Essentially, we seek a decomposition of transmitted information into copy and transformation that distinguishes the example provided in figure 1a, where the copy is perfect, from the one provided in figure 1b, where the message has been systematically scrambled, from the one provided in figure 1c, where the channel is completely noisy. Of course, we also want such a decomposition to apply in less extreme situations, where part of the information is copied and part is transformed.\n\nFigure 1. \nFigure 1. An illustration of the problem of copy and transformation. Consider three channels, each of which can transmit two messages, indicated by cat and snake (e.g. alarm calls in an animal communication system). In all panels, the rows indicate the message selected at the source, the columns indicate the message received at the destination, and the shade of the respective square indicates the conditional probability of the destination message given source message. For the channel in (a), all information is copied: the channel maps cat \u2192 cat and snake \u2192 snake with probability 1. For the channel in (b), all information is transformed: the channel maps cat \u2192 snake and snake \u2192 cat with probability 1. Note that, for any source distribution, the mutual information between source and destination is the same in (a) and (b). The channel in (c) is completely noisy: the probability of receiving a given message at the destination does not depend on the message selected at the source, and the mutual information between source and destination is 0. Observe that transformation is different from noise, in that it still involves the transmission of information.\n\nDownload figureOpen in new tabDownload PowerPoint\nThe distinction between copying and transformation is important in many other domains beyond the case of biological reproduction outlined above. For example, in many models of animal communication and language evolution, agents exchange signals across noisy channels and then use these signals to try to agree on common referents in the external world [10,19\u201325]. In such models, successful communication occurs when information is transmitted by copying; if signals are systematically transformed\u2014e.g. by scrambling\u2014the agents will not be mutually intelligible, even though MI between them may be high. As another example, the distinction between copying and transformation may be relevant in the study of information flow during biological development, where recent work has investigated the ability of regulatory networks to decode development signals, such as positional information, from gene expression patterns [26]. In this scenario, information is copied when developmental signals are decoded correctly, and transformed when they are systematically decoded in an incorrect manner. Yet other examples are provided by Markov chain models, which are commonly used to study computation and other dynamical processes in physics [27], biology [28] or sociology [29], among other fields. In fact, a Markov chain can be seen as a communication channel in which the system state transmits information from the past into the future. In this context, copying occurs when the system maintains its state constant over time (remains in fixed points) and transformation occurs when the state undergoes systematic changes (e.g. performs some kind of non-trivial computations).\n\nInterestingly, while the distinction between copy and transformation information seems natural, it has not been previously considered in the information-theoretic literature. This may be partly due to the different roles that information theory has historically played: on the one hand, a field of applied mathematics concerned with the engineering problem of optimizing information transmission (its original purpose); on the other, a set of quantitative tools for describing and analysing intrinsic properties of real-world systems. Because of its origins in engineering, much of information theory\u2014including Shannon\u2019s channel-coding theorem, which established MI as a fundamental measure of transmitted information [2,30,31]\u2014is formulated under the assumption of an external agent who can appropriately encode and decode information for transmission across a given communication channel, in this way accounting for any transformations performed by the channel. However, in many real-world systems, there is no additional external agent who codes for the channel [10,32], and one is interested in quantifying the ability of a channel to copy information without any additional encoding or decoding. This latter problem is the main subject of this paper.\n\nA final word is required to motivate our information-theoretic approach. It is standard to characterize the ability of a channel to copy messages via the \u2018probability of error\u2019 [2], which we indicate as \u03b5. In particular, \u03b5 is the probability that the destination receives a different message from the one that was sent by the source, while 1 \u2212 \u03b5 is the probability that the destination receives the same message as was sent by the source. However, for our purposes, this approach is insufficient. First of all, while 1 \u2212 \u03b5 quantifies the propensity of a channel to copy information, \u03b5 does not quantify the propensity to transmit information by transformation, since \u03b5 increases both in the presence of transformation and in the presence of noise (in other words, \u03b5 is high both in a channel like figure 1b and in a channel like figure 1c). Among other things, this means that 1 \u2212 \u03b5 and \u03b5 cannot be used to compute a channel\u2019s \u2018copying efficiency\u2019 (i.e. which portion of the total information transmitted across a channel is copied). Second, and more fundamentally, \u03b5 and 1 \u2212 \u03b5 are not information-theoretic quantities, in the sense that they do not measure an amount of information. For instance, 1 \u2212 \u03b5 is bounded between 0 and 1 for all channels, whether considering a simple binary channel or a high-speed fibre-optic line. In the language of physics, one might say that \u03b5 is an intensive property, rather than an extensive one that scales with the size of the channel. We instead seek measures which quantify the amount of copied and transformed information, and which can grow as the capacity of the channel under consideration increases.\n\nIn this paper, we present a decomposition of information that distinguishes copied from transformed information. We derive our decomposition by proposing four natural axioms that copy and transformation information should satisfy, and then identifying the unique measure that satisfies these axioms. Our resulting measure is easy to compute and can be used to decompose either the total MI flowing across a channel, or the specific MI corresponding to a given source message, or an even more general measure of acquired information called Bayesian surprise.\n\nThe paper is laid out as follows. We present our approach in the next section. In \u00a73, we show that, while our basic decomposition is defined for discrete-state channels where the source and destination share the same set of possible messages (so that the notion of \u2018exact copy\u2019 is simple to define), our measures can be generalized to channels with different source and destination messages, to continuous-valued channels, and to other definitions of copying. We also discuss how our approach relates to rate distortion in information theory [2]. In \u00a74, we show that our measure can be used to quantify the thermodynamic efficiency of physical copying processes, a central topic in biological physics. In \u00a75, we demonstrate our measures on a real-world dataset of amino acid substitution rates.\n\n2. Copy and transformation information\n2.1. Preliminaries\nWe briefly present some basic concepts from information theory that will be useful for our further developments.\n\nWe use the random variables X and Y to indicate the source and destination, respectively, of a communication channel (as defined in detail below). We assume that the source X and destination Y both take outcomes from the same countable set \ud835\udc9c\n. We use \u0394 to indicate the set of all probability distributions whose support is equal to or a subset of \ud835\udc9c\n. We use notation like pY, qY, \u2026 \u2208 \u0394 to indicate marginal distributions over Y, and pY|x, qY|x, \u2026 \u2208 \u0394 to indicate conditional distributions over Y, given the event X = x. Where clear from context, we will simply write p(y), q(y), \u2026 and p(y|x), q(y|x), \u2026 , and drop the subscripts.\n\nFor some distribution p over random variable X, we write the Shannon entropy as \ud835\udc3b\u2061(\ud835\udc5d\u2061(\ud835\udc4b)):=\u2212\u2211\ud835\udc65\ud835\udc5d\u2061(\ud835\udc65)\u2062log\u2061\ud835\udc5d\u2061(\ud835\udc65)\n, or simply H(X). For any two distributions s and q over the same set of outcomes, the Kullback\u2013Leibler (KL) divergence is defined as\n\n\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc60\u2225\ud835\udc5e):= \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062log\u2061\n\ud835\udc60\u2061(\ud835\udc65)\n\ud835\udc5e\u2061(\ud835\udc65)\n \n.\n2.1\nKL is non-negative and equal to 0 if and only if s(x) = q(x) for all x. It is infinite when the support of s is not a subset of the support of q. In this paper, we will also make use of the KL between Bernoulli distributions\u2014that is, distributions over two states of the type (a, 1 \u2212 a)\u2014which is sometimes called \u2018binary KL\u2019. We will use the notation \ud835\uddbd\u2061(\ud835\udc4e,\ud835\udc4f)\n to indicate the binary KL,\n\ud835\uddbd\u2061(\ud835\udc4e,\ud835\udc4f):=\ud835\udc4e\u2062log\u2061\n\ud835\udc4e\n\ud835\udc4f\n \n+(1\u2212\ud835\udc4e)\u2062log\u2061\n1\u2212\ud835\udc4e\n1\u2212\ud835\udc4f\n \n.\n2.2\nWe will in general assume that logs are in base 2 (so information is measured in bits), unless otherwise noted.\nIn information theory, a communication channel specifies the conditional probability distribution of receiving different messages at a destination given messages transmitted by a source. Let pY|X(y|x) indicate such a conditional probability distribution. The amount of intrinsic noise in the channel, given some probability distribution of source messages \ud835\udc60\ud835\udc4b\u2061(\ud835\udc65)\n, is the conditional Shannon entropy\n\ud835\udc3b\u2061(\ud835\udc4c\u2009|\u2009\ud835\udc4b):=\u2212 \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062 \n\u2211\n\ud835\udc66\n \ud835\udc5d\u2061(\ud835\udc66|\ud835\udc65)\u2062log\u2061\u2009\ud835\udc5d\u2061(\ud835\udc66|\ud835\udc65)\n. The amount of information transferred across a communication channel is quantified using the MI between the source and the destination [2],\n\n\ud835\udc3c\ud835\udc5d\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b):= \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062 \n\u2211\n\ud835\udc66\n \ud835\udc5d\u2061(\ud835\udc66|\ud835\udc65)\u2062log\u2061\n\u2009\ud835\udc5d\u2061(\ud835\udc66|\ud835\udc65)\n\ud835\udc5d\u2061(\ud835\udc66)\n \n,\n2.3\nwhere p(y) is the marginal probability of receiving message y at the destination, defined as\n\ud835\udc5d\u2061(\ud835\udc66):= \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062\ud835\udc5d\u2061(\ud835\udc66|\ud835\udc65).\n2.4\nWhen writing Ip (Y : X), we will omit the subscript p indicating the channel where it is clear from context. MI is a fundamental measure of information transmission and can be operationalized in numerous ways [2]. It is non-negative, and large when (on average) the uncertainty about the message at the destination decreases by a large amount, given the source message. MI can also be written as a weighted sum of so-called specific MI1 terms [33\u201335], one for each outcome of X,\n\ud835\udc3c\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b)= \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062\ud835\udc3c\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b=\ud835\udc65),\n2.5\nwhere the specific MI for outcome x is given by\n\ud835\udc3c\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b=\ud835\udc65):= \n\u2211\n\ud835\udc66\n \ud835\udc5d\u2061(\ud835\udc66|\ud835\udc65)\u2062log\u2061\n\u2009\ud835\udc5d\u2061(\ud835\udc66|\ud835\udc65)\n\ud835\udc5d\u2061(\ud835\udc66)\n \n=\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c).\n2.6\nEach I(Y : X = x) indicates the contribution to MI arising from the particular source message x. We will sometimes use the term total mutual information (total MI) to refer to equation (2.3), so as to distinguish it from specific MI.\nSpecific MI also has an important Bayesian interpretation. Consider an agent who begins with a set of prior beliefs about Y, as specified by the prior distribution pY(y). The agent then updates their beliefs conditioned on the event X = x, resulting in the posterior distribution pY|x. The KL divergence between the posterior and the prior, \ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n (equation (2.6)), is called Bayesian surprise [36], and quantifies the amount of information acquired by the agent. It reaches its minimum value of zero, indicating that no information is acquired, if and only if the prior and posterior distributions match exactly. Bayesian surprise plays a fundamental role in Bayesian theory, including in the design of optimal experiments [37\u201340] and the selection of \u2018non-informative priors\u2019 [41,42]. Specific MI is a special case of Bayesian surprise, when the prior pY is the marginal distribution at the destination, as determined by a choice of source distribution sX and channel pY|X according to equation (2.4). In general, however, Bayesian surprise may be defined for any desired prior pY and posterior distribution pY|x, without necessarily making reference to a source distribution sX and communication channel pY|X.\n\nBecause Bayesian surprise is a general measure that includes specific MI as a special case, we will formulate our analysis of copy and transformation information in terms of Bayesian surprise, \ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n. Note that, while the notation pY|x implies conditioning on the event X = x, formally pY|x can be any distribution whatsoever. Thus, we do not technically require that there exists some full joint or conditional probability distribution over X and Y. Throughout the paper, we will refer to the distributions pY|x and pY as the \u2018posterior\u2019 and \u2018prior\u2019.\n\nProofs and derivations are contained in the electronic supplementary material.\n\n2.2. Axioms for copy information\nWe propose that any measure of copy information should satisfy a set of four axioms. Our set-up is motivated in the following way. First, our decomposition should apply at the level of the individual source message, i.e. we wish to be able to decompose each specific MI term (or, more generally, Bayesian surprise) into a non-negative (specific) copy information term and a non-negative (specific) transformation information term. Second, we postulate that if there are two channels with the same marginal distribution at the destination, then the channel with the larger pY|X(x|x) (probability of destination getting message x when the source transmits message x) should have larger copy information for source message x (this is, so to speak, our \u2018central axiom\u2019). This postulate can also be interpreted in a Bayesian way. Imagine two Bayesian agents with the same prior distribution over beliefs, pY, who update their beliefs conditioned on the event X = x. We postulate that the agent with the larger posterior probability on Y = x should have greater copy information.\n\nFormally, we assume that each copy information term is a real-valued function of the posterior distribution, the prior distribution and the source message x, written generically as F(pY|x, pY, x). Given any measure of copy information F, the transformation information associated with message x is then the remainder of \ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n beyond F,\n\n\ud835\udc39\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65,\ud835\udc5d\ud835\udc4c,\ud835\udc65):=\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\u2212\ud835\udc39\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65,\ud835\udc5d\ud835\udc4c,\ud835\udc65).\n2.7\nWe now propose a set of axioms that any measure of copy information F should satisfy.\nFirst, we postulate that copy information should be bounded between 0 and the Bayesian surprise, \ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n. Given equation (2.7), this guarantees that both F and \ud835\udc39\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n are non-negative.\n\nAxiom 2.1.\n\ud835\udc39\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65,\ud835\udc5d\ud835\udc4c,\ud835\udc65)\u22650.\n\nAxiom 2.2.\n\ud835\udc39\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65,\ud835\udc5d\ud835\udc4c,\ud835\udc65)\u2264\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n.\n\nThen, we postulate that copy information for source message x should increase monotonically as the posterior probability of x increases, assuming the prior distribution is held fixed (this is the \u2018central axiom\u2019 mentioned above).\n\nAxiom 2.3.\nIf pY|x(x) \u2264 qY|x(x), then F(pY|x, pY, x) \u2264 F(qY|x, pY, x).\n\nIn electronic supplementary material, section B, we show that any measure of copy information that satisfies the above three axioms must obey F(pY|x, pY, x) = 0 whenever pY|x(x) \u2264 pY(x). We also show that one particular measure of copy information, which is called \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n and is discussed in the next section, is the largest measure that satisfies the above three axioms. However, the three axioms do not uniquely determine what happens when pY|x(x) > pY(x). This means that \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n is not unique, and, in fact, there are some trivial measures (such as F(pY|x, pY, x) = 0 for all pY|x, pY and x) that also satisfy the above axioms. Such trivial cases are excluded by our final axiom, which states that for all prior distributions and all posterior probabilities pY|x(x) > pY(x), there are posterior distributions that contain only copy information. As we will see below, \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n is the unique satisfying measure once this axiom is added.\n\nAxiom 2.4.\nFor any pY and c \u2208 [pY(x), 1], there exists a posterior distribution pY|x such that pY|x(x) = c and \ud835\udc39\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65,\ud835\udc5d\ud835\udc4c,\ud835\udc65)=\n \ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n.\n\n2.3. The measure \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\nWe now present \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n, the unique measure that satisfies the four copy information axioms proposed in the last section. Given a prior distribution pY, posterior distribution pY|x and source message x, this measure is defined as\n\n\ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)={ \n\ud835\uddbd\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2061(\ud835\udc65),\ud835\udc5d\ud835\udc4c\u2061(\ud835\udc65))\tif\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2061(\ud835\udc65)>\ud835\udc5d\ud835\udc4c\u2061(\ud835\udc65)\n0\totherwise,\n \u2009\n2.8\nwhere we have used the notation of equation (2.2). We now state the main result of our paper, which is as follows.\nTheorem 2.5.\n\ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n is the unique measure which satisfies axioms 2.1\u20132.4.\n\nIn electronic supplementary material, section A, we demonstrate that \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n satisfies all the axioms, and in electronic supplementary material, section B, we prove that it is the only measure that satisfies them. We further show that if one drops axiom 2.4, then \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n is the largest possible measure that can satisfy the remaining axioms.\n\nGiven the definition of \ud835\udc39\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n in equation (2.7), \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n also defines a non-negative measure of transformation information, which we call \ud835\udc37\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\n,\n\n\ud835\udc37\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)=\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\u2212\ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c).\n2.4. Decomposing mutual information\nWe now show that \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n and \ud835\udc37\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\n allow for a decomposition of MI into MI due to copying and MI due to transformation. Recall that MI can be written as an expectation over specific MI terms, as shown in equation (2.6). Each specific MI term can be seen as a Bayesian surprise, where the prior distribution is the marginal distribution at the destination (see equation (2.4)), and the posterior distribution is the conditional distribution of destination messages given a particular source message. Thus, our definitions of \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n and \ud835\udc37\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\n provide a non-negative decomposition of each specific MI term,\n\n\ud835\udc3c\ud835\udc5d\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b=\ud835\udc65)=\ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)+\ud835\udc37\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c).\n2.9\nIn consequence, they also provide a non-negative decomposition of the total MI into two non-negative terms: the total copy information and the total transformation information,\n\ud835\udc3c\ud835\udc5d\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b)=\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc5d\u2061(\ud835\udc4b\u2192\ud835\udc4c)+\ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc5d\u2061(\ud835\udc4b\u2192\ud835\udc4c),\nwhere \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc5d\u2061(\ud835\udc4b\u2192\ud835\udc4c)\n and \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc5d\u2061(\ud835\udc4b\u2192\ud835\udc4c)\n are given by\n\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc5d\u2061(\ud835\udc4b\u2192\ud835\udc4c):= \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062\ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n2.10\nand\u2009\u2009\ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc5d\u2061(\ud835\udc4b\u2192\ud835\udc4c):= \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062\ud835\udc37\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c).\n2.11\n(When writing \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n and \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n, we will often omit the subscript p when the channel is clear from the context.) By a simple manipulation, we can also decompose the marginal entropy of the destination H(Y) into three non-negative components,\n\ud835\udc3b\u2061(\ud835\udc4c)=\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\u2061(\ud835\udc4b\u2192\ud835\udc4c)+\ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\u2061(\ud835\udc4b\u2192\ud835\udc4c)+\ud835\udc3b\u2061(\ud835\udc4c|\ud835\udc4b).\n2.12\nThus, given a channel from X to Y, the uncertainty in Y can be written as the sum of the copy information from X, the transformed information from X and the intrinsic noise in that channel from X to Y.\nFor illustration purposes, we plot the behaviour of \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n and \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n in the classical binary symmetric channel (BSC) in figure 2 (see caption for details). More detailed analysis of copy and transformation information in the BSC is discussed in electronic supplementary material, section E.\n\nFigure 2. \nFigure 2. The binary symmetric channel (BSC) with a uniform source distribution. We plot values of the MI I(Y : X), copy information \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\u2061(\ud835\udc4b\u2192\ud835\udc4c)\n (equation (2.10)) and transformation information \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\u2061(\ud835\udc4b\u2192\ud835\udc4c)\n (equation (2.11)) for the BSC along the whole range of error probabilities \u03b5 \u2208 [0, 1]. When \u03b5 \u2264 1/2, all mutual information is \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n (blue shading); when \u03b5 \u2265 1/2, all mutual information is \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n (orange shading). (Online version in colour.)\n\nDownload figureOpen in new tabDownload PowerPoint\nIt is worthwhile pointing out several important differences between our proposed measures and MI.\n\nFirst, in the definitions of \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\u2061(\ud835\udc4b\u2192\ud835\udc4c)\n and \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\u2061(\ud835\udc4b\u2192\ud835\udc4c)\n, the notation X \u2192 Y indicates that X is the source and Y is the destination. This is necessary because, unlike MI, \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n and \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n are in general non-symmetric, so it is possible that \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\u2061(\ud835\udc4b\u2192\ud835\udc4c)\u2260\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\u2061(\ud835\udc4c\u2192\ud835\udc4b)\n, and similarly for \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n. We also note that the above forms of \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n and \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n, where they are written as sums over the individual source message, are sometimes referred to as the trace-like forms in the literature, and are commonly desired characteristics of information-theoretic functionals [43,44].\n\nSecond, \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n and \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n do not obey the data-processing inequality [2], and can either decrease or increase as the destination undergoes further operations. In this respect, they are different from MI (the sum of \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n and \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n). As an example, consider the case where channel pY|X first transforms source message X into an encrypted message Y, and then another channel pX\u2032|Y decrypts Y back into a copy of X (so X\u2032 = X). In this example, \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\u2061(\ud835\udc4b\u2192\ud835\udc4b\u2032)>\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\u2061(\ud835\udc4b\u2192\ud835\udc4c)\n even though the Markov condition X \u2212 Y \u2212 X\u2032 holds.\n\nFinally, unlike MI, \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n and \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n are generally non-additive when multiple independent channels are concatenated. As an example, imagine that the source messages are bit strings of length n, which are transmitted through a product of n independent channels, p(y|x) = \u220fi pi(yi|xi). If the source bits are independent, s(x) = \u220fi si(xi), it is straightforward to show that the MI between X and Y has the additive form \ud835\udc3c\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b)=\u2211\ud835\udc56\ud835\udc3c\u2061(\ud835\udc4c\ud835\udc56\u2009:\u2009\ud835\udc4b\ud835\udc56)\n. However, \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n will generally not have this additive form, because copy information is defined in terms of the probability of exactly copying the entire source message (e.g. the entire n-bit-long string). Imagine that, in the above example, one of the bit-wise channels carries out a bit flip, pi(xi|yi) = 1 \u2212 \u03b4(xi, yi). In that case, the probability of receiving an exact copy of the source message at the destination is zero, and therefore \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n is also zero regardless of the nature of the other bit-wise channels pj for j \u2260 i. If desired, it is possible to derive an additive version of \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n by generalizing our measure with an appropriate \u2018loss function\u2019, as discussed in more detail in \u00a73 and electronic supplementary material, section C.3.\n\n2.5. Copying efficiency\nOur approach provides a way to quantify which portion of the information transmitted across a channel is due to copying rather than transformation, which we refer to as \u2018copying efficiency\u2019. Copying efficiency is defined at the level of individual source messages as\n\n\ud835\udf02\ud835\udc5d\u2061(\ud835\udc65):=\n\ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n \n\u2208[0,1],\n2.13\nwhere the bounds come directly from axioms 2.1 and 2.2. It can also be defined at the level of a channel as a whole as\n\ud835\udf02\ud835\udc5d:=\n\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\u2061(\ud835\udc4b\u2192\ud835\udc4c)\n\ud835\udc3c\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b)\n \n\u2208[0,1].\n2.14\nThe bounds follow simply given the above results.\nFor equations (2.13) and (2.14) to be useful efficiency measures, there should exist channels which are either \u2018completely inefficient\u2019 (have efficiency 0) or \u2018maximally efficient\u2019 (achieve efficiency 1). For the case of equation (2.13), the bounds can be saturated because of axiom 2.4, which guarantees that, for any source message x, prior pY and desired posterior probability pY|x(x) \u2265 pY(x), there exists a posterior pY|x such that the Bayesian surprise \ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n is composed entirely of copy information (for example, see electronic supplementary material, Eq. (A.1)).\n\nOne can show that the bounds in equation (2.14) can also be saturated. First, it can be verified that completely inefficient channels exist, since any channel which has pY|x(x) \u2264 pY(x) for all \ud835\udc65\u2208\ud835\udc9c\n will have \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\u2061(\ud835\udc4b\u2192\ud835\udc4c)=0\n (note that such channels exist at all levels of MI). We also show that maximally efficient channels exist, using the following result, which is proved in electronic supplementary material, section D.\n\nProposition 2.6.\nFor any source distribution sX with H(X) < \u221e, there exist channels pY|x for all levels of mutual information Ip (Y : X) \u2208 [0, H(X)] such that \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc5d\u2061(\ud835\udc4b\u2192\ud835\udc4c)=\ud835\udc3c\ud835\udc5d\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b)\n.\n\nProposition 2.6 shows that it is possible to achieve all values of total copy information, which is defined at the level of a channel. Note that this proposition does not follow immediately from axiom 2.4, which is a statement about copy information at the level of a prior pY and posterior pY|x, where no particular relationship between pY and pY|x is assumed.\n\n3. Generalization and relation to rate distortion\nWe now show that \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n can be written as a particular element among a broad family of copy information measures, which generalize the formal definition of what is meant by \u2018copying\u2019.\n\nAs we showed above, \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n is the unique measure that satisfies the four axioms proposed in \u00a72.2. In particular, it satisfies axiom 2.3, which states that, given the same prior pY, copy information should be larger for qY|x than for pY|x whenever qY|x(x) \u2265 pY|x(x). It also satisfies axiom 2.4, which states that there exist posterior distributions that have only copy information for all possible pY|x(x) \u2208 [pY(x), 1].\n\nThese axioms are based on one particular definition of copying, which states that copying occurs when the source and destination messages match perfectly. In fact, this can be generalized to other definitions of copying and transformation by using a loss function \u2113(x, y), which quantifies the dissimilarity between source message x and destination message y. For a given loss function, \u2113(x, y) = 0 indicates that x and y should be considered a perfect copy of each other, while \u2113(x, y) > 0 indicates that x and y should be considered as somewhat different. Importantly, \u2113(x, y) can quantify similarity in a graded manner, so that \u2113(x, y\u2032) > \u2113(x, y) indicates that y is closer to being a copy of x than y\u2032 (even though neither y nor y\u2032 may be a perfect copy of x).\n\nGiven an externally specified loss function \u2113(x, y), one can define axioms 2.3 and 2.4 in a generalized manner. The generalized version of axiom 2.3 states that posterior distribution qY|x should have higher copy information than pY|x whenever its expected loss is lower:\n\nAxiom 3.1*.\nIf \ud835\udd3c\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2061[\u2113\u2061(\ud835\udc65,\u2009\ud835\udc4c)]\u2265\ud835\udd3c\ud835\udc5e\ud835\udc4c|\ud835\udc65\u2061[\u2113\u2061(\ud835\udc65,\u2009\ud835\udc4c)],\n then F(pY|x, pY, x) \u2264 F(qY|x, pY, x).\n\nThe generalized version of axiom 2.4 states that, at all values of the expected loss which are lower than the expected loss achieved by pY, there are channels which transmit information only by copying.\n\nAxiom 3.2*.\nFor any pY and \ud835\udc50\u2208[min\ud835\udc66\u2061\u2113\u2061(\ud835\udc65,\u2009\ud835\udc66),\u2009\ud835\udd3c\u2009\ud835\udc5d\ud835\udc4c\u2061[\u2113\u2061(\ud835\udc65,\u2009\ud835\udc4c)]]\n, there exists a posterior distribution pY|x such that \ud835\udd3c\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2061[\u2113\u2061(\ud835\udc65,\u2009\ud835\udc4c)]=\ud835\udc50\n and \ud835\udc39\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65,\ud835\udc5d\ud835\udc4c,\u2009\ud835\udc65)=\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n.\n\nNote that, in defining axiom 3.2*, we used that min y\u2113(x, y) is the lowest expected loss that can be achieved by any posterior distribution.\n\nEach particular loss function induces its own measure of copy information. In fact, as we show in electronic supplementary material, section C.1, there is a unique measure of copy information which satisfies axioms 2.1 and 2.2, as defined in \u00a72.2, plus the generalized axioms 3.1* and 3.2*, as defined here in terms of the loss function \u2113(x, y). This generalized measure of copy information has the following form:\n\n\ud835\udc3a\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c):= \nmin\n\ud835\udc5f\ud835\udc4c\n \u2061\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc5f\ud835\udc4c\u2225\ud835\udc5d\ud835\udc4c)\n3.1\ns.t.\ud835\udd3c\ud835\udc5f\ud835\udc4c\u2061[\u2113\u2061(\ud835\udc65,\u2009\ud835\udc4c)]\u2264\ud835\udd3c\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2061[\u2113\u2061(\ud835\udc65,\u2009\ud835\udc4c)].\n3.2\nRecall that the KL divergence \ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc5f\ud835\udc4c\u2225\ud835\udc5d\ud835\udc4c)\n reflects the amount of information acquired by an agent in going from prior distribution pY to posterior distribution rY. Thus, \ud835\udc3a\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n quantifies the minimum information that must be acquired by an agent in order to match the copying performance of the actual posterior pY|x, as measured by the expected loss.\nEquation (3.1) is an instance of a \u2018minimum cross-entropy\u2019 problem, which is closely related to the \u2018maximum entropy\u2019 principle [45\u201347]. The distribution that optimizes equation (3.1) can be written in a simple form [48, pp. 299\u2013300],\n\n\ud835\udc64\u2061(\ud835\udc66)=\n1\n\ud835\udc4d\u2061(\ud835\udf06)\n \n\u2062\ud835\udc5d\ud835\udc4c\u2061(\ud835\udc66)\u2009e\u2212\ud835\udf06\u2062\u2113\u2061(\ud835\udc65,\ud835\udc66),\nwhere \u03bb \u2265 0 is a Lagrange multiplier chosen so that the constraint in equation (3.1) is satisfied, and \ud835\udc4d\u2061(\ud835\udf06)=\u2211\ud835\udc66\ud835\udc5d\ud835\udc4c\u2061(\ud835\udc66)\u2062\ud835\udc52\u2212\ud835\udf06\u2062\u2113\u2061(\ud835\udc65,\ud835\udc66)\n is a normalization constant. Note that, whenever \ud835\udd3c\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2061[\u2113\u2061(\ud835\udc65,\ud835\udc4c)]\u2265\ud835\udd3c\u2009\ud835\udc5d\ud835\udc4c\u2061[\u2113\u2061(\ud835\udc65,\ud835\udc4c)]\n, \u03bb = 0 and wY = pY [48]. Otherwise, \u03bb > 0 and the constraint in equation (3.1) will be tight up to equality. In practice, equation (3.1) can be solved by sweeping across the one-dimensional space of possible \u03bb \u2265 0 values (it can also be solved by standard convex optimization techniques). Once \u03bb is determined, the value of copy information is given by\n\ud835\udc3a\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65=\u2212\ud835\udf06\u2062\ud835\udd3c\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2061[\u2113\u2061(\ud835\udc65,\ud835\udc4c)]\u2212log\u2061\ud835\udc4d\u2061(\ud835\udf06).\nIt can be verified that \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n, the measure derived above, corresponds to the special case \u2113(x, y) := 1 \u2212 \u03b4(x, y), which is called \u20180\u20131 loss\u2019 in statistics [49] and \u2018Hamming distortion\u2019 in information theory [2] (see electronic supplementary material, section C.2).\nThe generalized measure \ud835\udc3a\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n has many similarities to \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n. Like \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n, it naturally leads to a non-negative measure of generalized transformation information,\n\n\ud835\udc3a\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)=\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\u2212\ud835\udc3a\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c).\n3.3\n\ud835\udc3a\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n can also be used to decompose total MI into (generalized) total copy and transformation information, akin to equations (2.10) and (2.11). Finally, one can use \ud835\udc3a\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n to define a generalized measure of copying efficiency, following the approach described in \u00a72.5.\nWhile we believe \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n, as defined via the 0\u20131 loss function, is a simple and reasonable choice in a variety of applications; in some cases, it may also be useful to consider other loss functions. One important example is when the source and destination have different sets of outcomes. Recall that \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n assumes that the source and destination share the same set of possible outcomes, \ud835\udc9c\n. When this assumption does not hold, generalized measures of copy and transformation information can still be defined, as long as an appropriate loss function \u2113\u2009:\u2009\ud835\udcb3\u00d7\ud835\udcb4\u2192\u211d\n is provided (where \ud835\udcb3\n and \ud835\udcb4\n indicate the outcomes of the source and destination, respectively).\n\nAnother important use case occurs when the loss function specifies continuously varying degrees of functional similarity between source and destination messages. For example, imagine that pY|X is an image compression algorithm which maps raw images X to compressed outputs Y. Research in computer vision has developed sophisticated loss functions for image compression which correlate strongly with human perceptual judgements [50]. By defining copy information in terms of such a loss function, one could measure how much perceptual information is copied by a particular image compression algorithm.\n\nOur generalized approach can also be used to define copy and transformation information for random variables with continuous-valued outcomes. The 0\u20131 loss function, as used in \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n, is not very meaningful for continuous-valued outcomes, since it depends on a measure-0 property of pY|x. A more natural measure of copy information is produced by the squared-error loss function \u2113(x, y): = (x \u2212 y)2, giving\n\nmin\n\ud835\udc5f\ud835\udc4c\n \u2061\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc5f\ud835\udc4c\u2225\ud835\udc5d\ud835\udc4c)s.t.\u2009\ud835\udd3c\ud835\udc5f\ud835\udc4c\u2061[(\ud835\udc4c\u2212\ud835\udc65)2]\u2264\ud835\udd3c\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2061[(\ud835\udc4c\u2212\ud835\udc65)2].\nThis particular optimization problem has been investigated in the maximum entropy literature, and has been shown to be particularly tractable when pY belongs to an exponential family [51\u201353].\nFinally, it is also possible to generalize this approach to vector-valued loss functions \u2113\u2009:\u2009\ud835\udcb3\u00d7\ud835\udcb4\u2192\u211d\ud835\udc5b\n, which allow one to specify dissimilarity in a multi-dimensional way. We discuss the relevant axioms and resulting copy information measure for vector-valued loss functions in electronic supplementary material, section C.3. We also demonstrate that vector-valued loss functions can be used to define measures of copy and transformation information that are additive for independent channels, in the sense discussed in \u00a72.4.\n\nAfter what we have discussed so far, it is natural to briefly review the similarities between our generalized approach and rate-distortion theory [2]. In rate-distortion theory, one is given a distribution over source messages sX and a \u2018distortion function\u2019 \u2113(x, y) which specifies the loss incurred when source message x is encoded with destination message y. The problem is to find the channel rY|X which minimizes MI without exceeding some constraint on the expected distortion,\n\nmin\n\ud835\udc5f\ud835\udc4c|\ud835\udc4b\n \u2061\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc5f\ud835\udc4c|\ud835\udc4b\u2225\ud835\udc5f\ud835\udc4c)s.t.\ud835\udd3c\ud835\udc5f\u2061[\u2113\u2061(\ud835\udc4b,\u2009\ud835\udc4c)]\u2264\ud835\udefc,\n3.4\nwhere \u03b1 is an externally determined parameter. The prototypical application of rate distortion is compression, i.e. to find a compression channel rY|X that has both low MI and low expected distortion. As can be seen by comparing equations (3.1) and (3.4), the optimization problem considered in our definition of generalized copy information and the optimization found in rate distortion are quite similar: they both involve minimizing a KL divergence subject to an expected loss constraint. Nonetheless, there are some important differences. First and foremost, the goals of the two approaches are different. In our approach, the aim is to decompose the information transmitted by a fixed externally specified channel into copy and transformation. In rate distortion, there is no externally specified channel and the aim is instead to find an optimal channel de novo. Second, our approach is motivated by a set of axioms which postulate how a measure of copy information should behave, rather than from channel-coding considerations which are used to derive the optimization problem in rate distortion [2]. Lastly, copy information is defined in a point-wise manner for each source message x, rather than for an entire set of source messages at once, as is rate distortion.\nWe finish by noting that one can also define equation (3.1) in a channel-wise manner (by minimizing \ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc5f\ud835\udc4c|\ud835\udc4b\u2225\ud835\udc5f\ud835\udc4c)\n, as in equation (3.4)) rather than a pointwise manner (minimize \ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc5f\ud835\udc4c|\ud835\udc4b=\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n, as in equation (3.1)). Under that formulation, one could no longer decompose specific MI into non-negative copy and information terms, though total MI could still be decomposed in that way. Interestingly, this alternative formulation would become equivalent to the so-called minimum information principle, a previous proposal for quantifying how much information about source messages is carried by different properties of destination messages [54].\n\n4. Thermodynamic costs of copying\nGiven the close connection between information theory and statistical physics, many information-theoretic quantities can be interpreted in thermodynamic terms [8]. As we show here, this includes our proposed measure of copy information, \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n. Specifically, we will show that \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n reflects the minimal amount of thermodynamic work necessary to copy a physical entity such as a polymer molecule. This latter example emphasizes the difference between information transfer by copying versus by transformation in a fundamental, biologically inspired physical set-up.\n\nConsider a physical system coupled to a heat bath at temperature T, and which is initially in an equilibrium distribution \ud835\udf0b\u2061(\ud835\udc56)\u221d\ud835\udc52\u2212\ud835\udc38\u2061(\ud835\udc56)/(\ud835\udc58\u2062\ud835\udc47)\n with respect to some Hamiltonian E (k is Boltzmann\u2019s constant). Now imagine that the system is driven to some non-equilibrium distribution p by a physical process, and that by the end of the process the Hamiltonian is again equal to E. The minimal amount of work required by any such process is related to the KL divergence between p and \u03c0 [55],\n\n\ud835\udc4a\u2265\ud835\udc58\u2062\ud835\udc47\u2009\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc5d\u2225\ud835\udf0b).\n4.1\nThe limit is achieved by thermodynamically reversible processes. (In this subsection, in accordance with the convention in physics, we assume that all logarithms are in base e, so information is measured in nats.)\nRecent work has analysed the fundamental thermodynamic constraints on copying in a physical system, for example for an information-carrying polymer like DNA [56,57]. Here, we will generally follow the model described in [56], while using our notation and omitting some details that are irrelevant for our purposes (such as the microstate/macrostate distinction). In this model, the source X represents the state of the original system (e.g. the polymer to be copied), and the destination Y represents the state of the replicate (e.g. the polymer produced by the copying mechanism). We make several assumptions. First, the source X is not modified during the copying process. Second, X and Y have the same Hamiltonian before and after the copying process. Finally, we follow [56] in assuming that Y is a persistent copy of X, meaning that before and after the copying process Y is physically separated from X and there is no interaction energy between them. This does not preclude X and Y from coming into contact and interacting energetically during intermediate stages of the copying process (for instance by template binding). The assumption of persistent copying means that there are no unaccounted energetic costs involved in preparing the copying system and transporting the produced replicate (e.g. moving the replicate Y to a daughter cell).\n\nAssume that Y starts in the equilibrium distribution, indicated as \u03c0Y (note that, by our persistent copy assumption, the equilibrium distribution cannot depend on the state of X). Let pY|x(x) indicate the conditional distribution of replicates after the end of the copying process, where x is the state of the original system X. Following equation (4.1), the minimal work required to bring Y out of equilibrium and produce replicates according to pY|x(x) is given by\n\n\ud835\udc4a\u2061(\ud835\udc65)\u2265\ud835\udc58\u2062\ud835\udc47\u2009\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udf0b\ud835\udc4c).\n4.2\nNote that equation (4.2) specifies the minimal work required to create the overall distribution pY|x. However, in many real-world scenarios, probably including DNA copying, the primary goal is to create exact copies of the original state, not transformed versions of it (such as non-random mutations). That means that, for a given source state x, the quality of the replication process can be quantified by the probability of making an exact copy, pY|x(x). We can now ask: what is the minimal work required by a physical replication process whose probability of making exact copies is at least as large as pY|x(x)? To make the comparison fair, we require that the process begins and ends with the same equilibrium distribution, \u03c0Y. The answer is given by the minimum of the RHS of equation (4.2) under a constraint on the exact-copy yield, which is exactly proportional to \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n,\n\n\ud835\udc4aexact\nmin\u2061(\ud835\udc65)=\ud835\udc58\u2062\ud835\udc47\u2009[ \nmin\n\ud835\udc5f\ud835\udc4c:\ud835\udc5f\ud835\udc4c\u2061(\ud835\udc65)\u2265\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2061(\ud835\udc65)\n \u2061\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc5f\ud835\udc4c\u2225\ud835\udf0b\ud835\udc4c)]\n4.3\n=\ud835\udc58\u2062\ud835\udc47\u2009\ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udf0b\ud835\udc4c),\n4.4\nwhere equation (4.4) follows from electronic supplementary material, section C.2. The additional work that is expended by the replication process is then lower bounded by a quantity proportional to \ud835\udc37\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\n,\n\ud835\udc4a\u2061(\ud835\udc65)\u2212\ud835\udc4aexact\nmin\u2061(\ud835\udc65)\u2265\ud835\udc58\u2062\ud835\udc47\u2062\ud835\udc37\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udf0b\ud835\udc4c).\n4.5\nThis shows formally the intuitive idea that transformation information contributes to thermodynamic costs but not to the accuracy of correct copying.\nIn most cases, a replication system is designed for copying not just one source state x, but an entire ensemble of source states (for example, the DNA replication system can copy a huge ensemble of source DNA sequences, not just one). Assume that X is distributed according to some sX(x). Across this ensemble of source states, the minimal amount of expected thermodynamic work required to produce replicates according to conditional distribution pY|X is given by\n\n\u27e8\ud835\udc4a\u27e9\u2265\ud835\udc58\u2062\ud835\udc47\u2009 \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udf0b\ud835\udc4c)\n4.6\n=\ud835\udc58\u2062\ud835\udc47\u2062[\ud835\udc3c\ud835\udc5d\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b)+\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc5d\ud835\udc4c\u2225\ud835\udf0b\ud835\udc4c)].\n4.7\nSince KL is non-negative, the minimum expected work is lowest when the equilibrium distribution \u03c0Y matches the marginal distribution of replicates, \ud835\udc5d\ud835\udc4c\u2061(\ud835\udc66)=\u2211\ud835\udc65\ud835\udc60\u2061(\ud835\udc65)\u2062\ud835\udc5d\u2061(\ud835\udc66|\ud835\udc65)\n. Using similar arguments as above, we can ask about the minimum expected work required to produce replicates, assuming each source state x achieves an exact-copy yield of at least pY|x(x). This turns out to be the expectation of equation (4.4),\n\u27e8\ud835\udc4aexact\nmin\u27e9=\ud835\udc58\u2062\ud835\udc47\u2009 \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062\ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2061(\ud835\udc65)\u2225\ud835\udf0b\ud835\udc4c)\n4.8\n=\ud835\udc58\u2062\ud835\udc47\u2062[\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc5d\u2061(\ud835\udc4b\u2192\ud835\udc4c)+\ud835\udc37\ud835\uddaa\u2062\ud835\uddab\u2061(\ud835\udc5d\ud835\udc4c\u2225\ud835\udf0b\ud835\udc4c)].\n4.9\nThe additional expected work that is needed by the replication process, above and beyond an optimal process that achieves the same exact-copy yield, is lower bounded by the transformation information,\n\u27e8\ud835\udc4a\u27e9\u2212\u27e8\ud835\udc4aexact\nmin\u27e9\u2265\ud835\udc58\u2062\ud835\udc47\u2062\ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc5d\u2061(\ud835\udc4b\u2192\ud835\udc4c).\n4.10\nWhen the equilibrium distribution \u03c0Y matches the marginal distribution pY, \u27e8\ud835\udc4aexact\nmin\u27e9\n is exactly equal to \ud835\udc58\u2062\ud835\udc47\u2062\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n. Furthermore, in this special case, the thermodynamic efficiency of exact copying, defined as the ratio of minimal work to actual work, becomes equal to the information-theoretic copying efficiency of p, as defined in equation (2.14),\n\u27e8\ud835\udc4aexact\nmin\u27e9\n\u27e8\ud835\udc4a\u27e9\n \n=\n\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc5d\u2061(\ud835\udc4b\u2192\ud835\udc4c)\n\ud835\udc3c\ud835\udc5d\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b)\n \n=\ud835\udf02\ud835\udc5d.\n4.11\nAs can be seen, standard information-theoretic measures, such as equation (4.2), bound the minimal thermodynamic costs of transferring information from one physical system to another, whether that transfer happens by copying or by transformation. However, as we have argued above, the difference between copying and transformation is essential in many biological scenarios, as well as other domains. In such cases, \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n arises naturally as the minimal thermodynamic work required to replicate information by copying.\n\nConcerning the example of DNA copying that we discussed throughout this section, our results should be interpreted with some care. We have generally imagined that the source system represents the state of an entire polymer, e.g. the state of an entire DNA molecule, and that the probability of exact copying refers to the probability that the entire sequence is reproduced without any errors. Alternatively, one can use the same framework to consider the probability of copying a single monomer in a long polymer (assuming that the thermodynamics of polymerization can be disregarded), as might be represented for instance by a single-nucleotide DNA substitution matrix [17], as analysed in the last section. Generally speaking, \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n computed at the level of single monomers will be different from \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n computed at the level of entire polymers, since the probability of exact copying means different things in these two formulations.\n\n5. Copy and transformation in amino acid substitution matrices\nIn the previous section, we saw how \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n and \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n arise naturally when studying the fundamental limits on the thermodynamics of copying, which includes the special case of replicating information-bearing polymers. Here, we demonstrate how these measures can be used to characterize the information-transmission properties of a real-world biological replication system, as formalized by a communication channel pY|X from parent to offspring [17,58]. In this context, we show how \ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n can be used to quantify precisely how much information is transmitted by copying, without mutations. At the same time, we will use \ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n to quantify how much information is transmitted by transformation, that is, by systematic non-random mutations that carry information but do not preserve the identity of the original message [16\u201318]. We also quantify the effect of purely random mutations, which correspond to the conditional entropy of the channel, H(Y|X).\n\nWe demonstrate these measures on empirical data of point accepted mutations (PAMs) of amino acids. PAM data represent the rates of substitutions between different amino acids during the course of biological evolution, and have various applications, including evolutionary modelling, phylogenetic reconstructions and protein alignment [58]. We emphasize that amino acid PAM matrices do not reflect the direct physical transfer of information from protein to protein, but rather the effects of underlying processes of DNA-based replication and selection, followed by translation.\n\nFormally, an amino acid PAM matrix Q is a continuous-time rate matrix. Qyx represents the instantaneous rates of substitutions from amino acid x to amino acid y, where both x and y belong to \ud835\udc9c={1,\u2009\u2026,20}\n, representing the 20 standard amino acids. We performed our analysis on a particular PAM matrix Q which was published by Le & Gascuel [58] (this matrix was provided by the pyvolve Python package [59]). We calculated a discrete-time conditional probability distribution pY|X from this matrix by computing the matrix exponential pY|X = exp(\u03c4Q). Thus, p(y|x) represents the probability that amino acid x is replaced by amino acid y over time scale \u03c4. For simplicity, we used time scale \u03c4 = 1. We used the stationary distribution of Q as the source distribution sX, which correlates closely with empirically observed amino acid frequencies [58, fig. 1]. Using the decomposition presented in equation (2.11), we arrived at the following values for the communication channel described by the conditional probabilities pY|X:\n\n\ud835\udc3c\u2061(\ud835\udc4c\u2009:\u2009\ud835\udc4b)=\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\u2061(\ud835\udc4b\u2192\ud835\udc4c)+\ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\u2061(\ud835\udc4b\u2192\ud835\udc4c)\u22481.2\u2009bits,\nwhere\n\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\u2061(\ud835\udc4b\u2192\ud835\udc4c)\t= \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062\ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\u22480.88\u2009bits\nand\ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\u2061(\ud835\udc4b\u2192\ud835\udc4c)\t= \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062\ud835\udc37\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\u22480.32\u2009bits.\n \u2009\nWe also computed the intrinsic noise for this channel (see equation (2.12)),\n\ud835\udc3b\u2061(\ud835\udc4c|\ud835\udc4b)= \n\u2211\n\ud835\udc65\n \ud835\udc60\u2061(\ud835\udc65)\u2062\ud835\udc3b\u2061(\ud835\udc4c|\ud835\udc4b=\ud835\udc65)\u22482.97\u2009bits.\nFinally, we computed the specific copy and transformation information, \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\n and \ud835\udc37\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\n, for different amino acids. The results are shown in figure 3. We remind the reader that the sum of \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n and \ud835\udc37\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n for each amino acid x\u2014that is, the total height of the stacked bar plots in the figure\u2014is equal to the specific MI I(Y : X = x) for that x, as explained in the decomposition of equation (2.9).\nFigure 3. \nFigure 3. Copy and transformation information for different amino acids, based on an empirical PAM matrix [58]. We show magnitude of \ud835\udc37\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2\n\ud835\udc65\u2061(\u2009\ud835\udc5d\ud835\udc4c|\ud835\udc65\u2225\ud835\udc5d\ud835\udc4c)\n in blue; on top of this is the amount of transformation information in orange. The sum of both is the specific MI for each amino acid, according to the decomposition given in equation (2.9). (Online version in colour.)\n\nDownload figureOpen in new tabDownload PowerPoint\nWhile we do not dive deeply in the biological significance of these results, we highlight several interesting findings. First, for this PAM matrix and time scale (\u03c4 = 1), a considerable fraction of the information (\u22481/4) is transmitted not by copying but by non-random mutations. Generally, such non-random mutations represent underlying physical, genetic and biological constraints that allow some pairs of amino acids to substitute each other more readily than other pairs.\n\nSecond, we observe considerable variation in the amount of specific MI, copy information and transformation between different amino acids, as well as different ratios of copy information to transformation information. In general, amino acids with more copy information are conserved unchanged over evolutionary time scales. At the same time, it is known that conserved amino acids tend to be \u2018outliers\u2019 in terms of their physio-chemical properties (such as hydrophobicity, volume, polarity, etc.), since mutations to such outliers are likely to alter protein function in deleterious ways [60,61]. To analyse this quantitatively, we used Miyata\u2019s measure of distance between amino acids, which is based on differences in volume and polarity [62]. For each amino acid, we quantified its degree of \u2018outlierness\u2019 in terms of its mean Miyata distance to all 19 other amino acids. The Spearman rank correlation between this outlierness measure and copy information (as shown in figure 3) was 0.57 (p = 0.009). On the other hand, the rank correlation between outlierness and transformation information was 0.22 (p = 0.352). Similar results were observed for other chemically motivated measures of amino acid distance, such as Grantham\u2019s distance [63] and Sneath\u2019s index [64]. This demonstrates that amino acids with unique chemical characteristics tend to have more copy information, but not more transformation information.\n\n6. Discussion\nAlthough MI is a very common and successful measure of transmitted information, it is insensitive to the distinction between information that is transmitted by copying versus information that is transmitted by transformation. Nonetheless, as we have argued, this distinction is of fundamental importance in many real-world systems.\n\nIn this paper, we propose a rigorous and practical way to decompose specific MI, and more generally Bayesian surprise, into two non-negative terms corresponding to copy and transformation, \ud835\udc3c=\ud835\udc3c\ud835\uddbc\u2062\ud835\uddc8\u2062\ud835\uddc9\u2062\ud835\uddd2+\ud835\udc3c\ud835\uddcd\u2062\ud835\uddcb\u2062\ud835\uddba\u2062\ud835\uddc7\u2062\ud835\uddcc\n. We derive our decomposition using an axiomatic framework: we propose a set of four axioms that any measure of copy information should obey, and then identify the unique measure that satisfies those axioms. At the same time, we show that our measure of copy information is one of a family of functionals, each of which corresponds to a different way of quantifying error in transmission. We also demonstrate that our measures have a natural interpretation in thermodynamic terms, which suggests novel approaches for understanding the thermodynamic efficiency of biological replication processes, in particular DNA and RNA duplication. Finally, we demonstrate our results on real-world biological data, exploring copy and transformation information of amino acid substitution rates. We find significant variation among the amount of information transmitted by copying versus transformation among different amino acids.\n\nSeveral directions for future work present themselves.\n\nFirst, there is a large range of practical and theoretical application of our measures, from analysis of biological and neural information transmission to the study of the thermodynamics of self-replication, a fundamental and challenging problem in biophysics [65].\n\nSecond, we suspect our measures of copy and transformation information have further connections to existing formal treatments in information theory, in particular, rate-distortion theory [2], whose connections we started to explore here. We also believe that our decomposition may be generalizable beyond Bayesian surprise and MI to include other information-theoretic measures, including conditional MI and multi-information. Decomposing conditional MI is of particular interest, since it will permit a decomposition of the commonly used transfer entropy [66] measure into copy and transformation components, thus separating two different modes of dynamical information flow between systems.\n\nFinally, we point out that our proposed decomposition has some high-level similarities to other recent proposals for information-theoretic decomposition, such as the \u2018partial information decomposition\u2019 of multivariate information into redundant and synergistic components [67], integrated information decompositions [68,69] and decompositions of MI into \u2018semantic\u2019 (valuable) and \u2018non-semantic\u2019 (non-valuable) information [70]. We also mention another recent proposal for an alternative information-theoretic notion of \u2018copying\u2019 [71], in which copying is said to occur in a multivariate system when information that is present in one variable spreads to other variables (regardless of any transformations that information may undergo). Further research should explore if and how the decomposition proposed in this paper relates to these other approaches."
            ]
          },
          "status": "finished_successfully",
          "end_turn": null,
          "weight": 1.0,
          "metadata": {
            "request_id": "8d43e6fce93dab27-YYZ",
            "message_source": null,
            "timestamp_": "absolute",
            "message_type": null
          },
          "recipient": "all",
          "channel": null
        },
        "parent": "de8d8eee-b9a3-430d-9c50-200ed2f6ded5",
        "children": [
          "f3450118-7b1a-4650-8f9e-3764869b0c88"
        ]
      },
      "f3450118-7b1a-4650-8f9e-3764869b0c88": {
        "id": "f3450118-7b1a-4650-8f9e-3764869b0c88",
        "message": {
          "id": "f3450118-7b1a-4650-8f9e-3764869b0c88",
          "author": {
            "role": "tool",
            "name": "a8km123",
            "metadata": {}
          },
          "create_time": 1729206046.718543,
          "update_time": null,
          "content": {
            "content_type": "text",
            "parts": [
              "**Piecing together connections**\n\nI\u2019ll need to read the provided paper carefully to understand how copying is introduced in information theory and then delve into physical copying's relationship with information theory.\n\n**Mapping contributions**\n\nI\u2019m detailing the paper's key contributions, including the introduction of copying to information theory and how it relates to physical copying.\n\n**Understanding MI decomposition**\n\nI\u2019m exploring mutual information and its breakdown into copy and transformation information, acknowledging their distinct roles in data transmission. This involves recognizing data's inherent nature and its transmission methods.\n\n**Defining and aligning measures**\n\nFirst, they define copy and transformation information for specific messages. Then, they prove their measure uniquely adheres to four important axioms, linking it to thermodynamic costs and validating it with amino acid substitution rates.\n\n**Charting the itinerary**\n\nI\u2019m piecing together the assistant's approach to explaining the introduction of copying within information theory, covering definitions, axioms, and key findings. Then, I\u2019ll discuss physical copying, including thermodynamics and Szilard's engine, linking it to information theory."
            ]
          },
          "status": "finished_successfully",
          "end_turn": null,
          "weight": 0.0,
          "metadata": {
            "finished_text": "Thought for 14 seconds",
            "initial_text": "Thinking",
            "parent_id": "aaa2f3c2-e547-4767-baa4-60aa4ccd1884",
            "request_id": "8d43e6fce93dab27-YYZ",
            "timestamp_": "absolute",
            "message_type": null
          },
          "recipient": "all",
          "channel": null
        },
        "parent": "aaa2f3c2-e547-4767-baa4-60aa4ccd1884",
        "children": [
          "5cecce6f-2512-4fb7-bf9c-35ad45ccd76b"
        ]
      },
      "5cecce6f-2512-4fb7-bf9c-35ad45ccd76b": {
        "id": "5cecce6f-2512-4fb7-bf9c-35ad45ccd76b",
        "message": {
          "id": "5cecce6f-2512-4fb7-bf9c-35ad45ccd76b",
          "author": {
            "role": "assistant",
            "name": null,
            "metadata": {}
          },
          "create_time": 1729206046.718701,
          "update_time": null,
          "content": {
            "content_type": "text",
            "parts": [
              "**Introduction**\n\nIn information theory, mutual information (MI) is a fundamental measure that quantifies the amount of information transmitted from a source to a destination. However, MI does not differentiate between information transmitted by **copying** (exact replication of messages) and **transformation** (systematic modification of messages). This distinction is crucial in many real-world systems where the fidelity of copying has different implications compared to transformation. The paper introduces a formal decomposition of MI into copy and transformation components, providing a framework to quantify these two distinct modes of information transmission.\n\n**How the Paper Introduces Copying to Information Theory**\n\n**Motivation**\n\nThe authors highlight that in various systems\u2014such as genetic replication, animal communication, and physical processes\u2014information can be transmitted either by copying or transformation. For example, during DNA replication, genetic information is copied, but mutations (which can be biased) represent transformations. Standard MI measures do not distinguish between these modes, although they have different mechanisms and consequences.\n\n**Axiomatic Approach**\n\nTo address this, the paper proposes four axioms that any measure of copy information should satisfy:\n\n1. **Non-negativity**: Copy information should be greater than or equal to zero.\n2. **Boundedness**: Copy information should not exceed the Bayesian surprise (the KL divergence between the posterior and prior distributions).\n3. **Monotonicity**: Copy information should increase as the probability of exact copying increases.\n4. **Maximal Copy Information**: For any prior distribution and posterior probability where the destination matches the source with higher probability than the prior, there exists a posterior distribution where all the information is due to copying.\n\n**Definition of Copy and Transformation Information**\n\nBased on these axioms, the authors derive a unique measure of copy information for a specific source message \\( x \\):\n\n\\[\nD_{\\text{copy}_x}(p_{Y|x} \\parallel p_Y) =\n\\begin{cases}\nd(p_{Y|x}(x), p_Y(x)) & \\text{if } p_{Y|x}(x) > p_Y(x) \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n\nHere, \\( p_{Y|x} \\) is the posterior distribution (conditional probability of the destination given the source), \\( p_Y \\) is the prior distribution at the destination, and \\( d(a, b) \\) is the binary KL divergence:\n\n\\[\nd(a, b) = a \\log \\frac{a}{b} + (1 - a) \\log \\frac{1 - a}{1 - b}\n\\]\n\nThe transformation information \\( D_{\\text{trans}_x} \\) is then the remainder of the Bayesian surprise:\n\n\\[\nD_{\\text{trans}_x}(p_{Y|x} \\parallel p_Y) = D_{\\text{KL}}(p_{Y|x} \\parallel p_Y) - D_{\\text{copy}_x}(p_{Y|x} \\parallel p_Y)\n\\]\n\n**Decomposition of Mutual Information**\n\nThe total mutual information can be decomposed into copy and transformation components by averaging over all source messages:\n\n\\[\nI(Y : X) = I_{\\text{copy}}(X \\rightarrow Y) + I_{\\text{trans}}(X \\rightarrow Y)\n\\]\n\nWhere:\n\n\\[\nI_{\\text{copy}}(X \\rightarrow Y) = \\sum_x s(x) D_{\\text{copy}_x}(p_{Y|x} \\parallel p_Y)\n\\]\n\n\\[\nI_{\\text{trans}}(X \\rightarrow Y) = \\sum_x s(x) D_{\\text{trans}_x}(p_{Y|x} \\parallel p_Y)\n\\]\n\n\\( s(x) \\) is the source distribution.\n\n**Generalization Using Loss Functions**\n\nThe paper generalizes the concept of copying by introducing a loss function \\( \\ell(x, y) \\) that quantifies the dissimilarity between the source and destination messages. This allows for broader definitions of copying beyond exact matches. The generalized copy information \\( G_{\\text{copy}_x} \\) is defined through an optimization problem:\n\n\\[\nG_{\\text{copy}_x}(p_{Y|x} \\parallel p_Y) = \\min_{r_Y} D_{\\text{KL}}(r_Y \\parallel p_Y) \\quad \\text{subject to} \\quad \\mathbb{E}_{r_Y}[\\ell(x, Y)] \\leq \\mathbb{E}_{p_{Y|x}}[\\ell(x, Y)]\n\\]\n\nThis approach aligns with rate-distortion theory but focuses on decomposing the information in a given channel rather than optimizing the channel itself.\n\n**Relation to Rate-Distortion Theory**\n\nWhile both the proposed method and rate-distortion theory involve minimizing KL divergence under a distortion constraint, they serve different purposes. Rate-distortion theory seeks to find the minimal rate (information) required for a given level of distortion (loss), mainly for compression. In contrast, the paper's method decomposes existing mutual information into copy and transformation components based on the predefined loss function.\n\n**Thermodynamic Interpretation**\n\nThe authors connect their measures to thermodynamics by showing that the minimal work required to copy information physically is proportional to the copy information \\( D_{\\text{copy}_x} \\). This stems from the relationship between information and thermodynamic work, as described by the second law of thermodynamics and Landauer's principle.\n\nFor a physical copying process starting from equilibrium, the minimal work required is:\n\n\\[\nW_{\\text{min}}(x) = kT D_{\\text{copy}_x}(p_{Y|x} \\parallel \\pi_Y)\n\\]\n\nWhere \\( k \\) is Boltzmann's constant, \\( T \\) is temperature, and \\( \\pi_Y \\) is the equilibrium distribution.\n\n**Application to Amino Acid Substitution Rates**\n\nThe paper demonstrates the practical utility of their measures by analyzing amino acid substitution matrices (PAM matrices). These matrices represent the rates at which one amino acid substitutes for another over evolutionary time.\n\nKey findings include:\n\n- A significant portion of the mutual information is transmitted by copying rather than transformation.\n- Amino acids with unique chemical properties tend to have higher copy information because substitutions (transformations) are more likely to be deleterious.\n- The decomposition provides insights into evolutionary constraints and the preservation of functional properties in proteins.\n\n**Physical Copying and Its Relation to Information Theory**\n\n**Landauer's Principle**\n\nLandauer's principle states that erasing one bit of information in a computational device dissipates at least \\( kT \\ln 2 \\) amount of energy as heat, where \\( k \\) is Boltzmann's constant and \\( T \\) is the temperature of the heat bath. This principle links information processing to physical thermodynamics, emphasizing that information is physical.\n\n**Thermodynamics of Computation**\n\nThe thermodynamics of computation explores the physical limits of computation, including copying and erasing information. Reversible computing, where computation is performed without energy dissipation, is a key concept here. However, perfect copying in a thermodynamic sense is subject to fundamental limits due to entropy considerations.\n\n**Physical Models of Copying**\n\n- **Szilard's Engine**: A thought experiment that illustrates the relationship between information and entropy. It shows that acquiring information about a system (like the position of a particle) can be converted into work, but this process must consider the thermodynamic cost of measurement and erasure.\n- **Maxwell's Demon**: A hypothetical creature that seemingly violates the second law of thermodynamics by decreasing entropy without expending work. Information theory resolves this paradox by accounting for the information processing activities of the demon.\n- **DNA Replication**: DNA replication is a biological example of physical copying. It involves complex molecular machinery that ensures high-fidelity copying of genetic information. The thermodynamics of DNA replication consider the energy costs associated with nucleotide polymerization and error correction mechanisms.\n\n**Information Theory and Physical Copying**\n\n- **Mutual Information and Work**: The mutual information between the initial and final states of a system sets a lower bound on the work required for physical processes like copying or erasing information.\n- **Copying Efficiency**: Defined as the ratio of copy information to the total mutual information. It reflects how effectively a system transmits information through copying versus transformation.\n- **Entropy and Noise**: In physical systems, noise and entropy production are inevitable. Information theory provides tools to quantify the effects of noise on the fidelity of copying processes.\n\n**Formulations Relating Physical Copying to Information Theory**\n\n- **Stochastic Thermodynamics**: Provides a framework to study small systems where thermal fluctuations are significant. It connects thermodynamic quantities with information-theoretic measures like entropy and mutual information.\n- **Work Extraction and Feedback Control**: Investigates how information gained about a system can be used to extract work, emphasizing the cost of acquiring and utilizing information.\n\n**Conclusion**\n\nThe paper advances our understanding of information transmission by formally distinguishing between copying and transformation. By introducing a decomposition of mutual information based on a set of intuitive axioms, it provides a valuable tool for analyzing systems where the fidelity of copying is crucial. The thermodynamic interpretation bridges the gap between abstract information measures and physical processes, offering insights into the fundamental limits of copying in both biological and artificial systems.\n\n**Key Takeaways**\n\n- **Copy vs. Transformation**: Recognizing the difference is essential in contexts like genetics, communication systems, and thermodynamics.\n- **Unique Measure**: The proposed measure of copy information is uniquely determined by the axioms and reflects the minimal work required for copying.\n- **Practical Implications**: Understanding how information is transmitted can inform the design of efficient communication systems and elucidate biological processes like evolution and replication.\n\n**References for Further Reading**\n\n- Shannon, C. E. (1948). A Mathematical Theory of Communication.\n- Landauer, R. (1961). Irreversibility and Heat Generation in the Computing Process.\n- Cover, T. M., & Thomas, J. A. (2006). Elements of Information Theory.\n- Parrondo, J. M. R., Horowitz, J. M., & Sagawa, T. (2015). Thermodynamics of Information."
            ]
          },
          "status": "finished_successfully",
          "end_turn": true,
          "weight": 1.0,
          "metadata": {
            "citations": [],
            "content_references": [],
            "gizmo_id": null,
            "finish_details": {
              "type": "stop",
              "stop_tokens": [
                199999
              ]
            },
            "is_complete": true,
            "message_type": null,
            "model_slug": "o1-preview",
            "default_model_slug": "o1-preview",
            "parent_id": "f3450118-7b1a-4650-8f9e-3764869b0c88",
            "request_id": "8d43e6fce93dab27-YYZ",
            "timestamp_": "absolute"
          },
          "recipient": "all",
          "channel": null
        },
        "parent": "f3450118-7b1a-4650-8f9e-3764869b0c88",
        "children": []
      }
    },
    "moderation_results": [],
    "current_node": "5cecce6f-2512-4fb7-bf9c-35ad45ccd76b",
    "plugin_ids": null,
    "conversation_id": "67119692-34a0-8005-b9c2-51cc0c05d0a3",
    "conversation_template_id": null,
    "gizmo_id": null,
    "gizmo_type": null,
    "is_archived": false,
    "is_starred": null,
    "safe_urls": [],
    "default_model_slug": "o1-preview",
    "conversation_origin": null,
    "voice": null,
    "async_status": null,
    "disabled_tool_ids": [],
    "id": "67119692-34a0-8005-b9c2-51cc0c05d0a3"
  },
  {
    "title": "Time Symmetry and Energy Conservation",
    "create_time": 1729133033.38889,
    "update_time": 1729133052.014787,
    "mapping": {
      "776f719a-044a-47a6-a6b3-f7de63f32a31": {
        "id": "776f719a-044a-47a6-a6b3-f7de63f32a31",
        "message": {
          "id": "776f719a-044a-47a6-a6b3-f7de63f32a31",
          "author": {
            "role": "system",
            "name": null,
            "metadata": {}
          },
          "create_time": null,
          "update_time": null,
          "content": {
            "content_type": "text",
            "parts": [
              ""
            ]
          },
          "status": "finished_successfully",
          "end_turn": true,
          "weight": 0.0,
          "metadata": {
            "is_visually_hidden_from_conversation": true
          },
          "recipient": "all",
          "channel": null
        },
        "parent": "fa1554ae-e02a-4465-afe5-dbf7f3a246a6",
        "children": [
          "2f20e67b-6855-46e8-8d88-e2339065c39b"
        ]
      },
      "fa1554ae-e02a-4465-afe5-dbf7f3a246a6": {
        "id": "fa1554ae-e02a-4465-afe5-dbf7f3a246a6",
        "message": null,
        "parent": null,
        "children": [
          "776f719a-044a-47a6-a6b3-f7de63f32a31"
        ]
      },
      "2f20e67b-6855-46e8-8d88-e2339065c39b": {
        "id": "2f20e67b-6855-46e8-8d88-e2339065c39b",
        "message": {
          "id": "2f20e67b-6855-46e8-8d88-e2339065c39b",
          "author": {
            "role": "user",
            "name": null,
            "metadata": {}
          },
          "create_time": 1729133033.394024,
          "update_time": null,
          "content": {
            "content_type": "text",
            "parts": [
              "Explain what exactly \u201ctime symmetry\u201d refers to when people use noethers laws to derive energy conservation. Does it mean the laws of physics are the same at every point in time? Or that they are reversible? Why does a made up physics which is non local not have energy conservation if the DE is the same for every time step and only a function of space?"
            ]
          },
          "status": "finished_successfully",
          "end_turn": null,
          "weight": 1.0,
          "metadata": {
            "request_id": "8d3cf18efdef36d1-YYZ",
            "message_source": null,
            "timestamp_": "absolute",
            "message_type": null
          },
          "recipient": "all",
          "channel": null
        },
        "parent": "776f719a-044a-47a6-a6b3-f7de63f32a31",
        "children": [
          "786ac4df-7fd8-4266-a4e3-bff4a6980b74"
        ]
      },
      "786ac4df-7fd8-4266-a4e3-bff4a6980b74": {
        "id": "786ac4df-7fd8-4266-a4e3-bff4a6980b74",
        "message": {
          "id": "786ac4df-7fd8-4266-a4e3-bff4a6980b74",
          "author": {
            "role": "tool",
            "name": "a8km123",
            "metadata": {}
          },
          "create_time": 1729133050.540019,
          "update_time": null,
          "content": {
            "content_type": "text",
            "parts": [
              "**Unraveling time symmetry**\n\nI\u2019m breaking down \"time symmetry\" in the context of Noether's theorem. This involves examining its implications for energy conservation and whether it suggests the laws of physics are consistent or reversible across time.\n\n**Examining Noether's theorem**\n\nI\u2019m thinking through how time symmetry in Noether's theorem implies energy conservation by showing physical laws' invariance under time translation.\n\n**Exploring energy conservation**\n\nOK, let's think it through: in a non-local physics context, energy conservation might falter if these laws don't link energy to time or spacetime coordinates."
            ]
          },
          "status": "finished_successfully",
          "end_turn": null,
          "weight": 0.0,
          "metadata": {
            "finished_text": "Thought for 6 seconds",
            "initial_text": "Thinking",
            "parent_id": "2f20e67b-6855-46e8-8d88-e2339065c39b",
            "request_id": "8d3cf18efdef36d1-YYZ",
            "timestamp_": "absolute",
            "message_type": null
          },
          "recipient": "all",
          "channel": null
        },
        "parent": "2f20e67b-6855-46e8-8d88-e2339065c39b",
        "children": [
          "c92a796a-f0de-4bd1-bd9d-0fbb176d116f"
        ]
      },
      "c92a796a-f0de-4bd1-bd9d-0fbb176d116f": {
        "id": "c92a796a-f0de-4bd1-bd9d-0fbb176d116f",
        "message": {
          "id": "c92a796a-f0de-4bd1-bd9d-0fbb176d116f",
          "author": {
            "role": "assistant",
            "name": null,
            "metadata": {}
          },
          "create_time": 1729133050.540132,
          "update_time": null,
          "content": {
            "content_type": "text",
            "parts": [
              "**Time Symmetry in Noether's Theorem and Energy Conservation**\n\n**1. What Does \"Time Symmetry\" Refer To in Noether's Theorem?**\n\nIn the context of Noether's theorem, **time symmetry** refers to the invariance of the physical laws under **continuous translations in time**. This means that the fundamental equations describing a physical system remain unchanged if we shift the time coordinate by any constant amount. Mathematically, if \\( t \\rightarrow t + \\delta t \\) leaves the action integral invariant (where \\( \\delta t \\) is any constant), the system exhibits time translation symmetry.\n\n**2. Does It Mean the Laws of Physics Are the Same at Every Point in Time?**\n\nYes, precisely. Time symmetry implies that the **laws of physics are time-independent**; they do not change over time. This constancy ensures that experiments conducted under the same conditions will yield the same results, regardless of when they are performed. It's this uniformity that, via Noether's theorem, leads to the conservation of energy.\n\n**3. Or That They Are Reversible?**\n\nTime symmetry in Noether's theorem refers to **time translation symmetry**, not **time reversal symmetry**. Time reversal symmetry involves the invariance of physical laws under the transformation \\( t \\rightarrow -t \\), effectively running time backward. While some fundamental laws are time-reversal invariant (like classical mechanics without friction), others (like the second law of thermodynamics) are not. Therefore, **time translation symmetry (time invariance)** and **time reversal symmetry** are distinct concepts.\n\n**4. Why Does a Non-Local Physics Without Time Dependence Lack Energy Conservation?**\n\nIn a hypothetical physics framework where the laws are **non-local** (interactions occur instantaneously over distance without mediating particles or fields) and the differential equations are time-invariant but depend only on space, energy conservation may fail due to the breakdown of the prerequisites of Noether's theorem.\n\n**Explanation:**\n\n- **Noether's Theorem Requirements:** Noether's theorem applies to systems whose action is local and depends smoothly on the fields and their derivatives at each point in spacetime. It connects continuous symmetries of the action to conserved quantities.\n\n- **Non-Locality Issues:** In non-local theories, the action may involve integrals over spatially separated points, making it a function of fields at different locations simultaneously. This non-locality can prevent the action from being invariant under time translations in the way required by Noether's theorem.\n\n- **Failure of Energy Conservation:** Even if the differential equations are the same at every time step (time-invariant), the non-local interactions can introduce dependencies that violate the conservation laws derived from Noether's theorem. Specifically, the energy defined in local theories may not be conserved because the non-local interactions can effectively \"inject\" or \"remove\" energy from a system in a way that isn't accounted for by local energy densities.\n\n**Example to Illustrate:**\n\nConsider a system where particles interact instantaneously at a distance without any mediating fields (a non-local interaction). If particle A moves, particle B instantaneously feels this effect, regardless of the distance. This instantaneous action at a distance doesn't conform to local conservation laws because there's no mechanism accounting for the energy transfer through space.\n\n**Conclusion:**\n\n- **Time Symmetry and Energy Conservation:** Time translation symmetry (time invariance) in the laws of physics leads to energy conservation via Noether's theorem in systems where the action is local.\n\n- **Non-Local Theories:** In non-local physics, even if the differential equations are time-invariant and depend only on space, the lack of locality means that the usual derivation of conservation laws via Noether's theorem doesn't hold. The non-local interactions can disrupt the conservation of energy because they don't satisfy the conditions necessary for Noether's theorem to apply.\n\n**Summary:**\n\n- **Time Symmetry:** Refers to the invariance of physical laws under shifts in time\u2014laws remain the same at all times.\n- **Energy Conservation:** Arises from time symmetry in systems where the action is local, as per Noether's theorem.\n- **Non-Local Physics:** May lack energy conservation despite time-invariant laws because non-local interactions violate the conditions required for Noether's theorem to guarantee conservation laws."
            ]
          },
          "status": "finished_successfully",
          "end_turn": true,
          "weight": 1.0,
          "metadata": {
            "citations": [],
            "content_references": [],
            "gizmo_id": null,
            "finish_details": {
              "type": "stop",
              "stop_tokens": [
                199999
              ]
            },
            "is_complete": true,
            "message_type": null,
            "model_slug": "o1-preview",
            "default_model_slug": "o1-preview",
            "parent_id": "786ac4df-7fd8-4266-a4e3-bff4a6980b74",
            "request_id": "8d3cf18efdef36d1-YYZ",
            "timestamp_": "absolute"
          },
          "recipient": "all",
          "channel": null
        },
        "parent": "786ac4df-7fd8-4266-a4e3-bff4a6980b74",
        "children": []
      }
    },
    "moderation_results": [],
    "current_node": "c92a796a-f0de-4bd1-bd9d-0fbb176d116f",
    "plugin_ids": null,
    "conversation_id": "671079e8-d828-8005-a04f-18537df037c4",
    "conversation_template_id": null,
    "gizmo_id": null,
    "gizmo_type": null,
    "is_archived": false,
    "is_starred": null,
    "safe_urls": [],
    "default_model_slug": "o1-preview",
    "conversation_origin": null,
    "voice": null,
    "async_status": null,
    "disabled_tool_ids": [],
    "id": "671079e8-d828-8005-a04f-18537df037c4"
  }
]