<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Ivan Yevenko</title>
        <link rel="stylesheet" href="fonts.css">
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </head>
<body>
    <header>
        <h1>Ivan Yevenko</h1>
    </header>
    
    <main>
        <p>Welcome to the rabbit hole. First, watch this video:</p>
        <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%;">
            <iframe src="https://www.youtube.com/embed/HT49wpyux-k" allowfullscreen 
                    style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>
        </div>
        <p>
            Now go check out Bert Chan's Lenia <a href="https://chakazul.github.io/lenia.html" target="_blank">website</a>.
            Specifically, go through the jupyter notebook
            <a href="https://colab.research.google.com/github/OpenLenia/Lenia-Tutorial/blob/main/Tutorial_From_Conway_to_Lenia.ipynb" target="_blank">From Conway to Lenia</a>.
            Turns out we know practically nothing about why any of this happens, even though it takes a few lines of code to run.
        </p>

        <p>
            We don't even know why complex things happen in 1D binary systems where every possible rule set has been explored. Those
            were discovered over 40 years ago and studied extensively (e.g. <a href="https://www.wolframscience.com/nks/" target="_blank">Wolfram's NKS</a>).
            Yet, what we still don't know how to measure complexity, understand where it comes from, how to predict it, or how to design complex systems.
            We have no idea what complexity is in general, all we have are some heuristics to use in different contexts.
        </p>
        <p>
            Maybe the notion of information, or entropy is a good place to start? But no one knows what information is either!
            Sure, there's Shannon's entropy for bitstrings, Boltzmann's and Gibbs' formulations of entropy for thermodynamics,
            Von Neumann's entropy for quantum mechanics, and Kolmogorov-Sinai entropy for dynamical systems, etc... For a full
            list just take a look at the disambiguation page for entropy on <a href="https://en.wikipedia.org/wiki/Entropy_(disambiguation)" target="_blank">Wikipedia</a>.
        </p>
        <p>
            What about life? Do we know anything about life from biology? Well we certainly don't have a definition. We don't even
            know what to look for in the search for extraterrestrial life. We don't know how life started on Earth, or how it became
            intelligent.
        </p>
        <p>
            I'm certainly being very liberal with my usage of "we don't know", but I think it's justified until somebody can given me
            an explanation that makes sense. For example, I think we <i>do</i> know what evolution is, because there is a simple agreed-upon
            explanation that the lay-person can understand, and we can use our understanding as a tool to solve seemingly unrelated problems
            like mechanical design. Complexity, information, entropy, emergence, intelligence and life are not like that today, and that's
            exciting because that means there's lots more science to be done! 
        </p>

    </main>
</body>
</html>